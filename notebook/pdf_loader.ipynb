{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d539eb",
   "metadata": {},
   "source": [
    "##embedding And vectorStoreDB\n",
    "\n",
    "RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4475429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jasmi\\OneDrive\\Desktop\\RAGtut\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abf88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files to process\n",
      "\n",
      "Processing: 15 marks VDA.pdf\n",
      "  ✓ Loaded 29 pages\n",
      "\n",
      "Processing: bda ccet1.pdf\n",
      "  ✓ Loaded 58 pages\n",
      "\n",
      "Processing: Part B OE.pdf\n",
      "  ✓ Loaded 19 pages\n",
      "\n",
      "Processing: plm 15 marks.pdf\n",
      "  ✓ Loaded 23 pages\n",
      "\n",
      "Processing: UNIT-01_2M[1].pdf\n",
      "  ✓ Loaded 9 pages\n",
      "\n",
      "Total documents loaded: 138\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e7aa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content=\"15 marks VDA \\n \\n1.Significance of Software Standards in OpenGL and How It Ensures Cross-Platform Compatibility \\nSignificance of Software Standards in OpenGL \\nSoftware standards in OpenGL are essential for ensuring uniformity, performance, and compatibility \\nacross different computing environments. OpenGL is an open standard for graphics rendering, which \\nmeans it is maintained and updated by a consortium rather than a single company. The importance \\nof software standards in OpenGL can be explained through the following points: \\n1. Cross-Platform Consistency: \\nOpenGL provides a standardized API that enables graphics applications to run across multiple \\nplatforms, including Windows, macOS, Linux, and mobile operating systems like Android. This \\nreduces the need for platform-specific modifications and ensures consistent performance \\nacross different devices. \\n2. Hardware Abstraction: \\nOpenGL abstracts hardware details, allowing applications to interact with the GPU without \\nbeing tied to a specific graphics card manufacturer. This means developers do not have to \\nwrite separate code for AMD, NVIDIA, or Intel GPUs, as OpenGL provides a unified API. \\n3. Interoperability Across Vendors: \\nOpenGL follows a well-defined specification that multiple vendors adhere to. This ensures \\nthat OpenGL applications can run on different hardware implementations without significant \\nmodifications, fostering a competitive and diverse ecosystem. \\n4. Backward Compatibility: \\nOpenGL maintains backward compatibility between different versions, ensuring that \\napplications written for older OpenGL versions can still function on newer hardware with \\nminimal changes. This protects developers from frequent rewrites and ensures longevity for \\ngraphics applications. \\n5. Performance Optimization: \\nOpenGL standards define efficient rendering pipelines and data management techniques, \\nallowing applications to optimize GPU usage. Standardized API functions ensure that \\napplications use the best possible methods for rendering, irrespective of the underlying \\nhardware. \\n6. Scalability and Flexibility: \\nOpenGL is designed to support applications ranging from simple 2D graphics rendering to \\ncomplex 3D simulations and high-performance computing. It scales from embedded systems \\nto high-end gaming and visualization applications. \\n7. Adoption in Multiple Industries: \\nDue to its open nature and wide support, OpenGL is used in various industries, including \\ngaming, CAD (Computer-Aided Design), scientific visualization, virtual reality, and medical \\nimaging. This widespread adoption is made possible by standardized API guidelines. \\n8. Facilitation of Open-Source Development: \\nOpenGL's standardized nature has led to the development of numerous open-source\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='graphics engines, frameworks, and libraries, fostering innovation and collaboration in the \\nsoftware development community. \\nHow OpenGL Ensures Cross-Platform Compatibility \\nOpenGL achieves cross-platform compatibility through several key mechanisms: \\n1. Platform-Independent API: \\nOpenGL provides a single, consistent API that remains the same across different operating \\nsystems. This means that the same OpenGL code can be compiled and executed on various \\nplatforms with minimal modifications. \\n2. Graphics Driver Support: \\nGPU manufacturers like NVIDIA, AMD, and Intel provide OpenGL-compliant drivers that \\nimplement the OpenGL standard for their specific hardware. These drivers handle the \\ntranslation of OpenGL API calls into optimized machine-level instructions for the GPU. \\n3. Extension Mechanism: \\nOpenGL supports an extension system that allows hardware vendors to introduce new \\nfeatures while maintaining compatibility with the core OpenGL standard. Developers can \\nselectively use these extensions to take advantage of new hardware capabilities without \\nbreaking compatibility with older systems. \\n4. Use of Utility Libraries: \\nOpenGL does not handle system-specific tasks such as window management or input \\nhandling. Instead, libraries like GLUT (OpenGL Utility Toolkit) and GLFW (Graphics Library \\nFramework) provide cross-platform support for these functions, allowing OpenGL \\napplications to run on different operating systems seamlessly. \\n5. OpenGL Context Management: \\nOpenGL requires an OpenGL context to be created before rendering can begin. Libraries like \\nWGL (Windows), GLX (Linux), and CGL (macOS) handle the integration with the operating \\nsystem, ensuring smooth cross-platform support. \\n6. Shader Language Standardization (GLSL): \\nOpenGL includes the OpenGL Shading Language (GLSL), which allows developers to write \\nGPU-accelerated shaders in a hardware-independent way. GLSL ensures that custom graphics \\neffects and rendering techniques can work across different GPU brands without modification. \\n7. Adherence to OpenGL Standards by the Khronos Group: \\nThe Khronos Group oversees the OpenGL specification, ensuring that all implementations \\nfollow a common standard. This prevents fragmentation and ensures a stable and \\npredictable development environment. \\n8. Support for Different Rendering Pipelines: \\nOpenGL supports both fixed-function and programmable pipelines, making it compatible \\nwith both older and modern hardware. The introduction of modern pipeline techniques like \\nVertex Buffer Objects (VBOs) and Framebuffer Objects (FBOs) further enhances its flexibility \\nacross different platforms. \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL’s adherence to software standards ensures that developers can build applications that work \\nseamlessly across different operating systems and hardware configurations. Its cross-platform \\ncapabilities are maintained through standardized APIs, driver support, extension mechanisms, and \\nshader language standardization. As a result, OpenGL remains one of the most widely used graphics \\nAPIs for real-time rendering, making it a crucial tool for developers across multiple industries. \\n \\n(8marks) \\n3. Advantages and Limitations of DDA and Bresenham’s Line Drawing Algorithms \\nLine drawing is a fundamental operation in computer graphics, and DDA (Digital Differential \\nAnalyzer) algorithm and Bresenham’s algorithm are two commonly used techniques for rasterizing \\nlines on a pixel-based display. Both methods work by determining which pixels should be illuminated \\nto best approximate a straight line. \\nEach algorithm has its strengths and weaknesses in terms of speed, accuracy, computational \\nefficiency, and hardware implementation. Below is a detailed comparison of the advantages and \\nlimitations of both algorithms. \\n \\n1. Digital Differential Analyzer (DDA) Algorithm \\nOverview \\nThe Digital Differential Analyzer (DDA) algorithm is an incremental approach to line drawing. It \\ncalculates intermediate points between two endpoints using the slope-intercept equation of a line: \\ny=mx+cy = mx + cy=mx+c  \\nwhere m is the slope of the line. The algorithm incrementally updates x or y depending on the slope: \\n• If |m| ≤ 1, increment x and compute y. \\n• If |m| > 1, increment y and compute x. \\nThis method relies on floating-point arithmetic to determine pixel positions. \\n \\nAdvantages of DDA Algorithm \\n1. Simple and Easy to Implement \\no The logic follows the basic concept of linear interpolation. \\no The algorithm is easier to understand and implement compared to Bresenham’s \\nalgorithm. \\n2. Handles All Slopes Uniformly \\no DDA works for both steep and shallow slopes by adjusting the step size in either the \\nx or y direction. \\n3. Works Well with Floating-Point Arithmetic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Since DDA uses floating-point calculations, it provides higher precision in \\ndetermining pixel positions compared to purely integer-based methods. \\n4. Generalized for Other Shapes \\no The principles of DDA can be extended for circle drawing, curve generation, and \\nother geometric transformations in graphics. \\n \\nLimitations of DDA Algorithm \\n1. Computationally Expensive \\no DDA uses floating-point multiplication, division, and rounding operations, which \\nare slower than integer calculations. \\no This makes it less suitable for real-time applications and low-powered devices like \\nembedded systems. \\n2. Accumulation of Round-Off Errors \\no Since floating-point calculations are used, rounding errors accumulate over long \\nlines. \\no This can result in deviations from the true line as more pixels are plotted. \\n3. Less Efficient than Bresenham’s Algorithm \\no Requires additional multiplication and division operations in each step, making it \\ncomputationally slower than Bresenham’s algorithm. \\no Not ideal for applications that require high-speed rendering. \\n \\n2. Bresenham’s Line Drawing Algorithm \\nOverview \\nBresenham’s algorithm is an incremental decision-based approach that efficiently determines the \\nnext pixel using integer arithmetic instead of floating-point calculations. \\nIt avoids multiplication and division by using an error term (decision variable) to track the closest \\npixel. Instead of computing the exact floating-point values, it approximates the line by selecting the \\npixel that minimizes error accumulation. \\n \\nAdvantages of Bresenham’s Algorithm \\n1. More Efficient than DDA \\no Uses only integer addition and subtraction, making it significantly faster than the \\nDDA algorithm. \\no Suitable for real-time rendering and low-resource systems like embedded devices.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='2. Minimizes Round-Off Errors \\no Bresenham’s algorithm avoids floating-point calculations, eliminating accumulated \\nrounding errors found in DDA. \\no This makes it more accurate for pixel-based rendering. \\n3. Better Performance on Raster Displays \\no Since it directly selects the next pixel based on integer calculations, it optimally fits \\ninto raster display systems. \\no Works well for modern graphics hardware. \\n4. Ideal for Hardware Implementation \\no Bresenham’s algorithm is easy to implement in hardware circuits (like GPUs), making \\nit widely used in real-time rendering engines. \\n5. Reduces Computational Overhead \\no No need for complex arithmetic operations (multiplication or division), leading to \\nbetter execution speed. \\n \\nLimitations of Bresenham’s Algorithm \\n1. Limited to Straight Lines Only \\no Unlike DDA, Bresenham’s method is specifically designed for straight lines. \\no To draw curves, additional algorithms like Midpoint Circle Algorithm are needed. \\n2. Slightly More Complex Implementation \\no While more efficient, Bresenham’s algorithm is harder to understand and \\nimplement than DDA. \\n3. Does Not Support Anti-Aliasing \\no The algorithm produces jagged edges (aliasing effect) when rendering lines. \\no Additional anti-aliasing techniques like Wu’s Line Algorithm are needed for smooth \\nline rendering. \\n \\nComparison Table: DDA vs. Bresenham’s Algorithm \\nFeature DDA Algorithm Bresenham’s Algorithm \\nArithmetic Type Uses floating-point calculations Uses only integer calculations \\nExecution Speed Slower due to \\ndivision/multiplication Faster due to integer arithmetic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Feature DDA Algorithm Bresenham’s Algorithm \\nAccuracy Accumulates rounding errors More accurate, avoids rounding errors \\nEfficiency Less efficient for high-speed \\napplications Highly efficient for real-time rendering \\nComplexity Easier to implement More complex decision-making process \\nHandling of Slopes Works well for all slopes Requires different cases for steep/shallow \\nslopes \\nHardware \\nSuitability Not ideal for low-power devices Well-suited for hardware and embedded \\nsystems \\nAnti-Aliasing \\nSupport No built-in anti-aliasing Produces jagged edges (needs additional \\nsmoothing) \\n \\nConclusion \\nBoth DDA and Bresenham’s algorithm have their place in computer graphics: \\n• DDA is simple and useful for understanding basic line drawing principles but suffers from \\ncomputational inefficiency. \\n• Bresenham’s algorithm is the preferred choice for fast, accurate, and efficient rendering, \\nmaking it the standard for real-time graphics applications. \\nIf performance is a priority (e.g., in gaming, simulations, and embedded systems), Bresenham’s \\nalgorithm is the best choice. However, for floating-point precision and ease of implementation, \\nDDA can still be useful in applications where speed is not critical. \\n \\n \\n \\n(7 marks) \\n4.Construct homogeneous representation in computer graphics and discuss how it simplifies the \\nprocess of transformations along with its merits.(7) provide detailed answer \\nChatGPT said: \\nHomogeneous Representation in Computer Graphics \\nIn computer graphics, homogeneous coordinates are an extension of Cartesian coordinates that \\nfacilitate geometric transformations such as translation, scaling, rotation, and perspective projection. \\nHomogeneous coordinates introduce an additional dimension (w-component), making it easier to \\nexpress and perform transformations using matrix multiplication. \\nFor a 2D point (x,y)(x, y)(x,y) in Cartesian coordinates, the homogeneous representation is given as:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='How Homogeneous Coordinates Simplify Transformations \\nHomogeneous coordinates enable all transformations (translation, rotation, scaling, and perspective \\nprojection) to be expressed using matrix multiplication. In standard Cartesian coordinates, \\ntranslation cannot be performed using matrix operations alone, but in homogeneous form, all \\ntransformations become linear operations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Merits of Homogeneous Representation \\n1. Unified Matrix Representation for All Transformations \\no Rotation, scaling, translation, and projection can be performed using 4×4 matrices in \\n3D graphics, enabling efficient pipeline processing in graphics hardware. \\n2. Efficient Computation Using Matrix Multiplication \\no Since all transformations are represented as matrix multiplications, multiple \\ntransformations can be combined into a single matrix before applying them to a \\npoint or object. \\n3. Easier Composition of Transformations \\no Instead of applying transformations sequentially (which is computationally \\nexpensive), they can be pre-multiplied to obtain a composite transformation matrix. \\n4. Supports Perspective Projection \\no Homogeneous coordinates allow perspective transformations using a w-component, \\nwhich cannot be achieved using simple Cartesian coordinates. \\n5. Simplifies Complex Geometric Operations \\no Operations such as shearing, mirroring, and 3D clipping are easier to perform in \\nhomogeneous coordinates. \\n6. Optimized for Computer Graphics Pipelines \\no Modern GPU architectures and graphics APIs (OpenGL, DirectX) use homogeneous \\ncoordinates to efficiently process and render 3D objects. \\n7. Allows Infinite Points Representation \\no When w=0w = 0w=0, it represents a point at infinity, which is useful for handling \\nparallel lines and projections in computer graphics. \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 8, 'page_label': '9', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Homogeneous coordinates play a crucial role in computer graphics by simplifying transformations, \\nunifying geometric operations, and optimizing rendering efficiency. They enable complex \\ntransformations using simple matrix multiplications, making them a fundamental concept in 2D and \\n3D graphics applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(15marks) \\n7.Identify the various OpenGL functions used to create and manipulate points, lines, and filled areas \\nin 2D.(15) provide detailed answer for 15 markks \\nChatGPT said: \\nOpenGL Functions for Creating and Manipulating Points, Lines, and Filled Areas in 2D \\nOpenGL provides a set of functions to create and manipulate points, lines, and filled areas in 2D \\ngraphics. These functions are essential for rendering basic geometric shapes and building complex \\ngraphics applications. \\n \\n1. Functions for Creating and Manipulating Points \\nPoints are the simplest geometric primitives in OpenGL. A point is a single pixel on the screen. \\na) glBegin(GL_POINTS) and glEnd() \\nUsed to define and draw a set of individual points. Each point is specified using glVertex*(). \\ncpp \\nCopyEdit \\nglBegin(GL_POINTS);  \\n    glVertex2f(0.5, 0.5);  // Draw a point at (0.5, 0.5) \\n    glVertex2f(-0.5, -0.5); // Another point at (-0.5, -0.5) \\nglEnd(); \\nb) glPointSize(size) \\nControls the size of the points. \\ncpp \\nCopyEdit \\nglPointSize(5.0); // Sets point size to 5 pixels \\nc) glEnable(GL_POINT_SMOOTH) \\nEnables anti-aliasing for smooth points. \\ncpp \\nCopyEdit \\nglEnable(GL_POINT_SMOOTH); \\n \\n2. Functions for Creating and Manipulating Lines'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 10, 'page_label': '11', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Lines are fundamental graphics primitives used in drawing outlines, wireframes, and geometric \\nstructures. \\na) glBegin(GL_LINES) and glEnd() \\nDraws individual line segments between pairs of points. \\ncpp \\nCopyEdit \\nglBegin(GL_LINES); \\n    glVertex2f(-0.5, 0.0); // Start point \\n    glVertex2f(0.5, 0.0);  // End point \\nglEnd(); \\nb) glBegin(GL_LINE_STRIP) \\nDraws connected lines without closing the shape. \\ncpp \\nCopyEdit \\nglBegin(GL_LINE_STRIP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\n    glVertex2f(0.5, -0.5); \\nglEnd(); \\nc) glBegin(GL_LINE_LOOP) \\nDraws a closed polygonal shape with connected lines. \\ncpp \\nCopyEdit \\nglBegin(GL_LINE_LOOP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\nd) glLineWidth(width) \\nSets the width of the line in pixels.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 11, 'page_label': '12', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='cpp \\nCopyEdit \\nglLineWidth(3.0); // Sets the line thickness to 3 pixels \\ne) glEnable(GL_LINE_SMOOTH) \\nEnables anti-aliasing for smoother lines. \\ncpp \\nCopyEdit \\nglEnable(GL_LINE_SMOOTH); \\n \\n3. Functions for Creating and Manipulating Filled Areas (Polygons) \\nPolygons are used to represent filled areas such as rectangles, triangles, and complex shapes. \\na) glBegin(GL_TRIANGLES) \\nDraws a set of independent triangles (each group of three vertices forms a triangle). \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLES); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\nglEnd(); \\nb) glBegin(GL_TRIANGLE_STRIP) \\nDraws a connected series of triangles, reducing the number of vertices needed. \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLE_STRIP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\n    glVertex2f(0.5, 0.5); \\nglEnd(); \\nc) glBegin(GL_TRIANGLE_FAN)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Similar to a triangle strip, but all triangles share a common starting vertex. \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLE_FAN); \\n    glVertex2f(0.0, 0.5);  // Common center \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\nglEnd(); \\nd) glBegin(GL_QUADS) \\nDraws independent quadrilaterals (each four vertices form a closed shape). \\ncpp \\nCopyEdit \\nglBegin(GL_QUADS); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\ne) glBegin(GL_POLYGON) \\nDraws a general polygon with more than four vertices. \\ncpp \\nCopyEdit \\nglBegin(GL_POLYGON); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.7, 0.2); \\n    glVertex2f(0.3, 0.6); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\nf) glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Specifies whether the polygons should be drawn as filled, wireframe, or points. \\ncpp \\nCopyEdit \\nglPolygonMode(GL_FRONT_AND_BACK, GL_FILL); // Default: filled \\nglPolygonMode(GL_FRONT_AND_BACK, GL_LINE); // Wireframe mode \\nglPolygonMode(GL_FRONT_AND_BACK, GL_POINT); // Points only \\n \\n4. Additional OpenGL Functions for Manipulation \\na) glColor3f(r, g, b) – Sets Color \\nColors the primitives (values range from 0.0 to 1.0). \\ncpp \\nCopyEdit \\nglColor3f(1.0, 0.0, 0.0); // Red color \\nb) glViewport(x, y, width, height) \\nDefines the 2D rendering area on the screen. \\ncpp \\nCopyEdit \\nglViewport(0, 0, 800, 600); // Sets viewport to 800x600 pixels \\nc) glClear(GL_COLOR_BUFFER_BIT) \\nClears the screen before drawing. \\ncpp \\nCopyEdit \\nglClear(GL_COLOR_BUFFER_BIT); \\nd) glFlush() \\nForces immediate execution of OpenGL commands. \\ncpp \\nCopyEdit \\nglFlush(); \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL provides a rich set of functions for creating and manipulating points, lines, and filled \\npolygons in 2D graphics. These functions allow for efficient rendering and manipulation of shapes \\nusing vertex-based commands, matrix transformations, and rasterization techniques. By combining \\nthese basic elements, complex graphics applications can be developed efficiently. \\n4o \\nYou said: \\nfor the above question provide one whole program as a example \\nChatGPT said: \\nHere is a complete OpenGL program that demonstrates how to draw and manipulate points, lines, \\nand filled polygons in 2D using OpenGL and GLUT. \\nThis program: \\n   Draws a point \\n   Draws different types of lines (single lines, line strip, line loop) \\n   Draws filled polygons (triangles, quads, and a general polygon) \\n   Uses colors and transformations \\n \\nComplete OpenGL Program in C++ \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n \\n// Function to initialize OpenGL settings \\nvoid init() { \\n    glClearColor(1.0, 1.0, 1.0, 1.0); // White background \\n    glMatrixMode(GL_PROJECTION); \\n    gluOrtho2D(-1.0, 1.0, -1.0, 1.0); // Set coordinate system \\n} \\n \\n// Function to draw points, lines, and polygons \\nvoid display() { \\n    glClear(GL_COLOR_BUFFER_BIT); // Clear screen \\n     \\n    // **Draw a Point** \\n    glPointSize(5.0); // Set point size'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glColor3f(1.0, 0.0, 0.0); // Red color \\n    glBegin(GL_POINTS); \\n        glVertex2f(0.0, 0.8); \\n    glEnd(); \\n \\n    // **Draw a Line** \\n    glLineWidth(3.0); // Set line width \\n    glColor3f(0.0, 0.0, 1.0); // Blue color \\n    glBegin(GL_LINES); \\n        glVertex2f(-0.8, 0.5); \\n        glVertex2f(0.8, 0.5); \\n    glEnd(); \\n \\n    // **Draw a Line Strip** \\n    glColor3f(0.0, 0.7, 0.3); // Greenish color \\n    glBegin(GL_LINE_STRIP); \\n        glVertex2f(-0.8, 0.2); \\n        glVertex2f(-0.4, 0.3); \\n        glVertex2f(0.0, 0.2); \\n        glVertex2f(0.4, 0.3); \\n        glVertex2f(0.8, 0.2); \\n    glEnd(); \\n \\n    // **Draw a Triangle** \\n    glColor3f(1.0, 0.5, 0.0); // Orange color \\n    glBegin(GL_TRIANGLES); \\n        glVertex2f(-0.6, -0.2); \\n        glVertex2f(0.0, -0.2); \\n        glVertex2f(-0.3, -0.6); \\n    glEnd();'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 16, 'page_label': '17', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='// **Draw a Quad (Rectangle)** \\n    glColor3f(0.5, 0.0, 0.5); // Purple color \\n    glBegin(GL_QUADS); \\n        glVertex2f(0.2, -0.2); \\n        glVertex2f(0.6, -0.2); \\n        glVertex2f(0.6, -0.6); \\n        glVertex2f(0.2, -0.6); \\n    glEnd(); \\n \\n    // **Draw a Polygon** \\n    glColor3f(0.0, 0.7, 0.7); // Cyan color \\n    glBegin(GL_POLYGON); \\n        glVertex2f(-0.2, -0.8); \\n        glVertex2f(0.0, -0.7); \\n        glVertex2f(0.2, -0.8); \\n        glVertex2f(0.3, -0.9); \\n        glVertex2f(0.0, -1.0); \\n        glVertex2f(-0.3, -0.9); \\n    glEnd(); \\n \\n    glFlush(); // Force execution \\n} \\n \\n// Main function to initialize GLUT and start the program \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"OpenGL 2D Drawing Example\"); \\n \\n    init(); // Initialize OpenGL settings'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glutDisplayFunc(display); // Set display callback function \\n    glutMainLoop(); // Start the main event loop \\n \\n    return 0; \\n} \\n \\nExplanation of the Program \\n1. Initialization (init()) \\no Sets the background color to white. \\no Defines the coordinate system using gluOrtho2D(), setting the range from (-1,1) for \\nboth X and Y axes. \\n2. Display Function (display()) \\no Clears the screen before drawing. \\no Draws a single point at (0.0, 0.8) with glPointSize(5.0). \\no Draws a single line from (-0.8, 0.5) to (0.8, 0.5). \\no Draws a line strip, forming a connected zigzag pattern. \\no Draws a filled triangle, colored orange. \\no Draws a filled rectangle (quad), colored purple. \\no Draws a polygon, resembling a star-like shape. \\n3. GLUT Main Loop (main()) \\no Initializes GLUT. \\no Creates a 500x500 pixel window. \\no Calls init() for OpenGL settings. \\no Starts the event loop with glutMainLoop(), which continuously listens for display \\nevents. \\n \\nExpected Output \\nWhen you run this program, you will see:    A red point at the top \\n   A blue horizontal line in the middle \\n   A green zigzag line strip \\n   An orange triangle on the left \\n   A purple rectangle on the right \\n   A cyan star-like polygon at the bottom'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 18, 'page_label': '19', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='How to Compile and Run \\nFor Windows (MinGW) \\n1. Install MinGW and FreeGLUT. \\n2. Compile using:  \\nlua \\nCopyEdit \\ng++ filename.cpp -o output.exe -lfreeglut -lopengl32 -lglu32 \\n3. Run:  \\nlua \\nCopyEdit \\noutput.exe \\nFor Linux (g++) \\n1. Install OpenGL libraries:  \\narduino \\nCopyEdit \\nsudo apt-get install freeglut3-dev \\n2. Compile:  \\nlua \\nCopyEdit \\ng++ filename.cpp -o output -lGL -lGLU -lglut \\n3. Run:  \\nbash \\nCopyEdit \\n./output \\n \\nConclusion \\nThis program demonstrates all major OpenGL primitives used in 2D graphics, including points, lines, \\nand filled polygons. You can modify colors, shapes, and positions to experiment with different \\ngraphics rendering techniques.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(7 marks) \\n11. Build the Digital Differential Analyzer line drawing algorithm in detail.(7) provide detailed answer \\nChatGPT said: \\nDigital Differential Analyzer (DDA) Line Drawing Algorithm \\nThe Digital Differential Analyzer (DDA) algorithm is a fundamental rasterization algorithm used in \\ncomputer graphics to draw lines between two points. It is based on incremental calculations and is \\nwidely used due to its simplicity and efficiency. \\n \\nConcept of DDA Algorithm \\nThe DDA algorithm works by calculating intermediate points between the start and end points of a \\nline and plotting them pixel by pixel. It uses the equation of a straight line: \\ny=mx+cy = mx + cy=mx+c  \\nwhere m is the slope of the line, given by: \\nm=(y2−y1)(x2−x1)m = \\\\frac{(y_2 - y_1)}{(x_2 - x_1)}m=(x2−x1)(y2−y1)  \\nInstead of using floating-point arithmetic and multiplication, the algorithm increments either xxx or \\nyyy by a fixed step size and calculates the corresponding other coordinate. \\n \\nSteps of the DDA Algorithm \\n1. Input the start point (x1,y1)(x_1, y_1)(x1,y1) and end point (x2,y2)(x_2, y_2)(x2,y2). \\n2. Compute dx and dy: dx=x2−x1,dy=y2−y1dx = x_2 - x_1, \\\\quad dy = y_2 - y_1dx=x2−x1,dy=y2\\n−y1  \\n3. Find the step count (maximum change in either x or y): steps=max\\u2061(∣dx∣,∣dy∣)\\\\text{steps} = \\n\\\\max(|dx|, |dy|)steps=max(∣dx∣,∣dy∣)  \\n4. Compute the incremental step values: \\nXincrement=dxsteps,Yincrement=dystepsX_{\\\\text{increment}} = \\\\frac{dx}{\\\\text{steps}}, \\\\quad \\nY_{\\\\text{increment}} = \\\\frac{dy}{\\\\text{steps}}Xincrement=stepsdx,Yincrement=stepsdy  \\n5. Initialize starting position at (x1,y1)(x_1, y_1)(x1,y1). \\n6. Iterate through the step count, rounding the values to the nearest integer and plotting the \\npixel. \\n7. Repeat until the end point is reached. \\n \\nDDA Algorithm Implementation in C++ (Using OpenGL) \\nHere’s a complete C++ program that implements the DDA line drawing algorithm using OpenGL: \\ncpp'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='CopyEdit \\n#include <GL/glut.h> \\n#include <cmath> \\n#include <iostream> \\n \\nusing namespace std; \\n \\n// Function to implement the DDA line drawing algorithm \\nvoid drawDDA(float x1, float y1, float x2, float y2) { \\n    float dx = x2 - x1; \\n    float dy = y2 - y1; \\n     \\n    int steps = max(abs(dx), abs(dy));  // Find the number of steps \\n     \\n    float Xinc = dx / steps;  // Increment in x per step \\n    float Yinc = dy / steps;  // Increment in y per step \\n     \\n    float x = x1, y = y1; \\n     \\n    glBegin(GL_POINTS); \\n    for (int i = 0; i <= steps; i++) { \\n        glVertex2f(round(x), round(y));  // Plot the point \\n        x += Xinc;  // Increment x \\n        y += Yinc;  // Increment y \\n    } \\n    glEnd(); \\n} \\n \\n// Display function for OpenGL \\nvoid display() { \\n    glClear(GL_COLOR_BUFFER_BIT);'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 21, 'page_label': '22', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glColor3f(1.0, 0.0, 0.0);  // Red color \\n    drawDDA(-0.5, -0.5, 0.5, 0.5);  // Draw a diagonal line \\n     \\n    glFlush(); \\n} \\n \\n// Initialize OpenGL settings \\nvoid init() { \\n    glClearColor(1.0, 1.0, 1.0, 1.0);  // Set background to white \\n    glMatrixMode(GL_PROJECTION); \\n    gluOrtho2D(-1, 1, -1, 1);  // Set 2D coordinate system \\n} \\n \\n// Main function to set up GLUT and OpenGL \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"DDA Line Drawing Algorithm\"); \\n \\n    init(); \\n    glutDisplayFunc(display); \\n    glutMainLoop(); \\n    return 0; \\n} \\n \\nMerits of DDA Algorithm \\n   Fast and efficient: Uses simple addition operations instead of complex multiplications. \\n   Straightforward implementation: Easy to implement with basic calculations. \\n   Works for all slopes: Can draw lines with any inclination.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 22, 'page_label': '23', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Limitations of DDA Algorithm \\n  Floating-point rounding errors: Since floating-point numbers are used, rounding may introduce \\nsmall inaccuracies. \\n  Slower than Bresenham’s Algorithm: Uses floating-point operations which can be \\ncomputationally expensive. \\n  Not perfectly optimized for raster displays: May produce gaps or jagged lines if not rounded \\nproperly. \\n \\nConclusion \\nThe DDA algorithm is a simple and effective method for drawing lines in computer graphics. It \\nefficiently interpolates pixel positions using incremental calculations, making it widely used in \\ngraphics rendering. However, for performance-critical applications, Bresenham’s Line Algorithm is \\noften preferred due to its integer-based calculations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content=\"(8 marks) \\nApply the concept of clipping and the importance of the clipping window in computer graphics. \\nExplain how clipping is performed in OpenGL, including the types of clipping available (point, line, \\npolygon clipping).(8) provide detailed answer \\nChatGPT said: \\nClipping in Computer Graphics \\nIntroduction \\nClipping is a crucial process in computer graphics that determines which parts of objects (such as \\npoints, lines, and polygons) are visible within a designated region, called the clipping window. Any \\nportions that lie outside this window are removed to improve rendering efficiency and maintain \\ncorrect visualization. \\nClipping is essential for: \\n• Optimizing performance by eliminating unnecessary computations. \\n• Ensuring a proper display by keeping only the relevant portions of objects. \\n• Managing object visibility within a viewport or screen. \\n \\nConcept of the Clipping Window \\nA clipping window is a rectangular or defined boundary that sets the limits for rendering objects. \\nAny graphical entity outside this boundary is clipped (discarded). This boundary could represent a \\nviewport, screen area, or a defined viewing region in a graphical application. \\nThe clipping window is typically defined in: \\n• 2D space (rectangular) – Used in raster graphics for UI elements, maps, and simple \\nrendering. \\n• 3D space (viewing frustum) – Used in 3D graphics to define the camera's perspective. \\n \\nTypes of Clipping in Computer Graphics \\nClipping is applied to different types of graphical objects, and the most common types include: \\n1. Point Clipping \\n• Determines whether a point lies inside or outside the clipping region. \\n• A point (x,y)(x, y)(x,y) is visible if it satisfies the window boundaries: \\nxmin≤x≤xmax,ymin≤y≤ymaxx_{\\\\text{min}} \\\\leq x \\\\leq x_{\\\\text{max}}, \\\\quad y_{\\\\text{min}} \\n\\\\leq y \\\\leq y_{\\\\text{max}}xmin≤x≤xmax,ymin≤y≤ymax  \\n• Used in applications like point-based selections (mouse clicks, scatter plots, etc.). \\n2. Line Clipping (Cohen-Sutherland & Liang-Barsky Algorithms)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='• Determines whether a line is fully visible, partially visible, or completely outside the \\nclipping region. \\n• If partially visible, the intersecting points are computed, and only the visible segment is \\ndrawn. \\n• Algorithms used:  \\no Cohen-Sutherland Line Clipping – Uses region codes to classify line segments. \\no Liang-Barsky Algorithm – More efficient as it reduces computations using parametric \\nequations. \\n3. Polygon Clipping (Sutherland-Hodgman Algorithm) \\n• Used to clip polygons against the viewport. \\n• The Sutherland-Hodgman algorithm clips a polygon edge-by-edge against the clipping \\nwindow. \\n• Produces new clipped vertices to form a visible polygon. \\n• Used in maps, gaming, and rendering optimizations. \\n4. Text Clipping \\n• Ensures that only the visible portion of text is rendered within a defined region. \\n• Used in UI development (text fields, labels). \\n5. Curve Clipping \\n• Similar to line clipping but applied to curved shapes like circles, ellipses, and Bezier curves. \\n• Uses bounding boxes or incremental approximations. \\n \\nClipping in OpenGL \\nOpenGL provides automatic clipping based on the defined viewing volume. The clipping process \\noccurs in Normalized Device Coordinates (NDC), meaning anything outside the canonical clipping \\nvolume (from -1 to 1 in X, Y , and Z) is discarded. \\nHow Clipping is Performed in OpenGL \\n1. Defining the Clipping Region \\nOpenGL uses glOrtho() or gluOrtho2D() for 2D clipping and glFrustum() for 3D perspective \\nclipping. \\ncpp \\nCopyEdit \\nglMatrixMode(GL_PROJECTION); \\nglLoadIdentity(); \\ngluOrtho2D(-100, 100, -100, 100);  // Defines a clipping window from (-100, -100) to (100, 100)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='2. Applying Viewport Transformation \\nThe viewport maps the clipping region to a screen coordinate system: \\ncpp \\nCopyEdit \\nglViewport(0, 0, 800, 600); \\n3. Automatic Clipping by OpenGL \\no Any primitives outside the defined view volume are automatically clipped. \\no OpenGL uses a clipping plane to discard objects outside the NDC space. \\n4. Custom Clipping Planes in OpenGL \\nOpenGL allows defining custom clipping planes using glClipPlane(): \\ncpp \\nCopyEdit \\ndouble planeEq[] = {1.0, 0.0, 0.0, -50.0};  // Clipping plane along x = 50 \\nglEnable(GL_CLIP_PLANE0); \\nglClipPlane(GL_CLIP_PLANE0, planeEq); \\n \\nExample: Line Clipping using Cohen-Sutherland Algorithm in OpenGL \\nBelow is a C++ program that implements line clipping using the Cohen-Sutherland algorithm in \\nOpenGL. \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n#include <iostream> \\n \\nusing namespace std; \\n \\n// Clipping window boundaries \\n#define X_MIN -50 \\n#define X_MAX 50 \\n#define Y_MIN -50 \\n#define Y_MAX 50'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 26, 'page_label': '27', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='// Region codes \\nconst int INSIDE = 0, LEFT = 1, RIGHT = 2, BOTTOM = 4, TOP = 8; \\n \\n// Function to compute region code for a point (x, y) \\nint computeCode(float x, float y) { \\n    int code = INSIDE; \\n    if (x < X_MIN) code |= LEFT; \\n    else if (x > X_MAX) code |= RIGHT; \\n    if (y < Y_MIN) code |= BOTTOM; \\n    else if (y > Y_MAX) code |= TOP; \\n    return code; \\n} \\n \\n// Cohen-Sutherland Line Clipping Algorithm \\nvoid cohenSutherlandClip(float x1, float y1, float x2, float y2) { \\n    int code1 = computeCode(x1, y1); \\n    int code2 = computeCode(x2, y2); \\n    bool accept = false; \\n \\n    while (true) { \\n        if ((code1 == 0) && (code2 == 0)) { \\n            accept = true; \\n            break; \\n        } else if (code1 & code2) { \\n            break; \\n        } else { \\n            int code_out; \\n            float x, y; \\n \\n            code_out = (code1 != 0) ? code1 : code2;'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='if (code_out & TOP) { \\n                x = x1 + (x2 - x1) * (Y_MAX - y1) / (y2 - y1); \\n                y = Y_MAX; \\n            } else if (code_out & BOTTOM) { \\n                x = x1 + (x2 - x1) * (Y_MIN - y1) / (y2 - y1); \\n                y = Y_MIN; \\n            } else if (code_out & RIGHT) { \\n                y = y1 + (y2 - y1) * (X_MAX - x1) / (x2 - x1); \\n                x = X_MAX; \\n            } else if (code_out & LEFT) { \\n                y = y1 + (y2 - y1) * (X_MIN - x1) / (x2 - x1); \\n                x = X_MIN; \\n            } \\n \\n            if (code_out == code1) { \\n                x1 = x; y1 = y; code1 = computeCode(x1, y1); \\n            } else { \\n                x2 = x; y2 = y; code2 = computeCode(x2, y2); \\n            } \\n        } \\n    } \\n \\n    if (accept) { \\n        glBegin(GL_LINES); \\n        glVertex2f(x1, y1); \\n        glVertex2f(x2, y2); \\n        glEnd(); \\n    } \\n} \\n \\n// Display function'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='void display() { \\n    glClear(GL_COLOR_BUFFER_BIT); \\n    glColor3f(1.0, 0.0, 0.0); \\n    cohenSutherlandClip(-70, -30, 60, 40); \\n    glFlush(); \\n} \\n \\n// Initialize OpenGL \\nvoid init() { \\n    gluOrtho2D(-100, 100, -100, 100); \\n} \\n \\n// Main function \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"Line Clipping - Cohen Sutherland\"); \\n \\n    init(); \\n    glutDisplayFunc(display); \\n    glutMainLoop(); \\n    return 0; \\n} \\n \\nConclusion \\nClipping is an essential technique in computer graphics that improves performance by removing \\nunnecessary parts of objects. OpenGL provides built-in clipping for optimized rendering, while \\nadvanced algorithms like Cohen-Sutherland and Sutherland-Hodgman offer efficient clipping \\nsolutions for lines and polygons.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. How MapReduce Computation is Executed? \\nMapReduce executes in two main stages: Map and Reduce. \\n1. Map Stage: The input data is split into chunks and processed by Mapper tasks in parallel. \\nEach Mapper processes a portion of the data and outputs key-value pairs. \\n2. Shuffle & Sort: The output of Mappers is sorted and grouped by key. The data is then \\nshuffled to appropriate Reducers. \\n3. Reduce Stage: The Reducer aggregates and processes data by key, generating the final \\noutput. \\nExample: Word count in MapReduce: \\n• Mapper: (\"Hadoop is great\") → [(Hadoop, 1), (is, 1), (great, 1)] \\n• Shuffle & Sort: Groups values by keys → [(Hadoop, [1,1,1]), (great, [1]), (is, [1,1])] \\n• Reducer: Aggregates counts → [(Hadoop, 3), (great, 1), (is, 2)] \\nMapReduce computation is executed through two main phases: a Map phase where data is \\nprocessed in parallel, and a Reduce phase where the results from the Map phase are aggregated and \\nprocessed to produce a final output.  \\nHere\\'s a more detailed breakdown: \\n1. Map Phase: \\n• Input: \\nThe MapReduce framework takes a large dataset as input, which is typically stored in a distributed \\nfile system like HDFS (Hadoop Distributed File System).  \\n• Splitting: \\nThe input data is divided into smaller chunks or \"splits\" that can be processed independently by \\ndifferent nodes in a cluster.  \\n• Mapping: \\nEach split is assigned to a mapper process, which applies a user-defined \"map\" function to each \\nrecord within the split.  \\n• Key-Value Pairs: \\nThe map function typically transforms the input data into key-value pairs.  \\n• Intermediate Output: \\nThe mappers produce a set of intermediate key-value pairs as output, which are then passed to the \\nshuffle and sort phase.  \\n2. Shuffle and Sort Phase: \\n• Shuffling: \\nThe intermediate key-value pairs from all mappers are shuffled and partitioned based on the keys.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Sorting: \\nThe framework sorts the shuffled key-value pairs, ensuring that all pairs with the same key are \\ngrouped together.  \\n3. Reduce Phase: \\n• Input: The sorted key-value pairs are passed to the reducer processes.  \\n• Reducing: Each reducer receives a group of key-value pairs with the same key.  \\n• Aggregation: The user-defined \"reduce\" function processes these grouped values and \\nproduces a final set of key-value pairs as output.  \\n• Output: The final output from the reducers is stored in the distributed file system.  \\nIn summary, MapReduce is a distributed computing model that breaks down large data processing \\ntasks into smaller, parallelizable tasks (the map phase) and then combines the results (the reduce \\nphase) to produce a final output.  \\n2. Show the key advantages in Hadoop. \\nHadoop offers several key advantages, including scalability for handling massive \\ndatasets, cost-effectiveness through commodity hardware, fault tolerance with \\ndata replication, and flexibility in handling various data types.  \\nHere\\'s a more detailed breakdown of Hadoop\\'s key advantages: \\n• Scalability: \\nHadoop\\'s architecture allows for horizontal scaling, meaning you can add more \\nnodes to the cluster to handle increasing data volumes and processing needs \\nwithout significant infrastructure changes.  \\n• Cost-Effectiveness: \\nHadoop is designed to run on inexpensive, standard hardware, making it a cost-\\neffective solution compared to traditional, expensive data storage and processing \\nsystems.  \\n• Fault Tolerance: \\nHadoop replicates data across multiple nodes, ensuring that data is not lost if one \\nor more nodes fail.  \\n• Flexibility: \\nHadoop can handle various data types, including structured, semi-structured, and \\nunstructured data, making it a versatile platform for diverse data analytics needs.  \\n• Data Locality: \\nHadoop\\'s architecture allows for processing data where it resides, minimizing data \\nmovement and improving performance.  \\n• Open Source: \\nHadoop is an open-source project, meaning its source code is freely available and \\ncan be modified and distributed.  \\n• YARN (Yet Another Resource Negotiator):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='YARN is a resource management system that enables efficient resource allocation \\nand management within the Hadoop ecosystem, allowing for better utilization of \\ncluster resources.  \\n• Data Governance and Management: \\nHadoop provides tools and features for data governance, including metadata \\nmanagement, data lineage, and security controls.  \\n• Security: \\nHadoop offers robust security features, including authentication, authorization, \\nand encryption, to protect sensitive data.  \\n• Data Lake: \\nHadoop can be used as a foundation for building data lakes, which are centralized \\nrepositories for storing diverse data formats and sources.  \\n3. What is the purpose of the NodeManager in \\nYARN? \\nThe NodeManager in YARN is a per-machine agent responsible for launching and \\nmanaging containers on a worker node, monitoring their resource usage, and reporting \\nthis information to the ResourceManager \\nThe NodeManager is responsible for: \\n• Monitoring resource usage (CPU, memory) on a node. \\n• Managing containers for executing tasks. \\n• Reporting node health to the ResourceManager. \\nExample: In a Hadoop cluster, each server runs a NodeManager to manage local \\ncomputation tasks and communicate with the ResourceManager. \\n4. Explain the main components of YARN. \\n4. Main Components of YARN \\n1. ResourceManager (RM) – Allocates resources across the cluster. \\n2. NodeManager (NM) – Monitors nodes and manages task execution. \\n3. ApplicationMaster (AM) – Manages the execution of an application. \\n4. Container – A runtime environment that executes tasks. \\nExample: A Spark job requests resources from the RM, which assigns it to a \\nNodeManager to execute the tasks within containers. \\n5. How does MapReduce achieve parallel \\nprocessing?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 3, 'page_label': '4', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='MapReduce achieves parallel processing by dividing large datasets into smaller chunks, \\ndistributing these chunks across multiple machines for processing (the \"Map\" phase), \\nand then aggregating the results from these machines (the \"Reduce\" phase).  \\nHere\\'s a more detailed explanation: \\n• Data Splitting: \\nThe input data is split into smaller, manageable chunks, which can be processed \\nindependently by different machines.  \\n• Map Phase: \\nEach chunk of data is processed independently by a \"mapper\" function, which \\ntransforms the data into key-value pairs.  \\n• Reduce Phase: \\nThe \"reducer\" function then takes the key-value pairs generated by the mappers and \\naggregates them to produce the final output.  \\n• Parallelism: \\nThe Map and Reduce phases can be executed in parallel across multiple machines, \\nenabling efficient processing of large datasets.  \\n• Fault Tolerance: \\nMapReduce is designed to be fault-tolerant, meaning that if a machine fails, the \\nMapReduce framework can automatically reassign the tasks to other machines, \\nensuring that the job continues to run.  \\n• Hadoop Ecosystem: \\nMapReduce is a fundamental part of the Apache Hadoop ecosystem, a distributed \\ncomputing framework designed for processing large datasets.  \\n• Data Access and Storage: \\nMapReduce uses Hadoop Distributed File System (HDFS) for both input and output.  \\n6. What is the role of the \\'Mapper\\' in MapReduce? \\n• Takes input data and converts it into key-value pairs. \\n• Runs in parallel on multiple nodes. \\n• Sends intermediate output to the Reduce phase. \\nExample: In a sales dataset, the Mapper outputs (city, sales): \\n• Input: (\"New York, 200\"), (\"Los Angeles, 150\") \\n• Output: [(New York, 200), (Los Angeles, 150)]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"7.Justify the need for replication in HDFS. \\nHDFS data replication is the process of creating multiple copies (replicas) of data blocks \\nacross different DataNodes in the HDFS cluster. This redundancy ensures that if one or more \\nDataNodes fail, the data can still be accessed from the remaining replicas, providing high \\navailability and fault tolerance. \\n8. Purpose of Secondary NameNode in HDFS \\n• Periodically merges edit logs with the namespace image to prevent the NameNode \\nfrom being overloaded. \\n• Acts as a checkpointing mechanism but not a failover node. \\nExample: If the NameNode fails, the Secondary NameNode’s last checkpointed metadata \\ncan be used to restore the system. \\nIn HDFS, the secondary namenode's primary purpose is to act as a helper node for the \\nprimary namenode, performing periodic checkpoints of the namespace and merging edit \\nlogs with the fsimage to prevent the edit logs from growing too large, which can lead to \\nslow NameNode restarts.  \\n9,12. Explain the steps in map reduce algorithm. \\nThe MapReduce algorithm consists of two main phases: Map and Reduce, with an \\nintermediate shuffle and sort step. The map phase processes data into key-value pairs, \\nwhich are then shuffled and sorted before the reduce phase aggregates the data.  \\nHere's a more detailed breakdown: \\n1. Input Splitting:  \\n• The input data is split into smaller chunks, which are then assigned to different \\nworker nodes in the cluster. \\n• These chunks are processed independently by the mapper functions. \\n2. Map Phase: \\n• Each mapper function receives a chunk of data as input.  \\n• The mapper processes the data and produces a set of intermediate key-value pairs.  \\n• The output of the map phase is a list of key-value pairs.  \\n3. Shuffle and Sort:  \\n• The intermediate key-value pairs are grouped and shuffled based on their keys. \\n• The keys are sorted, ensuring that all occurrences of the same key are grouped \\ntogether. \\n4. Reduce Phase: \\n• The reducer functions receive the shuffled and sorted key-value pairs as input.  \\n• The reducer aggregates the values associated with the same key, producing the final \\noutput.  \\nSteps in the MapReduce Algorithm \\n1. Input Splitting – Data is divided into chunks. \\n2. Mapping – Mappers process chunks and output key-value pairs. \\n3. Shuffling & Sorting – Intermediate key-value pairs are grouped. \\n4. Reducing – Reducers aggregate data.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 5, 'page_label': '6', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Final Output – Processed data is written to HDFS. \\nExample: Counting occurrences of words in a text file follows these steps. \\n11. Differentiate between HDFS and a traditional file \\nsystem. \\nFeature HDFS Traditional File System \\nStorage Type Distributed Local disk storage \\nFault Tolerance High (Replication) Low \\nScalability Linear scaling Limited by disk size \\nData Processing Optimized for batch processing General purpose \\nExample: HDFS stores and processes logs from a web service, while a traditional file \\nsystem is used for local document storage. \\n13. How partitions are shuffled in map reduce. \\n• The Partitioner determines which Reducer a key-value pair is sent to. \\n• The Shuffle Phase transfers data from Mappers to appropriate Reducers. \\n• The Sorting Phase organizes data before being processed by the Reducers. \\nExample: In a sales dataset, a partitioner ensures all sales records for a city go to the same \\nReducer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='14.Illustrate the basic structure of the MapReduce \\nprogramming model.\\n \\n15 Marks: \\n1.Describe with relevant examples the 5Vs of Big Data. \\n5Vs of Big Data with Examples (Expanded 15 Marks Answer) \\nBig Data is defined by 5 key characteristics known as the 5Vs: Volume, Velocity, Variety, \\nVeracity, and Value. These attributes help in understanding how data is generated, \\nprocessed, stored, and analyzed across various industries. \\n \\n1. Volume (Size of Data) – 3 Marks \\nDefinition: \\nVolume refers to the sheer amount of data generated every second from different sources \\nlike social media, IoT devices, and financial transactions. As organizations collect and store \\nmassive datasets, traditional storage and processing methods become inadequate. \\nExample: \\n• Social Media: Facebook processes over 4 petabytes of data daily, including posts, \\nimages, and videos. \\n• E-Commerce: Amazon and Flipkart handle billions of customer transactions every \\nyear. \\n• Healthcare: Medical imaging data, such as MRI scans, generates terabytes of data \\nper hospital per year. \\nImpact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Requires scalable storage solutions like Hadoop Distributed File System (HDFS) and \\ncloud-based storage (AWS, Google Cloud, Azure). \\n• Traditional relational databases (RDBMS) struggle with such large data volumes, \\nleading to the adoption of NoSQL databases like Apache Cassandra and MongoDB. \\n \\n2. Velocity (Speed of Data Generation & Processing) – 3 Marks \\nDefinition: \\nVelocity represents the speed at which data is generated, collected, and processed. Some \\ndata streams require real-time processing to provide instant insights and responses. \\nExample: \\n• Stock Markets: Millions of trades per second require real-time analytics to detect \\nfluctuations. \\n• Autonomous Vehicles: Self-driving cars like Tesla process sensor data in \\nmilliseconds for quick decision-making. \\n• Google Search: Handles over 100,000 searches per second, requiring fast indexing \\nand retrieval. \\nImpact: \\n• Technologies like Apache Kafka, Apache Flink, and Spark Streaming are used for \\nreal-time data processing. \\n• Businesses use real-time analytics for fraud detection, recommendation systems, \\nand operational efficiency. \\n \\n3. Variety (Different Data Types) – 3 Marks \\nDefinition: \\nVariety refers to the different formats and sources of data—structured, semi-structured, \\nand unstructured. Unlike traditional databases that handle only structured data, Big Data \\nsystems must process diverse formats. \\nExample: \\n• Structured Data: Relational databases storing customer details, order history \\n(MySQL, PostgreSQL). \\n• Semi-Structured Data: JSON or XML logs generated by web applications, sensors, \\nand IoT devices. \\n• Unstructured Data:  \\no YouTube processes over 500 hours of video uploads every minute. \\no Social media generates billions of posts, images, and comments daily. \\no Audio recordings from customer support call centers need sentiment \\nanalysis. \\nImpact: \\n• NoSQL databases like MongoDB (document-based) and Apache Cassandra (column-\\nbased) allow flexible data models. \\n• AI models process unstructured data (text, images, audio) for fraud detection, \\nsentiment analysis, and medical diagnosis. \\n \\n4. Veracity (Data Accuracy & Trustworthiness) – 3 Marks \\nDefinition: \\nVeracity concerns the quality, reliability, and accuracy of data. Big Data is often messy, \\ncontaining inconsistencies, errors, or biases, making data cleaning essential.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Example: \\n• Fake News Detection: Social media platforms like Facebook use AI to filter out false \\ninformation. \\n• Financial Transactions: Banks use fraud detection models to analyze suspicious \\ntransactions and prevent fraud. \\n• Healthcare Data: Medical datasets must be cleaned and verified before being used \\nin AI-driven diagnosis models. \\nImpact: \\n• ETL (Extract, Transform, Load) pipelines clean and structure raw data before \\nanalysis. \\n• Machine learning models rely on high-quality, unbiased datasets to avoid inaccurate \\npredictions. \\n• Data governance policies ensure compliance with regulations like GDPR and HIPAA. \\n \\n5. Value (Business & Analytical Insights) – 3 Marks \\nDefinition: \\nValue is the most critical aspect of Big Data—it refers to extracting meaningful insights that \\ndrive business decisions. Without proper analysis, raw data is just a storage burden. \\nExample: \\n• Netflix’s Recommendation System: Uses machine learning on user data to suggest \\npersonalized content. \\n• Retail Industry: Walmart predicts demand based on customer purchases and adjusts \\nstock levels. \\n• Healthcare AI: Predictive analytics helps doctors diagnose diseases earlier based on \\nlarge datasets. \\nImpact: \\n• Data-driven decision-making leads to higher efficiency and revenue. \\n• AI-powered insights enable better customer experiences, fraud detection, and \\noperational automation. \\n• Companies use big data analytics tools like Hadoop, Spark, and Tableau to derive \\nbusiness intelligence. \\n \\nConclusion \\nThe 5Vs of Big Data—Volume, Velocity, Variety, Veracity, and Value—are essential for \\nhandling large-scale data efficiently. Businesses leverage advanced cloud computing, AI, \\nand NoSQL databases to process, analyze, and gain insights from Big Data. \\n2,3. Describe the importance of tools in Big Data with \\nemphasis on the trends and technology in big data. \\nImportance of Big Data Tools with Trends & Technology (Expanded 15 Marks Answer) \\nThe explosion of Big Data has transformed industries, requiring advanced tools to efficiently \\nstore, process, and analyze vast datasets. Traditional databases fail to handle the Volume, \\nVariety, and Velocity of modern data, making Big Data tools essential for decision-making, \\nautomation, and real-time analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Importance of Big Data Tools – 5 Marks \\nBig Data tools offer: \\n   Efficient Data Management – Handling petabytes of structured, semi-structured, and \\nunstructured data. \\n   Scalability & Flexibility – Supporting horizontal scaling for cloud-based and on-premise \\nsystems. \\n   Real-time Processing – Enabling businesses to make instant decisions based on live data \\nstreams. \\n   Enhanced Security & Compliance – Implementing encryption and access control (GDPR, \\nHIPAA). \\n   AI & Machine Learning Integration – Powering intelligent automation and predictive \\nanalytics. \\n     Example: Facebook uses Apache Hadoop for storing and analyzing user interactions \\nacross its platform, improving targeted advertising. \\n \\n2. Key Big Data Tools & Technologies – 5 Marks \\nA. Data Storage & Management \\n  Hadoop Distributed File System (HDFS) – Scalable, fault-tolerant storage. \\n  Apache Cassandra – A NoSQL database optimized for high-speed transactions. \\n  Google BigQuery – Serverless cloud warehouse for massive data analysis. \\n     Example: Spotify uses BigQuery to analyze user streaming habits and personalize \\nplaylists. \\nB. Data Processing & Analytics \\n  Apache Spark – Fast in-memory processing for large datasets. \\n  Apache Flink – Real-time stream processing for instant decision-making. \\n  TensorFlow & PyTorch – AI-driven analytics for deep learning and automation. \\n     Example: Uber uses Apache Flink to process real-time ride requests, optimizing driver \\navailability. \\nC. Data Visualization & Business Intelligence (BI) \\n  Tableau – Interactive dashboards for business reporting. \\n  Power BI – Microsoft’s AI-powered analytics tool. \\n  Google Data Studio – Cloud-based visualization platform. \\n     Example: Coca-Cola uses Tableau to track supply chain efficiency and sales trends.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='3. Emerging Trends in Big Data – 5 Marks \\n        A. AI & Machine Learning Integration – Automating decision-making with predictive \\nanalytics. \\n        B. Edge Computing & IoT – Processing data closer to devices for real-time insights. \\n        C. Serverless & Cloud Computing – AWS, Azure, and Google Cloud enabling scalable \\nanalytics. \\n        D. Blockchain for Data Integrity – Securing transactions in finance and healthcare. \\n        E. Quantum Computing in Big Data – Unlocking new possibilities in large-scale data \\nprocessing. \\n     Example: Tesla leverages Edge AI & IoT to analyze autonomous driving data instantly, \\nimproving vehicle safety. \\n4,5. Illustrate the main features and evolution of big \\ndata in detail. provide answer for 15 marks \\nChatGPT said: \\nMain Features and Evolution of Big Data (15 Marks Answer) \\nBig Data refers to extremely large and complex datasets that require advanced techniques \\nfor processing, analyzing, and extracting insights. It has evolved significantly due to \\ntechnological advancements, increased digitalization, and the growing need for real-time \\ndecision-making. \\n \\n1. Main Features of Big Data – 5 Marks \\nBig Data is characterized by the 5Vs, which define its scale, complexity, and value: \\n1. Volume \\n• Definition: Refers to the enormous size of data generated daily from sources like \\nsocial media, IoT, and business transactions. \\n• Example: Facebook processes over 4 petabytes of data daily. \\n2. Velocity \\n• Definition: The speed at which data is generated and processed in real time. \\n• Example: Stock market trading systems process millions of transactions per second. \\n3. Variety \\n• Definition: Data exists in multiple formats (structured, semi-structured, \\nunstructured). \\n• Example: Emails, social media posts, IoT sensor data, and videos.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"4. Veracity \\n• Definition: The reliability and accuracy of data for meaningful decision-making. \\n• Example: Fake news detection in social media analytics. \\n5. Value \\n• Definition: Extracting meaningful insights and business advantages from data. \\n• Example: Netflix analyzes user preferences to recommend personalized content. \\n \\n2. Evolution of Big Data – 5 Marks \\nBig Data has evolved through different stages based on technological advancements: \\n1. Early Data Management (1960s - 1980s) \\n• Traditional relational databases (RDBMS) like Oracle and IBM DB2 emerged. \\n• Limited data storage and computing power. \\n2. Rise of the Internet (1990s - 2000s) \\n• Explosion of digital data with the growth of e-commerce, social media, and cloud \\ncomputing. \\n• Google developed MapReduce (2004) & Hadoop (2006) to process large datasets \\nefficiently. \\n3. Big Data Boom (2010s - Present) \\n• Technologies like Apache Spark, NoSQL, and real-time streaming gained traction. \\n• AI-driven analytics transformed industries like healthcare, finance, and marketing. \\n4. Future Trends (2025 & Beyond) \\n• Quantum computing to enhance Big Data processing. \\n• Edge computing for real-time analytics in IoT. \\n• Blockchain for secure data transactions. \\n     Example: Tesla's self-driving cars leverage Big Data to process real-time road \\nconditions. \\n \\n3. Impact of Big Data on Industries – 5 Marks \\n  Healthcare – AI-driven diagnostics (IBM Watson). \\n  Retail – Personalized recommendations (Amazon, Flipkart).\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Banking & Finance – Fraud detection (PayPal, Mastercard). \\n  Smart Cities – Traffic and energy management (Google Maps, Smart Grids). \\n \\nConclusion \\nBig Data has evolved from simple data management to AI-driven analytics, shaping \\nmodern industries. Its key features (5Vs) and technological advancements continue to \\ndrive innovation, automation, and smarter decision-making. Businesses leveraging Big \\nData will gain a competitive edge in the digital era. \\n6. Imagine you are a data scientist in 2025. How would \\nyou leverage the evolution of big data to solve a \\nspecific business problem in your industry?  \\nLeveraging Big Data Evolution to Solve a Business Problem (2025) \\nIntroduction \\nIn 2025, the evolution of big data analytics has transformed business decision-making. As \\na data scientist, I would leverage big data to solve a key problem: customer churn \\nprediction in e-commerce. Retaining customers is more profitable than acquiring new \\nones, and advanced data analytics can help predict and prevent churn. \\n \\n1. Evolution of Big Data & Its Role in Business \\nBig data has evolved in several ways: \\n• Scalability: Cloud-based solutions (AWS, Google BigQuery) enable real-time \\nprocessing. \\n• Speed: Technologies like Apache Spark process data faster than traditional systems. \\n• AI Integration: Machine learning & deep learning provide predictive insights. \\n• Diverse Data Sources: Data comes from IoT devices, social media, and web \\nanalytics. \\nThese advancements allow businesses to understand customer behavior, optimize \\nmarketing, and improve decision-making. \\n \\n2. How Big Data Analytics Solves Customer Churn \\na) Data Collection & Integration \\n• Structured Data: Purchase history, transaction records, and product reviews.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Unstructured Data: Customer sentiment from social media & chat support. \\n• Example: Netflix analyzes user watch history & engagement to predict \\ncancellations. \\nb) Predictive Analytics & Machine Learning \\n• Models like Random Forest, XGBoost, and Deep Learning identify churn patterns. \\n• Real-time processing (Apache Kafka, Spark) ensures quick insights. \\n• Example: Amazon uses ML to detect customers likely to switch to competitors. \\nc) AI-Driven Personalization \\n• Chatbots & recommendation engines offer real-time solutions. \\n• Sentiment analysis detects negative reviews & triggers retention strategies. \\n• Example: Spotify sends personalized playlists to keep users engaged. \\nd) Proactive Customer Retention Strategies \\n• Targeted discounts & loyalty programs for at-risk customers. \\n• Predictive alerts for sales teams to engage potential churners. \\n• Example: E-commerce platforms send discount codes to inactive users. \\n \\n3. Business Benefits & Impact \\n   Increased customer retention, reducing marketing costs. \\n   Higher revenue through personalized engagement strategies. \\n   Improved customer experience, enhancing brand loyalty. \\n \\nConclusion \\nBy leveraging big data analytics, AI, and real-time processing, businesses can predict \\ncustomer churn, engage users effectively, and drive growth. The evolution of big data \\nenables faster, smarter, and more personalized decision-making, ensuring a competitive \\nedge in 2025.         \\n7. Summarize the various industrial use cases of Big \\nData with examples. \\nIndustrial Use Cases of Big Data with Examples \\nIntroduction'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Big Data refers to the vast volumes of structured and unstructured data generated at high \\nvelocity from multiple sources. Organizations across industries harness big data \\ntechnologies such as Hadoop, Apache Spark, AI-driven analytics, and cloud computing to \\nextract valuable insights, improve efficiency, and make data-driven decisions. \\n \\n1. Healthcare & Pharmaceuticals \\nUse Case: Predictive Analytics and Personalized Medicine \\n• Example: IBM Watson Health leverages big data to analyze medical records and \\nassist doctors in diagnosing diseases like cancer at early stages. \\n• Impact:  \\no Enhanced patient care through early disease detection. \\no Personalized treatment plans based on patient history. \\no Faster drug discovery through AI-powered simulations. \\n \\n2. E-Commerce & Retail \\nUse Case: Customer Behavior Analysis & Demand Forecasting \\n• Example: Amazon and Flipkart use big data to analyze customer browsing and \\npurchase history, offering personalized product recommendations. \\n• Impact:  \\no Increased sales through targeted advertisements. \\no Optimized inventory management to reduce wastage. \\no Better understanding of customer preferences and trends. \\n \\n3. Finance & Banking \\nUse Case: Fraud Detection & Risk Management \\n• Example: Banks like JPMorgan Chase use machine learning algorithms to monitor \\ntransactions and detect anomalies indicating fraud. \\n• Impact:  \\no Reduction in fraudulent activities and cyber threats. \\no Enhanced customer trust through secure banking services. \\no Improved credit scoring and loan approval processes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Manufacturing & Supply Chain \\nUse Case: Predictive Maintenance & Process Optimization \\n• Example: General Electric (GE) uses IoT sensors and AI-powered analytics to predict \\nequipment failures and schedule maintenance. \\n• Impact:  \\no Reduced operational downtime and maintenance costs. \\no Increased production efficiency and output. \\no Smart inventory management and supply chain optimization. \\n \\n5. Telecommunications \\nUse Case: Network Optimization & Customer Retention \\n• Example: Vodafone and AT&T use big data to analyze call records, predict network \\ncongestion, and provide seamless connectivity. \\n• Impact:  \\no Improved network efficiency and reduced service disruptions. \\no Enhanced customer satisfaction through proactive troubleshooting. \\no Minimized customer churn by offering tailored data plans. \\n \\n6. Transportation & Smart Cities \\nUse Case: Traffic Management & Route Optimization \\n• Example: Google Maps utilizes real-time GPS data and big data analytics to provide \\noptimal travel routes and predict traffic conditions. \\n• Impact:  \\no Reduced traffic congestion and travel delays. \\no Efficient public transport scheduling and urban planning. \\no Safer roads through predictive accident analysis. \\n \\n7. Education & EdTech \\nUse Case: Adaptive Learning & Student Performance Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"• Example: Platforms like Coursera and Udemy use AI-driven analytics to track \\nstudent progress and recommend personalized learning paths. \\n• Impact:  \\no Enhanced learning outcomes through customized courses. \\no Early identification of struggling students for targeted assistance. \\no Data-driven improvements in course content and teaching strategies. \\n \\n8. Entertainment & Media \\nUse Case: Content Recommendation & Audience Insights \\n• Example: Netflix and Spotify analyze user preferences to offer personalized movie, \\nmusic, and podcast recommendations. \\n• Impact:  \\no Increased viewer engagement and retention. \\no Improved content production based on audience trends. \\no Higher advertising revenue through targeted ads. \\n \\n9. Agriculture & Farming \\nUse Case: Precision Farming & Yield Prediction \\n• Example: John Deere uses big data to analyze soil conditions, weather patterns, \\nand crop health for smart farming decisions. \\n• Impact:  \\no Increased crop yields through data-driven farming techniques. \\no Reduced resource wastage (water, fertilizers, and pesticides). \\no Early detection of plant diseases to prevent large-scale losses. \\n \\n10. Energy & Utilities \\nUse Case: Smart Grid Management & Energy Optimization \\n• Example: Tesla's Powerwall and smart grid systems use big data to predict energy \\ndemand and optimize electricity distribution. \\n• Impact:  \\no Reduced energy waste and improved efficiency.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='o Integration of renewable energy sources for sustainability. \\no Lower electricity costs for consumers. \\n \\nConclusion \\nBig Data is revolutionizing industries by driving efficiency, enhancing customer experience, \\nand enabling predictive decision-making. The continuous evolution of big data analytics, \\nAI, and IoT will further transform businesses, optimize operations, and shape the future of \\ndigital innovation.         \\n8. Identify the challenges of the Big Data applications \\nin Modern Data Analytics.  \\nChatGPT said: \\nChallenges of Big Data Applications in Modern Data Analytics \\nBig Data has transformed industries by enabling large-scale data processing, AI-driven \\ninsights, and real-time decision-making. However, its implementation faces significant \\nchallenges that organizations must address. \\n \\n1. Data Quality & Inconsistency \\n• Challenge: Inaccurate, incomplete, or redundant data affects analytical outcomes. \\n• Impact: Poor data quality leads to unreliable insights and flawed decision-making. \\n• Solution: Implement data cleaning, validation techniques, and ETL (Extract, \\nTransform, Load) processes. \\n \\n2. Data Storage & Management \\n• Challenge: Handling massive volumes of structured and unstructured data requires \\nefficient storage solutions. \\n• Impact: High storage costs and inefficient data retrieval slow down analytics. \\n• Solution: Utilize distributed storage systems like Hadoop HDFS, Amazon S3, and \\nGoogle BigQuery. \\n \\n3. Scalability & Processing Speed \\n• Challenge: Traditional databases struggle to scale and process real-time data \\nefficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Impact: Delayed insights reduce the effectiveness of time-sensitive analytics. \\n• Solution: Implement parallel computing, cloud-based architectures, and real-time \\nprocessing tools like Apache Spark. \\n \\n4. Data Security & Privacy \\n• Challenge: Sensitive data is vulnerable to breaches, cyberattacks, and unauthorized \\naccess. \\n• Impact: Non-compliance with data regulations (GDPR, HIPAA) can result in legal \\npenalties and loss of customer trust. \\n• Solution: Use encryption, access control, and blockchain-based security \\nframeworks. \\n \\n5. Integration of Diverse Data Sources \\n• Challenge: Big Data comes from multiple sources (IoT, social media, enterprise \\nsystems), leading to integration complexities. \\n• Impact: Data silos prevent seamless analytics and decision-making. \\n• Solution: Employ data lakes, APIs, and middleware solutions for interoperability. \\n \\n6. High Computational Costs \\n• Challenge: Running machine learning models and large-scale analytics requires \\nsignificant computational power. \\n• Impact: Expensive cloud resources and infrastructure investments can limit \\nscalability. \\n• Solution: Optimize data pipelines, use edge computing, and implement cost-\\nefficient AI models. \\n \\n7. Talent Shortage & Skill Gaps \\n• Challenge: Organizations struggle to find skilled professionals in Big Data, AI, and \\ncloud technologies. \\n• Impact: Delays in project implementation and reduced efficiency in analytics. \\n• Solution: Invest in employee training, AI-driven automation, and low-code/no-code \\nanalytics platforms.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='8. Ethical & Bias Issues \\n• Challenge: AI-driven analytics can inherit biases from historical data, leading to \\nunfair outcomes. \\n• Impact: Discrimination in hiring, finance, and healthcare decisions. \\n• Solution: Ensure diverse datasets, apply bias detection algorithms, and enforce \\nethical AI guidelines. \\n \\n9. Real-time Analytics Complexity \\n• Challenge: Processing high-velocity data streams from IoT, stock markets, and social \\nmedia requires advanced real-time analytics. \\n• Impact: Delays in detecting fraud, monitoring system failures, and responding to \\ncustomer needs. \\n• Solution: Leverage real-time processing frameworks like Apache Kafka, Apache \\nFlink, and in-memory databases. \\n \\n10. Compliance & Regulatory Challenges \\n• Challenge: Governments enforce strict regulations on data usage, retention, and \\ncross-border transfers. \\n• Impact: Non-compliance results in heavy fines and operational disruptions. \\n• Solution: Adopt transparent data governance policies, comply with GDPR, CCPA, \\nand industry standards. \\n \\nConclusion \\nAddressing these challenges is crucial for businesses to fully leverage Big Data analytics. By \\nadopting advanced AI, cloud computing, security protocols, and ethical AI practices, \\norganizations can unlock the true potential of Big Data for competitive advantage.         \\n9. Explain the big data architecture for \\nhandling large-scale social media analytics.  \\nChatGPT said: \\nBig Data Architecture for Handling Large-Scale Social Media Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Social media generates vast amounts of high-velocity, high-variety data. A robust Big Data \\nArchitecture is essential to process, analyze, and extract meaningful insights in real-time. \\n \\n1. Data Sources \\nSocial media data originates from: \\n• Platforms: Facebook, Twitter, Instagram, LinkedIn, TikTok, etc. \\n• Media Types: Text posts, images, videos, comments, reactions, hashtags. \\n• APIs & Streams: Twitter API, Facebook Graph API, YouTube Data API. \\n \\n2. Data Ingestion Layer \\nResponsible for collecting real-time and batch data. \\n• Tools: Apache Kafka, Apache Flume, AWS Kinesis, Google Pub/Sub. \\n• Process: Streams large volumes of unstructured and semi-structured data for \\nfurther processing. \\n \\n3. Storage Layer \\nStores raw, processed, and structured data efficiently. \\n• Distributed Storage: Hadoop HDFS, Amazon S3, Google Cloud Storage. \\n• NoSQL Databases: MongoDB, Cassandra for semi-structured and unstructured data. \\n• Columnar Databases: Apache HBase for fast read/write access. \\n \\n4. Processing & Analytics Layer \\nHandles real-time and batch processing for insights extraction. \\n• Batch Processing: Apache Spark, Hadoop MapReduce for deep historical analysis. \\n• Real-time Processing: Apache Flink, Storm, or Spark Streaming for immediate trend \\ndetection. \\n• Machine Learning & AI: TensorFlow, PyTorch, Scikit-Learn for sentiment analysis, \\nuser behavior prediction. \\n \\n5. Querying & Data Access Layer \\nProvides interfaces to access, query, and manipulate data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• SQL Query Engines: Apache Hive, Presto, Google BigQuery for structured queries. \\n• Graph Databases: Neo4j for relationship analysis (e.g., influencer networks). \\n• Data Indexing: Elasticsearch for fast text searches and log analysis. \\n \\n6. Visualization & Business Intelligence Layer \\nConverts data into meaningful insights for decision-making. \\n• Tools: Tableau, Power BI, Kibana, Google Data Studio. \\n• Use Cases: Engagement metrics, hashtag trends, audience demographics, and real-\\ntime dashboards. \\n \\n7. Security & Governance Layer \\nEnsures compliance, privacy, and data integrity. \\n• Access Control: AWS IAM, Role-Based Access Control (RBAC). \\n• Data Privacy: GDPR, CCPA compliance for personal data protection. \\n• Encryption & Logging: TLS, SSL, and logging tools for secure access. \\n \\nConclusion \\nA well-structured Big Data architecture enables organizations to analyze large-scale social \\nmedia data, detect trends, enhance customer engagement, and improve business \\nstrategies in real time.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='CH : 2 15 Marks \\n1.If a file of 5 GB is uploaded to HDFS and the block size is 1 \\nGB, explain how HDFS will divide and store this file across \\nmultiple DataNodes. Assume replication factor of 2. \\nHDFS File Storage and Replication in a Distributed System (15 Marks) \\nWhen a 5 GB file is uploaded to HDFS with a block size of 1 GB and a replication factor of 2, \\nthe system handles the storage and replication as follows: \\n \\n1. File Splitting in HDFS \\nHDFS follows a distributed file storage system, where large files are divided into fixed-size \\nblocks. Since the block size is 1 GB, a 5 GB file will be split into 5 blocks, each of size 1 GB. \\nBlock Number Size \\nBlock 1 1 GB \\nBlock 2 1 GB \\nBlock 3 1 GB \\nBlock 4 1 GB \\nBlock 5 1 GB \\nThese blocks are then distributed across multiple DataNodes for parallel processing and \\nfault tolerance. \\n \\n2. Data Distribution Across DataNodes \\nHDFS follows a master-slave architecture, where the NameNode manages metadata, and \\nDataNodes store the actual blocks. \\nEach of the 5 blocks is stored on a different DataNode, and since the replication factor is 2, \\neach block has two copies stored on different nodes. \\nBlock Primary DataNode Replica DataNode \\nBlock 1 DataNode A DataNode D \\nBlock 2 DataNode B DataNode E \\nBlock 3 DataNode C DataNode F'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Block Primary DataNode Replica DataNode \\nBlock 4 DataNode D DataNode A \\nBlock 5 DataNode E DataNode B \\nThis replication ensures fault tolerance, meaning if one DataNode fails, another node has a \\ncopy. \\n \\n3. Advantages of HDFS Storage Strategy \\nHDFS ensures reliability, fault tolerance, and parallel processing by implementing the \\nfollowing strategies: \\n   Fault Tolerance: \\n• Since each block has 2 copies, data is not lost even if a node crashes. \\n• The NameNode monitors block health and re-replicates blocks in case of failures. \\n   Load Balancing & Scalability: \\n• Blocks are evenly distributed across multiple DataNodes, preventing overloading \\non a single machine. \\n• The system scales horizontally by adding more DataNodes when storage demand \\nincreases. \\n   Parallel Processing (MapReduce Support): \\n• Since different blocks are stored on multiple nodes, parallel processing using \\nMapReduce speeds up large-scale computations. \\n   High Availability: \\n• The NameNode tracks all block locations and helps in quick recovery if a node fails. \\n• HDFS ensures continuous access to data even during failures. \\n \\n4. Step-by-Step Process of Uploading a File to HDFS \\n1⃣ File Upload: The 5 GB file is uploaded to HDFS. \\n2⃣ Block Splitting: HDFS divides the file into 5 blocks (each 1 GB). \\n3⃣ DataNode Assignment: The NameNode assigns DataNodes to store each block. \\n4⃣ Replication: Each block is replicated twice across different DataNodes. \\n5⃣ Data Retrieval: When a client requests the file, blocks are fetched and reassembled.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Conclusion \\nHDFS follows an efficient, scalable, and fault-tolerant architecture to manage large files in \\ndistributed environments. By splitting the file into blocks, distributing them across nodes, \\nand ensuring replication, HDFS ensures high availability, reliability, and efficient processing \\nof big data workloads. \\n2. Demonstrate the working of Map Reduce algorithm \\nto find the frequency of words in a given input text. \\nprovide answer for 15 marks \\nChatGPT said: \\nDemonstration of MapReduce Algorithm for Word Frequency Calculation (15 Marks) \\n1. Introduction to MapReduce \\nMapReduce is a distributed data processing model that divides tasks into two main \\nphases: \\n  Map Phase: Processes input data and generates key-value pairs. \\n  Reduce Phase: Aggregates and summarizes results. \\nIn this example, we demonstrate how MapReduce calculates word frequency from a given \\ninput text. \\n \\n2. Input Example \\nConsider the following input text: \\n\"Big data is powerful. Big data is everywhere.\" \\n \\n3. Step-by-Step Execution of MapReduce Algorithm \\nStep 1: Map Phase \\n• The input text is split into words. \\n• Each word is emitted as a key-value pair (word, 1). \\nInput (Text Split into Words) Mapper Output (Key-Value Pairs) \\nBig (Big, 1) \\ndata (data, 1) \\nis (is, 1)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Input (Text Split into Words) Mapper Output (Key-Value Pairs) \\npowerful. (powerful, 1) \\nBig (Big, 1) \\ndata (data, 1) \\nis (is, 1) \\neverywhere. (everywhere, 1) \\n  Intermediate Output from Mapper: \\nscss \\nCopyEdit \\n(Big, 1)   \\n(data, 1)   \\n(is, 1)   \\n(powerful, 1)   \\n(Big, 1)   \\n(data, 1)   \\n(is, 1)   \\n(everywhere, 1)   \\n \\nStep 2: Shuffle & Sort Phase \\n• The MapReduce framework groups values by keys before passing them to the \\nreducer. \\nShuffled Key-Value Pairs \\n(Big, [1, 1]) \\n(data, [1, 1]) \\n(is, [1, 1]) \\n(powerful, [1]) \\n(everywhere, [1])'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 26, 'page_label': '27', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Step 3: Reduce Phase \\n• The Reducer aggregates the counts for each word. \\nReducer Input Reducer Output \\n(Big, [1, 1]) (Big, 2) \\n(data, [1, 1]) (data, 2) \\n(is, [1, 1]) (is, 2) \\n(powerful, [1]) (powerful, 1) \\n(everywhere, [1]) (everywhere, 1) \\n  Final Output from Reducer: \\nscss \\nCopyEdit \\n(Big, 2)   \\n(data, 2)   \\n(is, 2)   \\n(powerful, 1)   \\n(everywhere, 1)   \\n \\n4. Python Implementation of MapReduce Word Count \\nHere’s a simple Python program to demonstrate word frequency using MapReduce: \\npython \\nCopyEdit \\nfrom collections import defaultdict \\n# Sample input text \\ntext = \"Big data is powerful. Big data is everywhere.\" \\n# Step 1: Mapper Function \\ndef mapper(text): \\n    words = text.lower().replace(\".\", \"\").split()  # Convert to lowercase & split \\n    return [(word, 1) for word in words]  # Emit (word, 1) pairs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='# Step 2: Shuffle & Sort (Grouping by Key) \\ndef shuffle_sort(mapped_data): \\n    grouped_data = defaultdict(list) \\n    for word, count in mapped_data: \\n        grouped_data[word].append(count) \\n    return grouped_data \\n# Step 3: Reducer Function \\ndef reducer(grouped_data): \\n    return {word: sum(counts) for word, counts in grouped_data.items()}  # Summing counts \\n# Running MapReduce \\nmapped_data = mapper(text) \\ngrouped_data = shuffle_sort(mapped_data) \\nword_frequencies = reducer(grouped_data) \\n# Display Output \\nprint(\"Word Frequencies:\", word_frequencies) \\n  Expected Output: \\nyaml \\nCopyEdit \\nWord Frequencies: {\\'big\\': 2, \\'data\\': 2, \\'is\\': 2, \\'powerful\\': 1, \\'everywhere\\': 1} \\n \\n5. Key Advantages of MapReduce for Word Count \\n   Scalability: Handles massive datasets across multiple machines. \\n   Fault Tolerance: Ensures high availability in distributed systems. \\n   Parallel Processing: Efficiently splits work across nodes. \\n   Load Balancing: Distributes tasks evenly to optimize performance. \\n \\n6. Conclusion \\nThe MapReduce algorithm efficiently counts word occurrences in large-scale datasets. It \\nsplits text into words, maps them to key-value pairs, shuffles & sorts the data, and reduces \\nit to compute the final word frequencies. This approach is widely used in big data \\nanalytics, log processing, and text mining applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='3. Explain in detail about Hadoop YARN. \\nhttps://www.geeksforgeeks.org/hadoop-yarn-architecture/ \\nHadoop YARN (Yet Another Resource Negotiator) - Detailed Explanation (15 Marks) \\n1. Introduction to Hadoop YARN \\nHadoop YARN (Yet Another Resource Negotiator) is the resource management layer in the \\nHadoop ecosystem. It allows multiple applications to share cluster resources efficiently \\nwhile ensuring scalability, fault tolerance, and high availability. \\nKey Features: \\n  Decouples resource management from job scheduling. \\n  Supports multi-tenancy and dynamic resource allocation. \\n  Enables parallel execution of applications. \\n \\n2. YARN Architecture & Components \\nHadoop YARN consists of the following key components: \\n1. ResourceManager (RM) \\n• Master daemon responsible for managing resources across the cluster. \\n• Has two main components:  \\no Scheduler: Allocates resources based on policies (FIFO, Fair, Capacity). \\no ApplicationManager: Manages application lifecycle. \\n2. NodeManager (NM) \\n• Runs on each worker node and monitors resource usage. \\n• Communicates with ResourceManager and executes tasks in containers. \\n3. ApplicationMaster (AM) \\n• Created for each application and negotiates resources from RM. \\n• Monitors execution of tasks and handles failures. \\n4. Containers \\n• Isolated execution environments where actual tasks run. \\n• Dynamically allocated by RM based on CPU & memory requirements. \\n \\n3. Working of YARN'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Step 1: Application Submission \\n• A user submits an application (MapReduce, Spark, Hive) to YARN. \\nStep 2: Resource Allocation \\n• RM assigns containers to run the ApplicationMaster (AM) for that job. \\nStep 3: Task Execution \\n• AM requests resources from RM. \\n• NM launches containers and executes tasks. \\nStep 4: Monitoring & Completion \\n• AM monitors task execution and reports status to RM. \\n• Once completed, AM releases resources back to the cluster. \\n \\n4. Advantages of YARN \\n   Efficient Resource Utilization – Dynamic allocation of CPU & memory. \\n   Multi-Tenancy Support – Runs multiple frameworks like Spark, Tez, Flink, HBase. \\n   Scalability – Handles thousands of nodes efficiently. \\n   Fault Tolerance – Automatically reschedules failed tasks. \\n \\n5. Real-World Applications of YARN \\n  Big Data Processing – Supports MapReduce, Apache Spark, Apache Flink. \\n  Data Warehousing – Runs Apache Hive for SQL-based queries. \\n  Machine Learning & AI – Supports ML frameworks like TensorFlow on YARN. \\n  Streaming Applications – Works with Apache Storm & Kafka for real-time analytics. \\n \\n6. Conclusion \\nHadoop YARN efficiently manages cluster resources, enabling large-scale data processing. \\nIts scalability, fault tolerance, and flexibility make it a backbone for big data applications. \\n        \\n5. Explain about Hadoop distributed file system \\narchitecture with neat diagram. \\nHadoop Distributed File System (HDFS) Architecture \\n1. Introduction to HDFS'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Hadoop Distributed File System (HDFS) is a highly scalable, fault-tolerant, and \\ndistributed storage system designed for big data applications. It enables efficient \\nstorage and processing of massive datasets across a cluster of commodity hardware. \\nHDFS is inspired by the Google File System (GFS) and follows a master-slave \\narchitecture to manage large-scale data efficiently. \\n \\n2. Key Components of HDFS \\nHDFS consists of three main components: \\n1. NameNode (Master Node) \\n• Acts as the central metadata manager of the HDFS cluster. \\n• Tracks file structure, block locations, and namespace metadata. \\n• Does not store actual data but manages file system operations. \\n• A single point of failure unless High Availability (HA) is enabled. \\n2. DataNode (Worker Nodes) \\n• Stores actual data blocks on local disks. \\n• Periodically sends heartbeat signals to the NameNode to confirm availability. \\n• Performs read, write, replication, and deletion operations based on NameNode \\ninstructions. \\n3. Secondary NameNode \\n• Not a backup NameNode but assists in maintaining system consistency. \\n• Periodically saves checkpoints of the file system metadata. \\n \\n3. HDFS File Storage and Block Replication \\n• When a file is uploaded, HDFS splits it into fixed-size blocks (default 128MB or \\n256MB). \\n• Each block is stored across multiple DataNodes for redundancy and fault tolerance. \\n• Default replication factor is 3, meaning each block is replicated on three different \\nnodes. \\n  Example: \\nA 600MB file stored in HDFS with a block size of 128MB will be divided into 5 blocks: \\nB1 (128MB), B2 (128MB), B3 (128MB), B4 (128MB), B5 (88MB) \\nEach block will be replicated three times across different DataNodes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Data Flow in HDFS \\nA. Writing Data to HDFS \\n     Client → NameNode → Assigns DataNodes → Data is split & stored with \\nreplication. \\n• The client requests the NameNode to store a file. \\n• The NameNode allocates DataNodes to store the file in blocks. \\n• Data is written in parallel across multiple DataNodes with replication. \\nB. Reading Data from HDFS \\n     Client → NameNode → Fetches block locations → Reads directly from DataNodes. \\n• The client requests the NameNode to read a file. \\n• The NameNode provides the list of DataNodes containing the requested blocks. \\n• The client retrieves data directly from the nearest DataNodes to optimize \\nperformance. \\n \\n5. HDFS Architecture Diagram \\n     Below is a neat diagram illustrating HDFS architecture: \\n(Insert a well-labeled diagram with NameNode, DataNodes, Replication, and Client \\ninteractions.) \\n \\n6. Features & Benefits of HDFS \\n   Scalability – Handles petabytes of data efficiently. \\n   Fault Tolerance – Data replication ensures no data loss. \\n   High Throughput – Optimized for large-scale batch processing. \\n   Cost-Effective – Works on commodity hardware, reducing infrastructure costs. \\n   Reliability – Automatic replication and fault recovery. \\n \\n7. Real-World Applications of HDFS \\n  Social Media Platforms – Facebook, Twitter, and Instagram store vast amounts of \\nuser data. \\n  Search Engines – Google and Bing use distributed storage for indexing and ranking \\nweb pages.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='E-Commerce Platforms – Amazon and Flipkart process customer data and \\ntransaction logs. \\n  Healthcare & Finance – Fraud detection, medical imaging, and data-driven \\ndecision-making. \\n \\n8. Conclusion \\nHDFS is the backbone of the Hadoop ecosystem, providing a reliable, scalable, and \\nefficient storage solution for big data applications. Its ability to handle large datasets \\nacross distributed environments makes it a crucial technology for modern data-driven \\nenterprises.         \\nhttps://www.geeksforgeeks.org/explain-the-hadoop-distributed-file-system-hdfs-\\narchitecture-and-advantages/ \\n \\n6.How does the MapReduce programming model \\nwork? Explain with an example.  \\nChatGPT said: \\nMapReduce Programming Model \\n1. Introduction to MapReduce \\nMapReduce is a distributed computing framework in Hadoop used for processing \\nlarge-scale datasets in parallel across multiple nodes. It follows a divide-and-conquer \\napproach, breaking tasks into smaller chunks that can be executed independently. \\n• Developed by Google, later implemented in Hadoop. \\n• Works on the principle of parallelism to process big data efficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Consists of two main phases: Map and Reduce. \\n \\n2. Working of MapReduce \\nA. Map Phase (Splitting & Processing) \\n• Input data is split into chunks (blocks) and processed in parallel. \\n• Each chunk is passed to a Mapper function, which processes the data and \\ngenerates intermediate key-value pairs. \\nB. Shuffle & Sort Phase (Intermediate Processing) \\n• The framework sorts and groups the key-value pairs from the Mappers. \\n• Ensures that all values associated with a specific key are grouped together before \\npassing them to the Reducer. \\nC. Reduce Phase (Aggregation & Output Generation) \\n• The grouped data is passed to the Reducer function, which processes and \\naggregates the values for each key. \\n• The final result is stored in HDFS. \\n \\n3. Example: Word Count Program Using MapReduce \\nProblem Statement: \\nFind the frequency of each word in a given input text file. \\nStep-by-Step Execution: \\n1⃣ Input Data (Stored in HDFS): \\ncsharp \\nCopyEdit \\nHello Hadoop   \\nHadoop is powerful   \\nHello Big Data   \\n2⃣ Map Phase (Key-Value Generation by Mapper): \\nEach line is processed by a Mapper function that splits text into words and assigns a \\ncount of 1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 34, 'page_label': '35', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Input Line Mapper Output (Key, Value) \\nHello Hadoop (Hello, 1), (Hadoop, 1) \\nHadoop is powerful (Hadoop, 1), (is, 1), (powerful, 1) \\nHello Big Data (Hello, 1), (Big, 1), (Data, 1) \\n3⃣ Shuffle & Sort Phase: \\nThe framework groups and sorts values by keys. \\nKey Grouped Values \\nBig (1) \\nData (1) \\nHadoop (1, 1) \\nHello (1, 1) \\nis (1) \\npowerful (1) \\n4⃣ Reduce Phase (Aggregation by Reducer): \\nThe Reducer function sums the values for each key. \\nKey Final Count (Reducer Output) \\nBig 1 \\nData 1 \\nHadoop 2 \\nHello 2 \\nis 1 \\npowerful 1 \\n5⃣ Final Output Stored in HDFS: \\ncsharp \\nCopyEdit \\nBig 1   \\nData 1'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Hadoop 2   \\nHello 2   \\nis 1   \\npowerful 1   \\n \\n4. Advantages of MapReduce \\n   Scalability – Can process petabytes of data efficiently. \\n   Fault Tolerance – Automatic recovery from node failures. \\n   Parallelism – Runs tasks across multiple nodes simultaneously. \\n   Ease of Use – Handles low-level distributed computing complexities. \\n \\n5. Conclusion \\nMapReduce is a highly efficient and scalable data processing model widely used in big \\ndata analytics. Its parallel processing capabilities make it suitable for handling massive \\ndatasets in real-world applications like log analysis, recommendation systems, and \\nmachine learning preprocessing.         \\n4o \\n7. Summarize the features of Hadoop and explain \\nthe functionalities of Hadoop cluster? provide \\nanswer for 15 marks  \\nChatGPT said: \\nHadoop: Features & Cluster Functionalities \\n1. Features of Hadoop \\nHadoop is an open-source framework for storing and processing large-scale data across \\na distributed computing environment. Its key features include: \\nA. Scalability \\n• Can handle petabytes of data by adding new nodes to the cluster without affecting \\nperformance. \\nB. Fault Tolerance \\n• Uses HDFS (Hadoop Distributed File System) to replicate data across multiple \\nnodes, ensuring reliability even if nodes fail.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Distributed Processing \\n• Utilizes the MapReduce programming model for parallel execution, reducing \\nprocessing time. \\nD. Cost-Effective \\n• Runs on commodity hardware, reducing infrastructure costs. \\nE. High Availability \\n• If one node fails, data remains accessible due to multiple copies stored across \\ndifferent nodes. \\nF. Flexibility \\n• Supports structured, semi-structured, and unstructured data from logs, social \\nmedia, emails, and sensors. \\nG. Security & Authentication \\n• Kerberos-based authentication and data encryption provide secure data access. \\n \\n2. Functionalities of a Hadoop Cluster \\nA Hadoop cluster is a group of interconnected computers that work together to store \\nand process massive datasets. It consists of the following key components: \\nA. Master Node (NameNode & ResourceManager) \\n1. NameNode (HDFS Master) \\no Manages metadata (file locations, permissions, hierarchy) in HDFS. \\no Directs DataNodes on where to store and retrieve data. \\n2. ResourceManager (YARN Master) \\no Allocates resources (CPU, memory) to applications running in the cluster. \\no Manages job scheduling and execution. \\nB. Worker Nodes (DataNodes & NodeManagers) \\n1. DataNodes (HDFS Workers) \\no Store actual data in block format (default 128MB per block). \\no Send heartbeat signals to the NameNode for health monitoring. \\n2. NodeManagers (YARN Workers) \\no Execute tasks assigned by the ResourceManager. \\no Monitor resource usage and report back to the ResourceManager.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Job Execution Process in a Hadoop Cluster \\n1. Data is split into blocks and distributed across DataNodes. \\n2. The NameNode keeps track of data locations. \\n3. The ResourceManager assigns computing resources for MapReduce tasks. \\n4. Mappers process data in parallel and send intermediate results to Reducers. \\n5. Reducers aggregate data and store the final output in HDFS. \\n \\n3. Conclusion \\nHadoop’s scalability, fault tolerance, and distributed processing make it ideal for big \\ndata applications like log analysis, recommendation systems, and real-time analytics. A \\nHadoop cluster efficiently manages data storage and processing, ensuring high \\navailability and performance across large datasets.  \\n \\n \\n8. Apply the limitations of the traditional \\nMapReduce model and how YARN helps overcome \\nthem. provide answer for 15 marks \\nChatGPT said: \\nLimitations of Traditional MapReduce and How YARN Overcomes Them \\n1. Introduction to MapReduce \\nMapReduce is a distributed processing model used in Hadoop for large-scale data \\nprocessing. It follows a two-stage process: \\n1. Map Phase: Processes input data and generates key-value pairs. \\n2. Reduce Phase: Aggregates the intermediate output to produce final results. \\nAlthough MapReduce was revolutionary in handling big data, it had several limitations \\nthat restricted its efficiency and scalability. YARN (Yet Another Resource Negotiator) \\nwas introduced in Hadoop 2.0 to overcome these challenges. \\n \\n2. Limitations of Traditional MapReduce \\nA. Scalability Issues'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Limitation: The JobTracker manages both resource allocation and job scheduling, \\nleading to bottlenecks. \\n• Impact: As clusters grow, JobTracker fails to efficiently manage resources, limiting \\nscalability. \\nB. Inefficient Resource Utilization \\n• Limitation: Each MapReduce job requires separate Map and Reduce slots, leading \\nto resource wastage. \\n• Impact: If only reduce tasks are running, map slots remain idle, reducing cluster \\nefficiency. \\nC. High Latency for Iterative Jobs \\n• Limitation: Each MapReduce job must reload data from HDFS, even if the same \\ndata is required multiple times. \\n• Impact: Machine Learning, Graph Processing, and Iterative Analytics suffer due to \\nrepeated I/O operations. \\nD. Limited Support for Real-Time and Interactive Processing \\n• Limitation: MapReduce is batch-oriented and not optimized for real-time or \\ninteractive queries. \\n• Impact: Not suitable for use cases like stream processing, ad-hoc querying, and \\nreal-time analytics. \\nE. Single Point of Failure \\n• Limitation: The JobTracker is a single component responsible for all jobs and \\nresources. \\n• Impact: If JobTracker fails, entire Hadoop processing stops. \\nF. Lack of Multi-Tenancy \\n• Limitation: Hadoop’s MapReduce jobs run in a single execution model, restricting \\nmultiple applications. \\n• Impact: Difficult to run multiple workloads (e.g., Spark, Tez, or Storm) on the same \\ncluster. \\n \\n3. How YARN Overcomes These Limitations \\nYARN decouples job management and resource management, making Hadoop more \\nefficient and flexible. \\nA. Enhanced Scalability with ResourceManager'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Solution: YARN introduces a ResourceManager (RM) and NodeManagers (NM) \\ninstead of JobTracker. \\n• Benefit: Distributes resource management, allowing thousands of nodes to be \\nhandled efficiently. \\nB. Improved Resource Utilization via Dynamic Allocation \\n• Solution: Uses containers instead of fixed slots for Map and Reduce tasks. \\n• Benefit: Resources are dynamically assigned based on actual needs, improving \\nutilization. \\nC. Reduced Latency for Iterative and Interactive Processing \\n• Solution: YARN allows applications like Apache Spark, Tez, and HBase to run \\nalongside MapReduce. \\n• Benefit: In-memory computing reduces latency for iterative jobs like machine \\nlearning and graph analytics. \\nD. Support for Real-Time Processing and Multi-Tenancy \\n• Solution: YARN enables multiple data processing frameworks (e.g., Spark, Flink, \\nStorm) to run concurrently. \\n• Benefit: Hadoop supports real-time, batch, and interactive applications \\nsimultaneously. \\nE. Fault Tolerance and High Availability \\n• Solution: JobTracker responsibilities are split into ResourceManager and \\nApplicationMaster. \\n• Benefit: Eliminates a single point of failure, making Hadoop more resilient. \\nF. Better Cluster Utilization \\n• Solution: Dynamic resource allocation prevents idle resources. \\n• Benefit: Maximizes cluster efficiency and reduces execution time for jobs. \\n \\n4. Conclusion \\nWhile traditional MapReduce had limitations like poor resource utilization, high \\nlatency, and lack of multi-tenancy, YARN provided a more efficient, scalable, and \\nflexible framework for Hadoop. By separating job scheduling and resource \\nmanagement, YARN supports multiple processing engines, better fault tolerance, and \\nimproved performance, making Hadoop suitable for modern big data workloads.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='UNIT 3 15 marks \\n1.Explain in detail about NoSQL databases and \\nschema less models . Justify the need for the same. \\nprovide answer for 15 marks \\nNoSQL Databases and Schema-less Models \\n1. Introduction to NoSQL Databases \\nNoSQL databases (Not Only SQL) are modern database management systems designed to \\nhandle large volumes of data that are often unstructured or semi-structured. Unlike \\ntraditional relational databases (RDBMS) that follow a strict schema with predefined \\ntables and relationships, NoSQL databases provide flexibility, scalability, and high \\nperformance by allowing dynamic schema and distributed storage. \\nNoSQL databases were developed to address the limitations of relational databases, \\nespecially in handling Big Data, real-time applications, and cloud computing environments. \\n \\n2. Characteristics of NoSQL Databases \\nNoSQL databases have the following key characteristics: \\n1. Schema-Less Data Model – No predefined schema is required, allowing easy \\nmodifications to data structures. \\n2. Scalability – NoSQL databases support horizontal scaling, meaning they can expand \\nacross multiple servers. \\n3. High Availability – Designed for distributed architecture, ensuring redundancy and \\nfault tolerance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. High-Speed Read/Write Operations – Optimized for handling real-time data \\nprocessing. \\n5. Flexible Data Storage – Supports structured, semi-structured, and unstructured \\ndata formats like JSON, XML, BSON, etc. \\n \\n3. Types of NoSQL Databases and Their Applications \\nNoSQL databases are classified based on their data model: \\nA. Key-Value Stores \\n• Data is stored as key-value pairs (like a dictionary). \\n• Provides fast lookups and is ideal for caching and session management. \\n• Examples: Redis, DynamoDB, Riak. \\n• Use Case: Storing user session data in web applications. \\nB. Document Stores \\n• Stores data in document format (JSON, BSON, or XML). \\n• Supports nested structures and dynamic fields. \\n• Examples: MongoDB, CouchDB, ArangoDB. \\n• Use Case: Storing customer profiles in e-commerce platforms. \\nC. Column-Family Stores \\n• Data is stored in columns rather than rows, making it efficient for large-scale data \\nprocessing. \\n• Suitable for handling big data workloads. \\n• Examples: Apache Cassandra, HBase. \\n• Use Case: Analyzing real-time user behavior in social media platforms. \\nD. Graph Databases \\n• Designed to store and analyze complex relationships between data. \\n• Used in applications requiring network analysis and recommendation systems. \\n• Examples: Neo4j, Amazon Neptune. \\n• Use Case: Social media relationship mapping and fraud detection. \\n \\n4. Understanding Schema-Less Models'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A schema-less model means that data can be stored without a predefined structure. \\nUnlike RDBMS, where each table has a fixed schema, NoSQL databases allow documents \\nor records to have different attributes, making them flexible and adaptable. \\nAdvantages of Schema-Less Models \\n✔ Flexibility – Developers can modify data structures without downtime. \\n✔ Scalability – Easier to distribute across multiple servers. \\n✔ Performance – Faster read/write operations due to denormalized storage. \\n✔ Ease of Integration – Works well with cloud-native and microservices architectures. \\n \\n5. Justification for Using NoSQL & Schema-Less Models \\nA. Big Data & Real-Time Processing \\n• Traditional relational databases struggle with high-velocity and high-volume data. \\n• NoSQL databases efficiently handle large-scale distributed data for real-time \\nanalytics. \\n• Example: Social media platforms processing millions of posts per second. \\nB. Horizontal Scalability \\n• RDBMS scales vertically (adding resources to a single machine), which is costly. \\n• NoSQL databases scale horizontally by distributing data across multiple servers, \\nreducing costs. \\n• Example: Cloud applications like Netflix store terabytes of user data using a \\nhorizontally scalable NoSQL architecture. \\nC. Handling Unstructured Data \\n• RDBMS enforces strict data formats, making it hard to store JSON, multimedia, \\nlogs, or IoT data. \\n• NoSQL databases can handle varied data formats without restructuring. \\n• Example: Storing user-generated content in a blogging platform. \\nD. Cloud and Microservices Adoption \\n• NoSQL databases support distributed computing and microservices-based \\narchitectures. \\n• Enable high availability, auto-scaling, and fast processing in cloud environments. \\n• Example: Online banking systems managing millions of transactions in real time.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Comparison: NoSQL vs. RDBMS \\nFeature NoSQL RDBMS \\nSchema Schema-less, flexible Fixed schema, structured \\nScalability Horizontally scalable Vertically scalable \\nPerformance High-speed read/write Slower for large data \\nData Format JSON, XML, Key-Value, Graph Tables with rows & columns \\nUse Case Big Data, IoT, real-time apps Banking, ERP , transactional apps \\n \\n7. Real-World Applications of NoSQL Databases \\n1. Social Media Platforms – Facebook uses Apache Cassandra to manage billions of \\nposts. \\n2. E-commerce Websites – Amazon uses DynamoDB for product recommendations. \\n3. Streaming Services – Netflix stores metadata in Apache Cassandra. \\n4. Banking and Finance – Fraud detection using graph databases like Neo4j. \\n5. IoT Applications – Smart devices store sensor data in MongoDB and CouchDB. \\n \\n8. Challenges of NoSQL Databases \\n1. Lack of Standardization – No universal query language like SQL. \\n2. Complex Transactions – Not ideal for applications requiring ACID compliance. \\n3. Security Risks – Schema-less design can introduce data inconsistency. \\n4. Backup & Recovery – Requires specialized tools for large-scale backups. \\n \\n9. Conclusion \\nNoSQL databases and schema-less models provide a powerful alternative to traditional \\nRDBMS, offering scalability, flexibility, and performance for modern applications. They are \\nessential for big data analytics, cloud computing, IoT, and AI-driven applications. As \\nbusinesses continue to generate vast amounts of unstructured data, NoSQL databases will \\nplay a crucial role in future data management.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. Explain the concept of High-Performance \\nArchitecture in big data processing. \\nHigh-Performance Architecture in Big Data Processing \\n1. Introduction to High-Performance Architecture \\nHigh-Performance Architecture (HPA) in big data processing refers to optimized computing \\nframeworks, storage systems, and network architectures that enable rapid, efficient, and \\nscalable data handling. With the massive growth of structured and unstructured data from \\nsources like IoT, social media, and enterprise applications, traditional architectures \\nstruggle to process data efficiently. \\nHPA leverages parallel processing, distributed computing, and advanced memory \\nmanagement to handle high-volume, high-velocity, and high-variety data. \\n \\n2. Key Components of High-Performance Big Data Architecture \\nA. Distributed Computing Frameworks \\n• Apache Hadoop: Uses HDFS (Hadoop Distributed File System) for storing and \\nMapReduce for parallel processing. \\n• Apache Spark: An in-memory, distributed processing engine that performs real-\\ntime analytics much faster than Hadoop. \\nB. Scalable Storage Systems \\n• HDFS (Hadoop Distributed File System): Stores large datasets across multiple \\nnodes. \\n• NoSQL Databases (MongoDB, Cassandra): Schema-less models that offer scalability. \\n• Cloud Storage (AWS S3, Google Cloud Storage): Provides elastic storage for \\nhandling dynamic workloads. \\nC. Parallel Processing Techniques \\n• Multi-threading and Multi-core Processing: Enhances CPU utilization for faster \\ncomputations. \\n• Batch and Stream Processing:  \\no Batch Processing (Hadoop): Suitable for large-scale, historical data analysis. \\no Stream Processing (Apache Kafka, Apache Flink): Handles real-time data \\nstreams. \\nD. High-Speed Networking and Interconnects'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Software-Defined Networking (SDN): Optimizes data flow and bandwidth \\nallocation. \\n• High-Performance Clusters: Uses InfiniBand, Fiber Optics, and 5G Networks to \\nreduce latency and improve throughput. \\n \\n3. Characteristics of High-Performance Architecture \\nFeature Description \\nScalability Supports horizontal scaling to handle massive data volumes. \\nLow Latency Reduces response time using in-memory processing (Spark, Redis). \\nFault Tolerance Ensures data redundancy and system recovery (HDFS, Replication). \\nReal-time Processing Uses streaming technologies for live data analysis. \\nOptimized Resource \\nUtilization \\nEfficiently manages CPU, memory, and storage using virtualization \\nand containerization (Docker, Kubernetes). \\n \\n4. Workflow of High-Performance Big Data Processing \\n1. Data Ingestion – Capturing data from IoT, logs, transactions, and social media. \\n2. Data Storage – Using HDFS, NoSQL, or cloud-based systems for scalable storage. \\n3. Data Processing –  \\no Batch Processing (Hadoop MapReduce) for historical data. \\no Real-time Processing (Apache Spark, Kafka) for event-driven analytics. \\n4. Data Analysis & Visualization – Using ML models, BI tools (Tableau, Power BI). \\n5. Data Output & Decision Making – Processed insights are used in AI, business \\nanalytics, and predictive modeling. \\n \\n5. Applications of High-Performance Architecture in Big Data \\n1. E-Commerce & Recommendation Systems – Amazon, Netflix use Spark for real-time \\nrecommendations. \\n2. Healthcare & Genomics – AI-driven diagnosis and genomic analysis with distributed \\ncomputing. \\n3. Financial Fraud Detection – Banks use Spark with Graph Processing for anomaly \\ndetection.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Smart Cities & IoT – Real-time traffic monitoring, energy grid management. \\n5. Cybersecurity – AI-based threat detection & log analysis using high-speed \\ncomputing. \\n \\n6. Advantages of High-Performance Big Data Architecture \\n✔ Faster Processing – In-memory computing reduces latency. \\n✔ Scalable & Cost-Effective – Cloud-based architecture adapts to workload demands. \\n✔ Improved Fault Tolerance – Distributed storage prevents single points of failure. \\n✔ Real-Time Insights – Businesses can act on live data streams. \\n✔ Optimized Hardware Utilization – Uses GPUs, TPUs, and FPGAs for AI workloads. \\n \\n7. Challenges & Future Trends \\n  Challenges: \\n• High infrastructure costs for real-time processing. \\n• Complexity in integrating multi-cloud and hybrid environments. \\n• Security & privacy concerns in distributed computing. \\n        Future Trends: \\n• AI-Driven Data Processing – Automated data pipeline optimization. \\n• Quantum Computing – Future applications for massive data analysis. \\n• Edge Computing – 5G-powered low-latency processing for IoT & autonomous \\nsystems. \\n \\n8. Conclusion \\nHigh-Performance Architecture in big data processing enables organizations to process \\nmassive datasets efficiently while ensuring scalability, low latency, and real-time insights. \\nTechnologies like Apache Spark, Hadoop, NoSQL, cloud computing, and AI-powered \\nanalytics will continue shaping the next generation of big data solutions.         \\n3.Describe schema-less models in NoSQL. How do \\nthey enhance data flexibility? providde answetr for 15 \\nmarks question  \\nChatGPT said:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Schema-less Models in NoSQL and Their Role in Enhancing Data Flexibility \\n1. Introduction to Schema-less Models in NoSQL \\nA schema-less model in NoSQL databases means that the database does not enforce a \\nrigid, predefined schema like traditional relational databases (RDBMS). Instead, data can \\nbe stored in a flexible format, allowing dynamic schema evolution based on application \\nneeds. \\nThis flexibility is particularly useful in big data, real-time applications, and unstructured \\ndata storage, where data structures frequently change. \\n \\n2. Types of Schema-less NoSQL Databases \\nNoSQL databases are categorized based on how they store data: \\nA. Document Stores (e.g., MongoDB, CouchDB) \\n• Stores data in JSON, BSON, or XML formats. \\n• Each document can have different fields, structures, or types. \\n• Example:  \\njson \\nCopyEdit \\n{ \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\" } \\n{ \"name\": \"Bob\", \"email\": \"bob@example.com\", \"purchases\": [\"Laptop\", \"Phone\"] } \\no Alice’s record has age & city, while Bob’s has email & purchases—no fixed \\nschema! \\nB. Key-Value Stores (e.g., Redis, DynamoDB) \\n• Data stored as key-value pairs. \\n• No constraints on the type of values (text, JSON, binary, etc.). \\n• Example:  \\njson \\nCopyEdit \\n\"user_101\" : { \"name\": \"Alice\", \"age\": 25 } \\n\"session_567\" : { \"lastLogin\": \"2024-03-20\", \"active\": true } \\no Different keys store different types of values without predefined schema. \\nC. Column-Family Stores (e.g., Cassandra, HBase)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Data stored in flexible columns, allowing variable attributes per row. \\n• Example:  \\nsql \\nCopyEdit \\nRow ID | Name  | Age | Email   \\n1      | Alice | 25  | -   \\n2      | Bob   | -   | bob@example.com   \\no Some rows may not have certain columns, unlike traditional RDBMS. \\nD. Graph Databases (e.g., Neo4j, ArangoDB) \\n• Stores nodes & relationships without fixed schema. \\n• Example:  \\nscss \\nCopyEdit \\n(Alice) -[:FRIENDS_WITH]-> (Bob) \\n(Bob) -[:PURCHASED]-> (\"Laptop\") \\no Flexible structures allow dynamic relationships. \\n \\n3. How Schema-less Models Enhance Data Flexibility \\n   A. Dynamic Schema Evolution \\n• NoSQL databases do not require ALTER TABLE commands. \\n• Applications can add new fields without downtime. \\n   B. Handling Unstructured & Semi-Structured Data \\n• Supports JSON, XML, key-value pairs, which can store logs, social media posts, and \\nIoT data. \\n• Example: A social media platform can store varying user details dynamically. \\n   C. Scalability & Performance Boost \\n• Schema-less NoSQL databases support horizontal scaling, ideal for big data \\napplications. \\n• Example: Cassandra scales across multiple nodes efficiently. \\n   D. Multi-format Data Storage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Unlike relational databases, which enforce uniform structure, schema-less models \\nallow storing heterogeneous data. \\n• Example: An e-commerce database can store both customer reviews (text) and \\nproduct specs (JSON) in the same collection. \\n   E. Faster Development & Agile Adaptability \\n• Developers can iterate faster without modifying schemas. \\n• Startups & AI-driven applications benefit from schema-less flexibility. \\n \\n4. Real-World Applications of Schema-less NoSQL Models \\nIndustry Use Case NoSQL Type \\nSocial Media Storing user posts, comments, likes Document Store \\n(MongoDB) \\nE-Commerce Product catalogs with varied \\nattributes Column Store (Cassandra) \\nIoT & Sensors Streaming real-time sensor data Key-Value Store (Redis) \\nFraud Detection Graph-based fraud detection Graph DB (Neo4j) \\nAI & Machine \\nLearning Storing unstructured training datasets Document Store (CouchDB) \\n \\n5. Advantages of Schema-less NoSQL Models \\nAdvantage Description \\nFlexibility Data can change dynamically without altering database \\nschema. \\nScalability NoSQL databases scale horizontally, handling massive data \\nloads. \\nPerformance Faster queries with distributed architecture (Cassandra, \\nDynamoDB). \\nMulti-format Data \\nSupport Stores structured, semi-structured, and unstructured data. \\nHigh Availability Supports replication & partitioning for data redundancy.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Challenges of Schema-less NoSQL Models \\nChallenge Solution \\nLack of ACID Compliance Use eventual consistency models for distributed transactions. \\nComplex Queries Use indexing & caching (Redis, Elasticsearch) for optimization. \\nData Integrity Issues Implement application-layer validation to ensure data quality. \\n \\n7. Conclusion \\nSchema-less models in NoSQL databases provide unmatched flexibility, scalability, and \\nadaptability for modern applications. Whether handling real-time data, unstructured \\ninformation, or high-velocity transactions, schema-less NoSQL solutions like MongoDB, \\nCassandra, Redis, and Neo4j are transforming big data storage, AI systems, and cloud \\napplications.         \\n4.Describe about document store and key value store \\nprovide answer in 15 marks detail approach \\nChatGPT said: \\nDocument Store and Key-Value Store in NoSQL \\n1. Introduction to NoSQL Databases \\nNoSQL databases are designed to handle large-scale, unstructured, or semi-structured \\ndata efficiently. Among the various types, Document Stores and Key-Value Stores play a \\ncrucial role in modern applications due to their scalability, flexibility, and high \\nperformance. \\n \\n2. Document Store \\nA. Definition \\nA document store is a NoSQL database that stores data in semi-structured formats such as \\nJSON, BSON, or XML. Each document contains a self-contained dataset that can have a \\nunique structure, making document stores highly flexible and schema-less. \\nB. Features of Document Stores \\n• Schema-flexibility: No predefined schema is required. \\n• Hierarchical Storage: Stores data as nested key-value pairs inside a document. \\n• Indexing & Querying: Allows complex queries using indexes on JSON fields.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Scalability: Supports horizontal scaling via sharding and replication. \\n• Embedded Relationships: Stores arrays and nested objects within documents. \\nC. Structure of a Document Store (Example in MongoDB) \\nA JSON-like document stored in MongoDB: \\njson \\nCopyEdit \\n{ \\n  \"_id\": \"101\", \\n  \"name\": \"Alice\", \\n  \"age\": 25, \\n  \"email\": \"alice@example.com\", \\n  \"orders\": [ \\n    { \"item\": \"Laptop\", \"price\": 1000 }, \\n    { \"item\": \"Mouse\", \"price\": 50 } \\n  ] \\n} \\n• Each document contains key-value pairs that store user information dynamically. \\n• Arrays (orders) allow storing multiple related objects inside a single document. \\nD. Examples of Document Store Databases \\nDatabase Description \\nMongoDB The most popular document store with JSON-like documents. \\nCouchDB Stores data in JSON format with RESTful APIs for querying. \\nRethinkDB Real-time document storage with continuous data updates. \\nFirebase Firestore A cloud-based NoSQL document database for mobile apps. \\nE. Use Cases of Document Stores \\nIndustry Application \\nE-commerce Storing customer profiles, product catalogs dynamically. \\nSocial Media Storing user posts, comments, likes in a flexible format.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Industry Application \\nHealthcare Managing patient records, prescriptions, and reports. \\nIoT & AI Handling sensor data & AI-generated unstructured datasets. \\n \\n3. Key-Value Store \\nA. Definition \\nA key-value store is the simplest form of a NoSQL database where data is stored as a \\nunique key and an associated value. These databases are highly optimized for read/write \\nspeed, making them ideal for caching and real-time applications. \\nB. Features of Key-Value Stores \\n• Simple Data Storage: Stores data as key-value pairs (similar to a dictionary). \\n• High Performance: Fast retrieval using hash indexing. \\n• Scalability: Easily scalable by distributing keys across multiple nodes. \\n• Eventual Consistency: Supports distributed caching with minimal latency. \\n• Schema-less: The value can be strings, JSON, binary, or even entire objects. \\nC. Structure of a Key-Value Store (Example in Redis) \\nA key-value pair stored in Redis: \\nsql \\nCopyEdit \\nSET user_101 \"{ \\'name\\': \\'Alice\\', \\'age\\': 25, \\'email\\': \\'alice@example.com\\' }\" \\nGET user_101 \\n• The key (\"user_101\") is mapped to a JSON-like object containing user details. \\n• Unlike document stores, key-value databases lack built-in query mechanisms and \\nrely on key lookups. \\nD. Examples of Key-Value Store Databases \\nDatabase Description \\nRedis An in-memory key-value store used for caching and real-time processing. \\nDynamoDB AWS managed NoSQL database for highly available applications. \\nRiak KV A distributed key-value store optimized for availability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Database Description \\nBerkeley DB A lightweight embedded key-value database. \\nE. Use Cases of Key-Value Stores \\nIndustry Application \\nWeb Applications Session storage, authentication tokens (e.g., JWT, OAuth). \\nGaming Storing leaderboards, player stats, real-time game states. \\nE-commerce Caching product details, frequently accessed data. \\nIoT Storing real-time sensor readings, device configurations. \\n \\n4. Comparison: Document Store vs. Key-Value Store \\nFeature Document Store Key-Value Store \\nData Format JSON, BSON, XML Key-value pairs (String, Binary, \\nJSON) \\nSchema Schema-less (Flexible structure) Schema-less (No structure) \\nQuerying Supports complex queries on nested \\nfields Only key-based retrieval \\nPerformance Fast but slower than key-value stores Ultra-fast lookup operations \\nUse Case Semi-structured data (profiles, logs, \\nproducts) Caching, real-time applications \\n \\n5. Conclusion \\nBoth document stores and key-value stores serve critical roles in modern NoSQL \\ndatabases: \\n• Document Stores (MongoDB, CouchDB) offer schema-flexibility and powerful \\nqueries for dynamic applications like e-commerce and social media. \\n• Key-Value Stores (Redis, DynamoDB) provide blazing-fast read/write speeds for use \\ncases like caching, gaming, and real-time analytics. \\nUnderstanding their strengths allows businesses to choose the right database based on \\napplication needs, ensuring scalability, flexibility, and high performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Explain in detail about system architecture of Hbase \\nSystem Architecture of HBase \\n1. Introduction to HBase \\nHBase is a distributed, column-oriented NoSQL database built on top of Hadoop’s HDFS \\n(Hadoop Distributed File System). It is designed to handle large-scale, real-time data \\nprocessing with high availability and scalability. \\nKey Characteristics of HBase: \\n• Modeled after Google’s Bigtable \\n• Stores structured data in tables \\n• Supports random reads and writes \\n• Provides high throughput for real-time applications \\n• Runs on top of HDFS for fault tolerance \\n \\n2. HBase System Architecture \\nHBase follows a master-slave architecture with three main components: \\nA. HBase Master Server \\nThe HBase Master Server manages and coordinates the RegionServers in the cluster. \\nFunctions of HBase Master: \\n1. Assigns regions (data partitions) to RegionServers. \\n2. Monitors RegionServers for health checks and load balancing. \\n3. Handles schema changes (table creation, deletion, and updates). \\n4. Manages splits and merges of regions dynamically. \\n5. Coordinates with ZooKeeper for failover and recovery. \\nB. RegionServers \\nRegionServers are responsible for handling read/write requests and managing data \\nstorage. \\nFunctions of RegionServers: \\n1. Stores actual data in HBase tables. \\n2. Handles client read/write requests. \\n3. Manages regions (subsets of tables) assigned by the Master.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Caches frequently accessed data for fast retrieval. \\n5. Flushes in-memory data to HDFS for durability. \\nEach RegionServer is responsible for multiple regions, and regions grow over time until \\nthey split into smaller ones. \\nC. ZooKeeper (Coordination Service) \\nHBase relies on ZooKeeper, a distributed coordination service, to manage cluster state. \\nFunctions of ZooKeeper in HBase: \\n1. Tracks active HBase Masters and RegionServers. \\n2. Manages leader election and failover. \\n3. Maintains metadata and configuration details. \\n4. Ensures synchronization between distributed components. \\n \\n3. Storage Architecture of HBase \\nHBase organizes data in a highly optimized format for high-speed reads and writes. The \\ncore storage architecture consists of: \\nA. Tables, Rows, and Column Families \\n• Data is stored in tables, consisting of rows and column families. \\n• Each row is identified by a unique row key. \\n• Column families group related columns together for better storage efficiency. \\nB. HFile (Storage on HDFS) \\nHBase stores data in HFiles on HDFS for durability and scalability. \\n• Data is first written to the Write-Ahead Log (WAL) to prevent data loss. \\n• Then, it is stored in MemStore (in-memory) before being flushed to HFiles on disk. \\nC. MemStore and HFile (Data Write Path) \\n1. Client writes data to MemStore (in-memory buffer). \\n2. Data is logged in Write-Ahead Log (WAL) to ensure durability. \\n3. When MemStore reaches a threshold, it flushes data to HFiles on HDFS. \\n4. Compaction merges smaller HFiles into larger ones to optimize read performance. \\nD. Data Read Path \\n1. Client sends a read request to the RegionServer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. RegionServer checks MemStore (if data is in-memory). \\n3. If not found, it searches in HFiles (on disk in HDFS). \\n4. Uses Bloom Filters and block cache to speed up reads. \\n \\n4. HBase Write and Read Operations \\nA. Write Operation (Insert/Update Data) \\n1. Client sends a write request to the appropriate RegionServer. \\n2. Data is first written to the Write-Ahead Log (WAL). \\n3. Data is then stored in MemStore (in-memory cache). \\n4. Once MemStore is full, data is flushed to disk (HFile on HDFS). \\n5. Periodic compaction merges HFiles to optimize storage. \\nB. Read Operation (Retrieve Data) \\n1. Client sends a read request to the RegionServer. \\n2. RegionServer first checks MemStore for recent data. \\n3. If data is not found, it checks BlockCache (cached data). \\n4. If still not found, it reads from HFiles on HDFS. \\n5. Data is returned to the client. \\n \\n5. HBase vs. Traditional RDBMS \\nFeature HBase (NoSQL) Traditional RDBMS \\nData Model Column-Oriented Row-Oriented \\nSchema Schema-less Fixed Schema \\nScalability Horizontally Scalable Limited Scalability \\nData Storage Distributed (HDFS) Centralized \\nRead/Write Speed High for big data Slower for large datasets \\nUse Case Real-time Big Data Processing OLTP Transactions \\n \\n6. Use Cases of HBase'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 57, 'page_label': '58', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Industry Application \\nSocial Media Storing user activity logs, messages, and posts \\nFinance Managing real-time stock market transactions \\nE-commerce Handling product catalogs and customer interactions \\nHealthcare Storing patient records and medical history \\nIoT & AI Processing sensor data from connected devices \\n \\n7. Conclusion \\nHBase is a highly scalable, real-time, column-oriented NoSQL database built for big data \\napplications. Its master-slave architecture, distributed storage on HDFS, and integration \\nwith Hadoop make it a powerful choice for applications requiring fast reads, writes, and \\nhigh availability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='(SET 1 15 marks) \\n1. Customer wants you to develop the software for the remote controlled electronic toy. \\nCustomer is not sure of all requirements for the product. She is provided with initial set of \\nrequirement with which she wants to have feel of the product. She has informed that few \\nmore requirements will be provided later. Explain observations made in this scenario and \\nwhich model will be suitable for this requirement. \\nChatGPT said: \\nObservations in the Given Scenario: \\n1. Unclear and Evolving Requirements – The customer has only provided an initial set \\nof requirements and plans to introduce more in the future. This indicates that the \\nproject scope is not fully defined at the start. \\n2. Incremental Development Expected – Since the customer wants to have a \"feel\" of \\nthe product with the available requirements, an iterative approach would be \\nbeneficial to incorporate feedback and evolving needs. \\n3. Customer Involvement is Key – The customer will be actively engaged, providing \\nnew requirements after evaluating initial versions of the software. \\n4. Need for Flexibility – A rigid development model (like the Waterfall model) may not \\nbe suitable since new requirements will emerge, necessitating adaptability. \\n5. Prototyping is Essential – The need for an early demonstration of the product \\nsuggests that an initial prototype or working version should be developed before \\nfinalizing the full requirements. \\nSuitable Software Development Model: Incremental Model \\nThe Incremental Model is the most suitable approach for this scenario because: \\n1. Iterative Delivery – The project can be broken into multiple increments. The first \\nincrement will be developed based on the initial requirements, and subsequent \\nincrements will incorporate new requirements as they are provided. \\n2. Customer Feedback Integration – Each increment allows the customer to test the \\nproduct, suggest modifications, and refine requirements, ensuring better alignment \\nwith their expectations. \\n3. Risk Reduction – Since the development happens in stages, issues can be identified \\nand fixed in earlier increments, reducing the risk of major failures later. \\n4. Faster Time-to-Market – A basic version of the software can be delivered early, \\nallowing the customer to interact with it before all features are implemented. \\n5. Flexibility to Accommodate Changes – New requirements can be easily incorporated \\nin upcoming increments without disrupting the entire project.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Alternative Model: Prototype Model \\nAnother possible approach is the Prototype Model, where an initial prototype is developed \\nbased on the given requirements. The customer can review it, suggest modifications, and \\nrefine the requirements before proceeding with the final development. However, if \\ncontinuous additions to the product are expected, the Incremental Model remains the \\nbetter choice. \\nConclusion \\nSince the customer is uncertain about all requirements and expects to refine them after \\ntesting an initial version, the Incremental Model is the most suitable approach. It ensures \\nearly delivery, flexibility, and continuous customer involvement, leading to a well-refined and \\ncustomer-centric final product. \\nObservations in the Given Scenario \\n1. Unclear and Evolving Requirements \\no The customer has only provided an initial set of requirements and has \\nexplicitly mentioned that more requirements will be given later. \\no This suggests that the customer does not have a complete vision of the final \\nproduct at the beginning of development. \\no The software development process must accommodate changes and new \\nrequirements as they emerge. \\n2. Customer Wants to Experience the Product Early \\no The customer wants to \"have a feel\" of the product before finalizing \\nadditional requirements. \\no This indicates the need for early software delivery so the customer can \\ninteract with it and provide feedback. \\no A development model that allows for early prototyping or incremental \\ndelivery is required. \\n3. Continuous Customer Involvement is Needed \\no Since the customer will be providing new requirements after testing the initial \\nversion, frequent interaction between the development team and the \\ncustomer is necessary. \\no The development approach should support continuous collaboration, \\nfeedback, and modifications. \\n4. Risk of Major Changes in Requirements \\no Given that the requirements are incomplete, there is a possibility of \\nsignificant changes in design, functionality, or scope during development.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o A rigid model like the Waterfall Model would be unsuitable because it does \\nnot accommodate requirement changes once development has started. \\n5. Prototype or Working Version Needed \\no Since the customer wants to experience the product before finalizing all \\nrequirements, a development approach that supports prototyping or early-\\nstage working versions of the product is essential. \\n \\nMost Suitable Software Development Model: Incremental Model \\nThe Incremental Model is the best choice for this scenario due to its flexibility, iterative \\nnature, and ability to incorporate evolving requirements. \\nExplanation of the Incremental Model \\nThe Incremental Model is a software development approach where the system is built in \\nmultiple small parts (increments). Each increment adds additional features to the existing \\nsystem based on user feedback and newly defined requirements. \\nHow the Incremental Model Works in This Scenario \\n1. First Increment \\no The development team builds the first version of the software based on the \\ninitial set of requirements. \\no This provides the customer with an early version to test and give feedback. \\n2. Customer Feedback & New Requirements \\no After testing the first increment, the customer provides additional \\nrequirements and modifications. \\no The development team incorporates these in the next increment. \\n3. Subsequent Increments \\no Each new increment builds upon the previous version, integrating newly \\nidentified requirements. \\no The process continues until the final product is complete. \\nAdvantages of Using the Incremental Model in This Case \\n   Accommodates Changing Requirements \\n• Since the customer has evolving needs, the incremental model allows new \\nrequirements to be added in later stages. \\n   Early Product Delivery'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"• A functional part of the software is delivered early, enabling the customer to interact \\nwith it and refine their expectations. \\n   Better Risk Management \\n• The project is developed in smaller parts, reducing risks compared to building the \\nentire system at once. \\n   Customer Involvement Throughout Development \\n• The customer actively participates in each increment, ensuring that the final product \\naligns with their expectations. \\n   Faster Identification and Resolution of Issues \\n• Bugs and design flaws can be detected early in the development process, reducing \\noverall project risks. \\n \\nAlternative Approach: Prototype Model \\nIf the customer is unsure about the product's functionality and wants an early \\ndemonstration before development starts, the Prototype Model could also be considered. \\nHow the Prototype Model Works \\n1. A quick prototype (basic working model) is created using initial requirements. \\n2. The customer reviews the prototype and suggests modifications. \\n3. The prototype is refined until the customer is satisfied. \\n4. Once finalized, the actual software development begins based on the refined \\nprototype. \\n   Suitable if the customer is unclear about core functionalities. \\n   Helps in requirement gathering and validation before full-scale development. \\n   Not ideal for continuous requirement changes. \\nSince the customer in this case will be providing additional requirements throughout the \\ndevelopment, the Incremental Model remains the best approach over the Prototype Model. \\n \\nConclusion \\nFor a remote-controlled electronic toy with evolving requirements, the Incremental Model is \\nthe best choice. It enables: \\n• Early delivery of working software, \\n• Continuous customer involvement,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"• Integration of new requirements at later stages, \\n• Better risk management, \\n• Higher flexibility and adaptability to changes. \\nThus, the Incremental Model ensures a structured yet flexible development approach, \\ndelivering a product that meets the customer's evolving expectations efficiently.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='SET2(15 marks) \\n1. Customer has provided you with all the requirements and also assured there will not be \\nany change in the requirement. He expects deliverables at every stage of development. You \\nhave carried out the similar project earlier and you are sure that the project could be \\nexecuted systemically. Explain observations made in this scenario and which model will be \\nsuitable for this requirement.  \\nChatGPT said: \\nObservations in the Given Scenario \\n1. Clearly Defined and Fixed Requirements \\no The customer has provided a complete and well-defined set of requirements. \\no There will be no changes in requirements throughout the project lifecycle. \\no This eliminates the need for an iterative or adaptive model that \\naccommodates changes. \\n2. Expectation of Deliverables at Every Stage \\no The customer expects progress to be demonstrated at each stage of \\ndevelopment. \\no This suggests a need for a structured development approach with clearly \\ndefined phases and deliverables at each stage. \\n3. Previous Experience with Similar Projects \\no The development team has already worked on a similar project before. \\no This indicates that the team has prior knowledge, making it easier to estimate \\ntimelines and execute the project efficiently. \\no There is low uncertainty, reducing the need for prototyping or iterative \\nrefinement. \\n4. Systematic Execution is Feasible \\no Since the project requirements are well-defined and the team is experienced, \\na well-structured development process can be followed without disruptions. \\no There is no need for frequent customer interaction beyond the predefined \\nmilestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario due to its structured, phase-wise \\napproach that aligns well with the given conditions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Explanation of the Waterfall Model \\nThe Waterfall Model is a linear, sequential approach where each phase is completed before \\nmoving on to the next. It follows a strict order, ensuring that deliverables are provided at the \\nend of each stage. \\nPhases of the Waterfall Model and How They Apply to This Project \\n1. Requirement Gathering and Analysis \\no Since the requirements are already fully defined and fixed, this phase will be \\nstraightforward. \\no The development team will document all requirements to ensure clarity. \\n2. System Design \\no Based on the requirements, the system architecture and design will be \\ncreated. \\no This includes database design, system interfaces, and technical specifications. \\no A deliverable (design document) can be provided to the customer at this \\nstage. \\n3. Implementation (Coding) \\no The actual development (coding) takes place based on the finalized design. \\no Modules are developed and integrated systematically. \\no Deliverables can include partially completed modules or milestone updates. \\n4. Testing \\no The entire system undergoes rigorous testing after implementation. \\no Unit testing, integration testing, and system testing ensure that the software \\nmeets all requirements. \\no The customer can receive a test report as a deliverable at this stage. \\n5. Deployment \\no The software is deployed for customer use. \\no Since no changes are expected, this phase will be straightforward. \\n6. Maintenance \\no Although the requirements are fixed, maintenance may still be required for \\nbug fixes and optimizations. \\no The customer will receive regular updates regarding software stability and \\nperformance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Why the Waterfall Model is the Best Choice \\n   Well-Suited for Fixed Requirements \\n• Since the customer has clearly defined requirements and no changes are expected, \\nthe Waterfall Model ensures structured execution without disruptions. \\n   Systematic Execution \\n• Each phase follows a strict order, ensuring that deliverables are provided at every \\nstage as per the customer’s expectations. \\n   Predictability and Efficiency \\n• The development team has prior experience with similar projects, making \\nestimations more accurate and execution more efficient. \\n   Minimizes Risk \\n• Since all requirements are known from the beginning, risks associated with \\nrequirement changes or scope creep are eliminated. \\n   Clear Customer Expectations \\n• The customer knows what to expect at each stage, as the Waterfall Model provides \\nformal documentation and structured progress updates. \\n \\nAlternative Approach: V-Model \\nIf the project requires early validation and continuous testing at every stage, the V-Model \\n(Validation & Verification Model) could also be considered. \\nHow the V-Model Works \\n• It follows the same structured approach as the Waterfall Model but integrates testing \\nat every stage. \\n• Each development phase has a corresponding testing phase, ensuring issues are \\ndetected early. \\n   Ensures high-quality software through continuous testing. \\n   Minimizes post-development defects. \\n   Not necessary if testing at the final stage is sufficient. \\nSince the scenario does not specify a need for early-stage validation, the Waterfall Model \\nremains the best choice for this project.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Conclusion \\nFor a project with fixed requirements, systematic execution, and predefined stage-wise \\ndeliverables, the Waterfall Model is the most suitable choice. It ensures: \\n• A structured development process, \\n• Predictable timelines and outcomes, \\n• Formal deliverables at every stage, \\n• Minimal risk due to well-defined requirements. \\nThus, the Waterfall Model provides a clear and efficient approach, ensuring successful \\nproject completion with minimal complications. \\n \\n                                                                        [OR] \\nObservations in the Given Scenario \\n1. Well-Defined and Fixed Requirements \\no The customer has provided a complete and unchanging set of requirements. \\no This means there is no need for continuous requirement gathering or \\niterative refinements. \\no There is little to no risk of scope creep (where additional features are \\nrequested mid-development). \\n2. Expectation of Deliverables at Every Stage \\no The customer wants structured updates and deliverables at each stage. \\no This suggests the need for a formal, well-documented approach where each \\nphase produces a tangible output before moving to the next. \\n3. Previous Experience with Similar Projects \\no The development team has already successfully executed a similar project. \\no This indicates that: \\n▪ The project’s complexity is understood. \\n▪ Development timelines and risks can be estimated accurately. \\n▪ There is no need for a prototype or experimental approach. \\n4. Systematic Execution is Feasible \\no Since the requirements are fixed and the development team is experienced, a \\nstructured and disciplined execution approach will work best.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o There is no need for high customer involvement beyond predefined \\nmilestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario because it follows a structured, \\nlinear, and sequential approach to development. \\n \\nDetailed Explanation of the Waterfall Model \\nThe Waterfall Model follows a step-by-step execution process, where each phase is \\ncompleted before moving to the next. It is best suited when the requirements are fixed, \\ndeliverables are expected at each stage, and the project can be executed systematically. \\nPhases of the Waterfall Model and Their Relevance to This Project \\n1. Requirement Analysis Phase \\no Since the customer has already provided a complete and unchanging set of \\nrequirements, this phase will focus on: \\n▪ Documenting the requirements formally in a Software Requirement \\nSpecification (SRS) document. \\n▪ Reviewing the requirements with the customer to ensure there are no \\ngaps. \\n▪ Identifying constraints and dependencies early on. \\no    Deliverable: Finalized SRS document approved by the customer. \\n2. System Design Phase \\no In this phase, the system’s architecture is designed based on the SRS. \\no It involves: \\n▪ High-level design (HLD): Defines system architecture, modules, and \\ncomponents. \\n▪ Low-level design (LLD): Specifies detailed logic for each module. \\n▪ Database design: Designing data storage and access mechanisms. \\no    Deliverable: System Design Document (SDD) with diagrams, workflows, \\nand technical specifications. \\n3. Implementation (Coding) Phase'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o The actual development (coding) of the system begins based on the approved \\ndesign. \\no Since the team has prior experience with similar projects, coding can be \\ndone efficiently with minimal risk. \\no The development is modular, meaning individual components can be tested \\nbefore integration. \\no    Deliverable: Working software modules or intermediate builds at \\npredefined milestones. \\n4. Integration and Testing Phase \\no After coding, the system is assembled and tested to ensure it meets the \\nspecified requirements. \\no Testing involves: \\n▪ Unit testing: Testing individual modules for correctness. \\n▪ Integration testing: Ensuring different modules work together. \\n▪ System testing: Verifying overall functionality and performance. \\n▪ Acceptance testing: Ensuring the system meets customer \\nexpectations. \\no Since the requirements are fixed, testing is straightforward, focusing on \\nverification rather than exploratory changes. \\no    Deliverable: Test reports and a fully tested software system. \\n5. Deployment Phase \\no The system is delivered to the customer and deployed in the intended \\nenvironment. \\no This phase includes: \\n▪ Installation and configuration on customer infrastructure. \\n▪ User training (if required). \\n▪ Final acceptance testing by the customer. \\no    Deliverable: Fully operational software deployed at the customer site. \\n6. Maintenance Phase \\no While the requirements will not change, bug fixes, performance \\noptimization, and minor updates may be required. \\no The team provides support and maintenance as per the agreement.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o    Deliverable: Software updates and maintenance reports. \\n \\nWhy the Waterfall Model is the Best Choice for This Project \\n   Well-Suited for Fixed Requirements \\n• Since the customer has already provided a complete and unchanging set of \\nrequirements, the Waterfall Model ensures that the project is executed \\nsystematically with no need for requirement adjustments. \\n   Structured Development Process \\n• The Waterfall Model follows a step-by-step approach where each phase is \\ncompleted before moving to the next. This matches the customer’s expectation of \\ndeliverables at every stage. \\n   Predictability and Efficiency \\n• The development team has prior experience with similar projects, making it easier \\nto estimate timelines, costs, and risks. \\n• Since the process is structured, project tracking is straightforward and transparent. \\n   Minimizes Risks and Errors \\n• Since testing is conducted only after implementation, defects are detected before \\ndeployment. \\n• The project follows a systematic verification process, ensuring a high-quality final \\nproduct. \\n   Clear Customer Expectations and Milestones \\n• The customer knows exactly what to expect at each stage since the Waterfall Model \\nhas predefined phases and deliverables. \\n• There is no need for continuous feedback and iteration, reducing delays and scope \\ncreep. \\n \\nAlternative Approach: V-Model (Verification & Validation Model) \\nIf testing at every phase is a major priority, the V-Model could be considered. It is an \\nenhanced version of the Waterfall Model where each development phase has a \\ncorresponding testing phase. \\nKey Features of the V-Model: \\n• Follows the same structured approach as the Waterfall Model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='• Testing is performed after every phase rather than waiting until the end. \\n• Ensures early defect detection and improves software quality. \\n   Better for projects where high reliability is required. \\n   Reduces cost of fixing bugs by identifying them early. \\n   Not necessary if post-development testing is sufficient. \\nSince the scenario does not emphasize the need for early-stage validation, the Waterfall \\nModel remains the best choice. \\n \\nConclusion \\nFor a project where all requirements are fixed, deliverables are expected at each stage, and \\nsystematic execution is feasible, the Waterfall Model is the most appropriate choice. It \\nensures: \\n  Structured execution with clearly defined phases \\n  Predictable timelines and outcomes \\n  Minimal risk due to fixed requirements \\n  Efficient project tracking and milestone-based deliverables \\nThus, the Waterfall Model provides a well-organized and disciplined development \\napproach, ensuring successful project completion while meeting customer expectations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"SET 1(7 & 8 marks): \\n(8 marks): \\nApply Bayesian Classification method with an example for class based modeling. provide \\ndetailed answer \\nChatGPT said: \\nApplying Bayesian Classification Method for Class-Based Modeling \\n1. Introduction to Bayesian Classification \\nBayesian Classification is based on Bayes' Theorem, which provides a probabilistic approach \\nto classification. It is widely used in machine learning, spam detection, medical diagnosis, \\nand text classification. \\nBayes' Theorem Formula \\n \\n2. Class-Based Modeling Using Bayesian Classification \\nClass-based modeling defines a set of classes representing different objects in a system, \\nwhere Bayesian classification helps predict the class of a new instance. \\nExample: Email Spam Detection (Class-Based Model with Bayesian Classification) \\nLet's consider a binary classification problem, where we classify an email as Spam (S) or Not \\nSpam (NS). \\nStep 1: Define Classes \\n• C1=C_1 =C1= Spam \\n• C2=C_2 =C2= Not Spam \\nStep 2: Collect Training Data (Feature Extraction)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Assume we have a dataset containing words like \"free\", \"win\", \"offer\", \"urgent\", which are \\ncommon in spam emails. The dataset looks like: \\n \\n \\n \\n3. Applications of Bayesian Classification in Class-Based Modeling'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Medical Diagnosis → Classifying diseases based on symptoms. \\n2. Customer Segmentation → Predicting if a customer belongs to a premium or regular \\nclass. \\n3. Fraud Detection → Identifying fraudulent transactions in banking. \\n4. Sentiment Analysis → Classifying reviews as positive or negative. \\n \\n4. Conclusion \\nBayesian Classification provides a probabilistic and interpretable approach to class-based \\nmodeling. It is effective for spam filtering, medical diagnostics, and fraud detection, as \\ndemonstrated in the email classification example.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='(7 marks): \\n(i) Build the various architectural styles and design for software design process development.  \\nChatGPT said: \\nArchitectural Styles and Design for Software Development \\nSoftware architecture defines the structure, behavior, and interaction of software \\ncomponents. Different architectural styles are used based on system requirements like \\nscalability, maintainability, and performance. \\n \\n1. Common Architectural Styles \\n1.1 Layered Architecture \\n  Description: Organizes software into multiple layers where each layer performs a specific \\nfunction. \\n  Best for: Large enterprise applications, web applications, and software with clear \\nseparation of concerns. \\nExample: Web Application Architecture \\nLayers: \\n1. Presentation Layer (UI): Handles user interactions. \\n2. Business Logic Layer: Processes business rules. \\n3. Data Access Layer: Manages database operations. \\n4. Database Layer: Stores and retrieves data. \\n     Example Technologies: React (UI), Node.js (Business Logic), MySQL (Database). \\n \\n1.2 Client-Server Architecture \\n  Description: Divides system into client (user interface) and server (backend processing). \\n  Best for: Web applications, online banking, cloud services. \\nExample: Online Banking System \\n1. Client: Web/Mobile app sending requests. \\n2. Server: Processes transactions, verifies data. \\n3. Database: Stores user data securely. \\n     Example Technologies: JavaScript (Client), Spring Boot (Server), PostgreSQL (Database).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1.3 Microservices Architecture \\n  Description: Breaks software into independent services that communicate via APIs. \\n  Best for: Scalable cloud applications, e-commerce, streaming platforms. \\nExample: E-commerce Application \\n• User Service: Handles authentication. \\n• Product Service: Manages product catalog. \\n• Payment Service: Handles transactions. \\n     Example Technologies: Docker, Kubernetes, REST APIs, AWS. \\n \\n1.4 Event-Driven Architecture \\n  Description: Components communicate through events, ensuring asynchronous \\nprocessing. \\n  Best for: Real-time applications, IoT, stock market systems. \\nExample: IoT Smart Home System \\n1. Sensors detect motion. \\n2. Event is published to an event queue. \\n3. Security system triggers an alarm. \\n     Example Technologies: Kafka, RabbitMQ, MQTT. \\n \\n1.5 Model-View-Controller (MVC) Architecture \\n  Description: Separates application logic into Model, View, and Controller. \\n  Best for: Web applications, GUI-based applications. \\nExample: Web Framework (Django, Laravel, ASP .NET MVC) \\n1. Model: Manages data (Database). \\n2. View: Displays information (UI). \\n3. Controller: Handles requests (Logic). \\n     Example Technologies: Django (Python), Laravel (PHP), Angular. \\n \\n1.6 Pipe-and-Filter Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Description: Data flows through filters (processing units) connected by pipes (data \\nstream). \\n  Best for: Data processing systems, AI/ML pipelines. \\nExample: Image Processing System \\n1. Filter 1: Read image file. \\n2. Filter 2: Apply color correction. \\n3. Filter 3: Save processed image. \\n     Example Technologies: Apache Spark, TensorFlow pipelines. \\n \\n1.7 Service-Oriented Architecture (SOA) \\n  Description: Uses reusable services that communicate via SOAP or REST. \\n  Best for: Enterprise applications, ERP systems. \\nExample: Airline Reservation System \\n• Flight Booking Service \\n• Payment Processing Service \\n• Notification Service \\n     Example Technologies: SOAP , XML, RESTful APIs. \\n \\nConclusion \\nChoosing the right architectural style ensures software is scalable, maintainable, and \\nefficient. Layered, Microservices, Event-Driven, and MVC are some of the most commonly \\nused architectures depending on business needs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='1.Five Phases of Product Lifecycle Development – Detailed Explanation \\nProduct Lifecycle Development (PLM) is a strategic approach that ensures a product evolves \\nefficiently from its initial concept to its end-of-life phase. This structured process helps organizations \\nstreamline product development, optimize resources, reduce costs, and improve overall product \\nquality. \\n \\n1. Concept & Ideation Phase (Idea Generation & Feasibility) \\nObjective: \\nTo generate and evaluate innovative product ideas based on market needs, technological feasibility, \\nand business goals. \\nKey Activities: \\n1. Market Research & Consumer Insights: \\no Identifying industry trends and market gaps. \\no Conducting customer surveys, focus groups, and competitor analysis. \\no Understanding pain points and unmet needs. \\n2. Brainstorming & Innovation: \\no Encouraging idea generation within R&D teams. \\no Utilizing design thinking methodologies for creative solutions. \\no Engaging stakeholders (marketing, sales, and engineering) to validate concepts. \\n3. Technical & Economic Feasibility Study: \\no Assessing the technological feasibility of the product. \\no Performing cost analysis, return on investment (ROI), and risk assessments. \\no Evaluating sustainability and regulatory compliance. \\n4. Product Requirement Definition: \\no Drafting high-level functional and technical requirements. \\no Setting initial specifications, including size, performance, and materials. \\no Identifying potential constraints (legal, environmental, and financial). \\nOutcome: \\n• Clear Product Concept Document (PCD) with specifications. \\n• Feasibility reports and go/no-go decision for product development. \\n \\n2. Design & Development Phase (Turning the Idea into a Prototype)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Objective: \\nTo transform the concept into a tangible product through detailed design, prototyping, and testing. \\nKey Activities: \\n1. Engineering & Product Design: \\no Creating Computer-Aided Design (CAD) models for visualization. \\no Selecting materials based on durability, cost, and sustainability. \\no Conducting stress tests, fluid dynamics simulations, and thermal analysis. \\n2. Prototype Development & Testing: \\no Building physical or digital prototypes for testing. \\no Running pilot tests to validate functionality, safety, and efficiency. \\no Gathering user feedback for improvements. \\n3. Software & Electronics Development (If Applicable): \\no Developing embedded software for IoT-enabled or digital products. \\no Ensuring cybersecurity measures and firmware updates. \\n4. Bill of Materials (BOM) & Supply Chain Planning: \\no Creating a multi-level BOM to define required parts and materials. \\no Identifying suppliers and manufacturers. \\n5. Regulatory & Compliance Checks: \\no Ensuring adherence to industry safety standards. \\no Getting necessary certifications (e.g., ISO, CE, FDA approvals). \\nOutcome: \\n• Fully functional prototype ready for validation. \\n• Finalized engineering documentation (BOM, design blueprints, regulatory reports). \\n \\n3. Manufacturing & Production Phase (Scaling Up for Market Readiness) \\nObjective: \\nTo transition from prototype to large-scale production while optimizing quality, efficiency, and cost. \\nKey Activities: \\n1. Production Process Planning: \\no Determining manufacturing methods (e.g., 3D printing, injection molding, CNC \\nmachining).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Optimizing production workflow for minimal waste. \\n2. Factory Setup & Automation: \\no Implementing smart manufacturing technologies such as IoT and AI-driven \\nautomation. \\no Establishing quality control stations and in-line testing. \\n3. Supply Chain & Logistics Optimization: \\no Coordinating with suppliers to ensure material availability. \\no Managing inventory using Enterprise Resource Planning (ERP) systems. \\n4. Quality Control & Testing: \\no Running stress, endurance, and safety tests on mass-produced units. \\no Conducting failure analysis and corrective actions. \\n5. Packaging & Distribution Readiness: \\no Designing sustainable packaging solutions to minimize waste. \\no Establishing distribution networks for global reach. \\nOutcome: \\n• Fully manufactured product meeting all safety, compliance, and market requirements. \\n• Scalability plan for mass production. \\n \\n4. Launch & Market Deployment Phase (Bringing the Product to Customers) \\nObjective: \\nTo introduce the product to the market through strategic sales, marketing, and distribution efforts. \\nKey Activities: \\n1. Go-To-Market (GTM) Strategy: \\no Defining pricing strategies (premium, penetration, skimming). \\no Selecting appropriate sales channels (e-commerce, retail, B2B partnerships). \\n2. Marketing & Branding: \\no Developing advertising campaigns (digital, social media, traditional). \\no Conducting product launch events and demos. \\n3. Customer Support & Service Preparation: \\no Setting up after-sales support teams for issue resolution. \\no Creating training materials for customers and retailers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='4. Feedback Collection & Market Adaptation: \\no Monitoring customer feedback via online reviews and surveys. \\no Adjusting marketing strategies based on sales performance. \\nOutcome: \\n• Successful market penetration and brand awareness. \\n• Sales data for performance evaluation. \\n \\n5. Service, Maintenance & End-of-Life (EOL) Phase (Sustaining & Retiring the Product) \\nObjective: \\nTo maintain product quality, enhance customer experience, and plan for its eventual phase-out. \\nKey Activities: \\n1. Customer Support & Warranty Management: \\no Providing help desks, chatbots, and service centers. \\no Handling repairs, replacements, and upgrades. \\n2. Software Updates & Security Patches (For Digital Products): \\no Rolling out firmware updates to fix vulnerabilities. \\no Ensuring compatibility with evolving technologies. \\n3. Product Enhancement & Redesign (If Applicable): \\no Gathering user feedback for next-gen product development. \\no Addressing common issues for an improved version. \\n4. End-of-Life (EOL) Planning & Sustainable Disposal: \\no Phasing out obsolete products responsibly. \\no Implementing circular economy strategies (recycling, repurposing). \\nOutcome: \\n• Extended product lifespan through continued support and updates. \\n• Responsible retirement or next-generation product planning. \\n \\nConclusion \\nThe five-phase PLM approach ensures products are innovative, efficiently developed, successfully \\nlaunched, and well-maintained. By leveraging advanced technologies like AI, IoT, and digital twins, \\ncompanies can optimize each phase, improve customer satisfaction, and drive long-term profitability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='7. Explain the steps for implementing a PLM strategy i want the answer for 15 marks so provide a \\ndetailed answer \\nChatGPT said: \\nSteps for Implementing a Product Lifecycle Management (PLM) Strategy \\nImplementing a PLM strategy is a complex yet critical process that enables businesses to manage a \\nproduct’s lifecycle from ideation to disposal. A well-structured PLM strategy enhances collaboration, \\nefficiency, innovation, and cost-effectiveness across the product’s journey. Below are the key steps \\nfor implementing a PLM strategy, explained in detail. \\n \\n1. Define Clear Business Objectives & Goals \\nBefore implementing PLM, organizations must clearly define their strategic objectives. These goals \\nshould align with the company’s overall business strategy. \\nKey Considerations: \\n• What problems will PLM solve? \\no Reduce product development time? \\no Improve collaboration? \\no Ensure regulatory compliance? \\n• Set Measurable Goals: \\no Reduce time-to-market by 20%. \\no Improve design efficiency by 30%. \\no Increase product reuse by 40%.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA car manufacturer wants to reduce design errors and improve collaboration between global teams \\nby implementing a PLM system. \\n \\n2. Conduct a Current State Assessment & Gap Analysis \\nA thorough assessment of the current product development process helps identify existing \\nchallenges, inefficiencies, and areas for improvement. \\nKey Considerations: \\n• Analyze Existing Processes & Tools: \\no How is product data managed today? \\no Are teams using spreadsheets, emails, or outdated software? \\n• Identify Gaps & Challenges: \\no Lack of real-time collaboration between teams. \\no Difficulty in tracking design changes and approvals. \\nExample: \\nAn electronics company finds that using manual document control for product data causes frequent \\nversion mismatches and production errors. \\n \\n3. Select the Right PLM System \\nChoosing the right PLM software is crucial for effective implementation. The selection should be \\nbased on the company’s size, industry, complexity, and long-term goals. \\nKey Considerations: \\n• PLM Features to Look for: \\no Cloud-based or on-premise? \\no Integration with ERP , CAD, and other systems? \\no User-friendly interface and accessibility? \\n• Popular PLM Solutions: \\no Siemens Teamcenter \\no PTC Windchill \\no Dassault Systèmes ENOVIA \\no Autodesk Fusion Lifecycle \\nExample:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='A medical device company selects PTC Windchill because it ensures FDA compliance and \\ntraceability for medical regulations. \\n \\n4. Develop a PLM Implementation Roadmap \\nA step-by-step implementation plan ensures smooth adoption of PLM without disrupting existing \\noperations. \\nKey Considerations: \\n• Phased Implementation vs. Full Rollout: \\no Start with a pilot program before enterprise-wide rollout. \\no Prioritize critical departments (e.g., R&D, Engineering). \\n• Implementation Timeline: \\no Phase 1: System Setup & Data Migration (3 months) \\no Phase 2: Integration with CAD & ERP (6 months) \\no Phase 3: Enterprise-wide Adoption (12 months) \\nExample: \\nAn aerospace company first implements PLM in design and engineering, then gradually expands it \\nto manufacturing and supply chain management. \\n \\n5. Migrate & Standardize Product Data \\nPLM requires clean, structured, and standardized product data for efficiency. This step involves data \\nmigration from legacy systems and ensuring a single source of truth for product information. \\nKey Considerations: \\n• Identify Critical Data to Migrate: \\no CAD files, BOM (Bill of Materials), compliance records. \\no Supplier information and past project documentation. \\n• Data Standardization: \\no Use common naming conventions and classification systems. \\no Ensure version control to avoid duplication errors. \\nExample: \\nA smartphone manufacturer standardizes its BOM structure in PLM to improve supplier \\ncoordination. \\n \\n6. Integrate PLM with Existing Enterprise Systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='PLM works best when integrated with other business-critical systems like ERP (Enterprise Resource \\nPlanning), CRM (Customer Relationship Management), and MES (Manufacturing Execution \\nSystems). \\nKey Considerations: \\n• Essential Integrations: \\no PLM + ERP: Ensures smooth manufacturing & procurement. \\no PLM + CAD: Seamless design data management. \\no PLM + CRM: Tracks customer feedback for product improvements. \\n• API & Data Flow Management: \\no Ensure real-time synchronization between PLM and other systems. \\nExample: \\nA fashion brand integrates PLM with ERP to manage raw materials, supplier orders, and production \\nschedules. \\n \\n7. Define & Automate Workflow Management \\nPLM improves efficiency by automating workflows such as design approvals, change requests, and \\ncompliance checks. \\nKey Considerations: \\n• Workflow Automation Examples: \\no Automated approval process for design changes. \\no Automatic alerts for compliance violations. \\n• Role-Based Access Control: \\no Ensure employees can access only relevant product data. \\nExample: \\nA pharmaceutical company automates FDA documentation approval workflows in its PLM system. \\n \\n8. Train Employees & Change Management \\nEmployees must understand and adapt to the new PLM system. Training and support are critical for \\nsuccessful adoption. \\nKey Considerations: \\n• Employee Training: \\no Conduct PLM workshops and hands-on practice sessions. \\no Provide user manuals and on-demand video tutorials.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• Change Management Strategies: \\no Address employee resistance by demonstrating PLM benefits. \\no Use pilot programs before full-scale implementation. \\nExample: \\nA car manufacturer trains engineers, supply chain managers, and designers to effectively use PLM \\nfor collaboration. \\n \\n9. Monitor Performance & Continuous Improvement \\nPost-implementation, companies must monitor PLM system performance and make improvements \\nbased on real-time data. \\nKey Considerations: \\n• Key PLM Performance Metrics: \\no Time-to-market reduction. \\no Number of design change requests. \\no Cost savings from improved efficiency. \\n• Continuous Improvement Approach: \\no Regular feedback from employees. \\no System updates and feature enhancements. \\nExample: \\nA consumer electronics company monitors PLM efficiency and adjusts workflows to further optimize \\nproduct development cycles. \\n \\nConclusion: Why a Structured PLM Strategy is Essential \\nA well-planned PLM strategy streamlines product development, collaboration, cost efficiency, and \\ninnovation. Companies that implement PLM effectively benefit from: \\n   Reduced time-to-market (faster product launches). \\n   Better data accuracy & version control. \\n   Improved supply chain coordination. \\n   Seamless compliance & regulatory management. \\nBy following these nine steps, organizations can achieve sustainable product innovation and market \\nleadership.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='12,4. Importance of Process Consistency in \\nProduct Lifecycle Management (PLM) \\nIntroduction \\nProcess consistency is a fundamental pillar of effective Product Lifecycle Management (PLM). It \\nensures standardized workflows, minimizes errors, enhances collaboration, and improves overall \\nefficiency in product development. PLM integrates multiple processes across design, manufacturing, \\nsupply chain, and customer service, making consistency crucial for seamless operations. \\nIn this detailed explanation, we will explore why process consistency is important and how it \\ncontributes to an organization’s efficiency, compliance, quality, and innovation. \\n \\n1. Enhances Efficiency & Reduces Operational Bottlenecks \\nWhy It Matters? \\nInconsistent processes lead to delays, confusion, and inefficiencies in product development. A lack \\nof uniformity in workflows causes teams to waste time resolving discrepancies, redoing tasks, and \\nhandling miscommunications. \\nBenefits of Process Consistency in Efficiency: \\n   Streamlined Workflows: Standardized procedures enable teams to follow clear steps for design, \\napprovals, and production. \\n   Faster Product Development: Consistent workflows minimize time spent on unnecessary \\nrevisions or approvals. \\n   Optimized Resource Utilization: Teams can focus on innovation rather than fixing process-\\nrelated issues.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA smartphone manufacturer follows a standardized change request approval process, reducing \\ndesign iteration delays and accelerating time-to-market. \\n \\n2. Ensures Compliance with Industry Standards & Regulations \\nWhy It Matters? \\nMany industries, such as automotive, aerospace, healthcare, and electronics, have strict regulatory \\nrequirements (ISO, FDA, CE, etc.). Without process consistency, tracking compliance documents and \\napprovals becomes chaotic. \\nBenefits of Process Consistency in Compliance: \\n   Traceability & Audit Readiness: Ensures all design changes, approvals, and certifications are \\nwell-documented. \\n   Reduces Risk of Regulatory Penalties: Prevents non-compliance issues that could lead to legal \\nactions or product recalls. \\n   Simplifies Certification Processes: Structured processes enable faster certification and approval \\nfrom regulatory bodies. \\n        Example: \\nA pharmaceutical company maintains a consistent documentation process for FDA approvals, \\npreventing delays in drug launches. \\n \\n3. Improves Collaboration Across Departments & Supply Chains \\nWhy It Matters? \\nModern product development involves multiple departments and global supply chains. Without \\nconsistency, different teams may use varied formats, tools, and workflows, leading to \\nmiscommunication and data silos. \\nBenefits of Process Consistency in Collaboration: \\n   Clear Communication Between Teams: Design, engineering, manufacturing, and suppliers follow \\na common workflow. \\n   Seamless Data Sharing: Standardized data formats ensure compatibility across PLM, ERP, and \\nCAD systems. \\n   Faster Issue Resolution: A structured change management process ensures everyone adapts to \\nmodifications quickly. \\n        Example: \\nA car manufacturer uses a standardized PLM platform for real-time collaboration between design \\nteams in the USA, Germany, and Japan, reducing errors in component specifications. \\n \\n4. Enhances Product Quality & Reduces Errors'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Why It Matters? \\nInconsistent processes can lead to design defects, manufacturing errors, and quality control \\nfailures. Standardization ensures error-free product development by defining clear quality \\ncheckpoints. \\nBenefits of Process Consistency in Quality Control: \\n   Early Detection of Defects: Ensures rigorous testing and validation at every stage. \\n   Reduces Manufacturing Errors: A structured Bill of Materials (BOM) process eliminates \\nmisconfigurations. \\n   Improves Customer Satisfaction: Ensures products meet or exceed quality expectations. \\n        Example: \\nAn aerospace company follows a standardized engineering change order (ECO) process, reducing \\nerrors in aircraft component assembly. \\n \\n5. Facilitates Scalability & Adaptability for Future Growth \\nWhy It Matters? \\nOrganizations need to scale their operations and adapt to new technologies. Without process \\nconsistency, scaling leads to chaos, inefficiencies, and quality issues. \\nBenefits of Process Consistency in Scalability: \\n   Easier Onboarding of New Employees & Suppliers: Standardized processes allow quick \\nadaptation. \\n   Smooth Expansion into New Markets: Ensures regulatory compliance and operational \\nalignment across regions. \\n   Enables Digital Transformation: Structured workflows help organizations adopt AI, IoT, and \\ncloud-based PLM solutions. \\n        Example: \\nA tech company expands its manufacturing to Asia and seamlessly integrates new suppliers due to \\nstandardized PLM workflows. \\n \\n6. Strengthens Change Management & Version Control \\nWhy It Matters? \\nFrequent product design changes are common in industries like electronics, automotive, and \\nmedical devices. A lack of process consistency leads to mismanaged updates, version confusion, \\nand costly recalls. \\nBenefits of Process Consistency in Change Management: \\n   Clear Change Request Approvals: Prevents uncontrolled modifications to product designs. \\n   Improved Version Control: Ensures teams work on the latest design versions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content=\"Minimizes Risk of Production Errors: Prevents manufacturers from using outdated product \\nspecifications. \\n        Example: \\nA smartwatch company maintains a single version of truth in PLM, preventing outdated firmware \\nfrom being installed on new models. \\n \\n7. Enables Cost Reduction & Maximizes ROI \\nWhy It Matters? \\nInconsistent processes lead to waste, rework, and inefficiencies that increase costs. Standardized \\nworkflows optimize costs and maximize return on investment (ROI). \\nBenefits of Process Consistency in Cost Optimization: \\n   Minimizes Rework & Waste: Reduces errors in design and production. \\n   Optimizes Resource Utilization: Prevents overproduction, material waste, and redundant efforts. \\n   Reduces Product Recalls & Failures: Ensures error-free product launches. \\n        Example: \\nA home appliance manufacturer cut production costs by 15% by standardizing supply chain \\nprocurement and assembly workflows. \\n \\nConclusion: Why Process Consistency is a Game-Changer \\nKey Takeaways: \\n✔ Ensures efficiency & faster time-to-market. \\n✔ Guarantees compliance & regulatory approval. \\n✔ Improves collaboration across teams & suppliers. \\n✔ Reduces errors & enhances product quality. \\n✔ Supports scalability & future growth. \\n✔ Strengthens change management & version control. \\n✔ Optimizes costs & maximizes ROI. \\nA structured, consistent PLM process is essential for companies aiming to innovate, reduce risks, \\nand stay competitive in today's fast-paced market.         \\n \\nWould you like an industry-specific case study on how Tesla, Apple, or Boeing leverage process \\nconsistency in PLM? Let me know!        \\n3.Explain the importance of product reuse and \\nsmart parts . \\nChatGPT said:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Importance of Product Reuse and Smart Parts in PLM \\nIntroduction \\nProduct reuse and smart parts are essential strategies in Product Lifecycle Management (PLM) to \\nenhance efficiency, reduce costs, and improve product quality. Organizations in industries like \\nautomotive, aerospace, electronics, and manufacturing leverage these concepts to accelerate \\ninnovation while maintaining sustainability and profitability. \\nIn this detailed explanation, we will explore: \\n• The importance of product reuse \\n• The role of smart parts \\n• Their benefits in PLM \\n \\n1. Importance of Product Reuse in PLM \\nWhat is Product Reuse? \\nProduct reuse refers to the practice of reusing existing designs, components, or modules in new \\nproduct development instead of creating them from scratch. This strategy enhances efficiency and \\npromotes sustainability. \\nKey Benefits of Product Reuse \\n   1. Reduces Development Time and Costs \\n  Eliminates the need for reinventing the wheel by leveraging existing parts and designs. \\n  Saves time and resources on design, testing, and validation. \\n  Engineers can focus on innovation rather than repetitive tasks. \\n        Example: \\nA smartphone company reuses the battery module and camera sensors from its previous model in a \\nnew version, significantly reducing R&D time and costs. \\n \\n   2. Enhances Product Quality and Reliability \\n  Reused components are already tested and validated, reducing defects. \\n  Improves product consistency by using proven designs. \\n  Minimizes unexpected failures in production. \\n        Example: \\nAn automotive manufacturer uses the same braking system across multiple car models, ensuring \\nreliability and performance. \\n \\n   3. Improves Supply Chain Efficiency'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Standardized parts streamline procurement and inventory management. \\n  Reduces dependency on multiple suppliers, lowering supply chain risks. \\n  Enables bulk purchasing, reducing overall costs. \\n        Example: \\nAn aerospace company standardizes fasteners, sensors, and actuators across aircraft models, \\nsimplifying supplier relationships. \\n \\n   4. Supports Sustainability and Environmental Goals \\n  Reduces material waste and lowers carbon footprint. \\n  Encourages eco-friendly manufacturing by minimizing redundant production. \\n  Promotes circular economy principles in product design. \\n        Example: \\nA consumer electronics brand reuses materials from recycled laptops to manufacture new models, \\nreducing electronic waste. \\n \\n   5. Accelerates Innovation and Scalability \\n  Speeds up new product introductions by leveraging existing modules. \\n  Enables mass customization by using a modular design approach. \\n  Ensures faster adaptation to market changes and customer demands. \\n        Example: \\nA medical device company reuses core software algorithms from a previous imaging system to \\ndevelop a next-generation scanner with AI capabilities. \\n \\n2. Importance of Smart Parts in PLM \\nWhat are Smart Parts? \\nSmart parts are intelligent, connected components that: \\n   Contain embedded sensors & chips for real-time monitoring. \\n   Enable data-driven decision-making in manufacturing. \\n   Improve predictive maintenance and self-diagnosis. \\nKey Benefits of Smart Parts \\n   1. Enables Real-Time Monitoring & Predictive Maintenance \\n  Smart parts collect data on performance, wear, and failure. \\n  Helps in preventive maintenance, reducing unexpected breakdowns. \\n  Extends product lifespan by optimizing usage conditions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA jet engine manufacturer installs IoT-enabled sensors in engines to monitor fuel efficiency and \\npredict maintenance needs. \\n \\n   2. Enhances Product Customization & Personalization \\n  Allows dynamic adjustments in performance based on user preferences. \\n  Supports on-the-fly reconfiguration for specific applications. \\n  Improves user experience through adaptive technology. \\n        Example: \\nA smart home appliance brand designs AI-powered washing machines that adjust wash cycles \\nbased on fabric type and weight. \\n \\n   3. Strengthens Supply Chain and Inventory Management \\n  Smart parts enable automated tracking and inventory updates. \\n  Reduce waste by monitoring usage patterns in real time. \\n  Enhances supply chain visibility, preventing shortages. \\n        Example: \\nAn automotive manufacturer uses RFID-equipped smart parts to track components in real time, \\nensuring smooth production flow. \\n \\n   4. Facilitates Digital Twin Implementation \\n  Smart parts provide real-time data for digital twin simulations. \\n  Helps in virtual prototyping, troubleshooting, and performance optimization. \\n  Reduces physical testing costs and accelerates innovation. \\n        Example: \\nA manufacturing plant creates a digital twin of its assembly line using data from smart robotic arms, \\noptimizing efficiency. \\n \\n   5. Increases Safety and Compliance \\n  Smart parts monitor temperature, pressure, and stress levels, preventing failures. \\n  Ensures compliance with industry safety regulations. \\n  Alerts operators to hazardous conditions in real time. \\n        Example: \\nAn oil & gas company uses smart pipeline sensors to detect leaks and prevent environmental \\nhazards.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Conclusion: Why Product Reuse & Smart Parts Are Game-Changers in PLM \\nKey Takeaways: \\n✔ Product Reuse reduces costs, accelerates development, and enhances sustainability. \\n✔ Smart Parts improve efficiency, enable real-time monitoring, and enhance product intelligence. \\n✔ Both strategies optimize the PLM process by reducing waste, improving quality, and ensuring \\nregulatory compliance. \\n✔ Organizations adopting reuse and smart part technologies gain a competitive advantage in the \\nglobal market. \\n        Future Outlook: \\n• AI-driven smart part analytics for predictive maintenance. \\n• Blockchain integration for tracking reusable parts in the supply chain. \\n• 3D printing & additive manufacturing for on-demand smart part production. \\nYou said: \\n11.Explain teamcenter workflow designer application \\nprovide detailed answer \\nChatGPT said: \\nTeamcenter Workflow Designer Application – A Detailed Explanation \\nIntroduction to Teamcenter Workflow Designer \\nTeamcenter Workflow Designer is a critical application within Siemens Teamcenter PLM (Product \\nLifecycle Management) that enables organizations to automate, standardize, and manage business \\nprocesses effectively. It provides a graphical interface for designing, configuring, and optimizing \\nworkflows, ensuring smooth collaboration between teams, departments, and external stakeholders. \\nThis tool is primarily used for: \\n   Automating approval processes (e.g., engineering change orders, document approvals). \\n   Defining roles, responsibilities, and process sequences. \\n   Ensuring process consistency across different product lifecycle stages. \\n \\nKey Features of Teamcenter Workflow Designer \\n1. Graphical Workflow Modeling \\n  Provides a drag-and-drop interface to design workflows visually. \\n  Users can create flowcharts, task sequences, and decision points. \\n  Reduces the need for manual coding when defining process logic. \\n        Example: \\nAn engineering team sets up an ECN (Engineering Change Notification) workflow, ensuring each \\ndesign change is reviewed by the right stakeholders before implementation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='2. Role-Based Task Assignment \\n  Assigns workflow steps based on user roles and access rights. \\n  Ensures the right engineers, managers, and stakeholders review the task at the right time. \\n  Supports parallel or sequential task execution. \\n        Example: \\nA design change request is automatically routed to the design engineer, quality manager, and \\nproduction lead, ensuring compliance before approval. \\n \\n3. Workflow Triggers & Conditions \\n  Enables workflows to start based on predefined events, conditions, or user actions. \\n  Automates decisions based on data inputs, status updates, or external triggers. \\n  Ensures compliance with standard operating procedures (SOPs). \\n        Example: \\nA manufacturing defect detected in the system automatically triggers a Corrective Action Process in \\nTeamcenter. \\n \\n4. Workflow Process Customization \\n  Users can create custom scripts for advanced automation. \\n  Supports integration with third-party tools and enterprise systems (ERP, MES, etc.). \\n  Provides flexibility for adapting workflows to unique business needs. \\n        Example: \\nA company integrates Teamcenter with SAP ERP , enabling automatic purchase order creation once a \\npart is approved in Teamcenter. \\n \\n5. Task Status Tracking & Monitoring \\n  Real-time visibility into workflow progress, bottlenecks, and pending tasks. \\n  Provides dashboards & reports for workflow analytics. \\n  Alerts users about pending approvals or overdue tasks. \\n        Example: \\nA project manager receives an alert when an engineering change approval is delayed, allowing them \\nto take corrective action. \\n \\nHow Teamcenter Workflow Designer Works – Step-by-Step Process \\n   Step 1: Define the Workflow Objectives'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Identify the business process to automate (e.g., design approval, change management). \\n✔ Establish workflow rules, required approvals, and role assignments. \\n        Example: \\nA company needs a New Product Introduction (NPI) workflow, ensuring design, compliance, and \\ncost approvals before production starts. \\n \\n   Step 2: Create the Workflow in Teamcenter \\n✔ Open Teamcenter Workflow Designer and select \"Create New Workflow.\" \\n✔ Define tasks, approvals, and decision gates. \\n✔ Configure dependencies (e.g., approvals must happen before production starts). \\n        Example: \\nA supplier part validation workflow includes tasks like supplier audit, material testing, and \\nprocurement approval. \\n \\n   Step 3: Assign Roles & Permissions \\n✔ Assign tasks to specific users, departments, or external stakeholders. \\n✔ Define access control to protect sensitive data. \\n        Example: \\nA regulatory compliance workflow assigns review tasks to engineering, legal, and compliance \\nteams. \\n \\n   Step 4: Configure Workflow Conditions & Triggers \\n✔ Set up automatic triggers for workflow initiation (e.g., upon document submission). \\n✔ Define decision rules (e.g., reject an engineering change if cost exceeds the budget). \\n        Example: \\nIf a cost analysis report exceeds the target budget, the workflow triggers a rework request to the \\nfinance team. \\n \\n   Step 5: Test & Validate the Workflow \\n✔ Run a test case to check for errors, inefficiencies, and bottlenecks. \\n✔ Ensure smooth integration with existing business systems. \\n        Example: \\nA manufacturing company tests an assembly process validation workflow, ensuring that all quality \\nchecks are correctly assigned before implementation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Step 6: Deploy & Monitor Workflow Execution \\n✔ Launch the workflow in a live environment. \\n✔ Track progress using Teamcenter reporting tools. \\n✔ Optimize the process based on real-time feedback and analytics. \\n        Example: \\nA project manager notices approval delays in an engineering change request workflow, prompting a \\nprocess optimization review. \\n \\nBenefits of Using Teamcenter Workflow Designer \\n        1. Increases Process Efficiency \\n✔ Automates repetitive tasks, reducing manual work. \\n✔ Eliminates process delays by ensuring real-time tracking & notifications. \\n        2. Ensures Compliance & Standardization \\n✔ Adheres to industry regulations and company policies. \\n✔ Provides an audit trail for all approvals and workflow actions. \\n        3. Improves Collaboration \\n✔ Connects multiple teams, including engineering, manufacturing, supply chain, and compliance. \\n✔ Provides role-based task assignments, ensuring the right people handle the right tasks. \\n        4. Enhances Decision-Making \\n✔ Provides real-time workflow visibility. \\n✔ Helps managers identify bottlenecks and inefficiencies. \\n        5. Reduces Errors & Rework \\n✔ Prevents unauthorized changes in product designs and approvals. \\n✔ Ensures each process follows a predefined sequence, avoiding skipped steps. \\n \\nUse Cases of Teamcenter Workflow Designer \\n     1. Engineering Change Management (ECM) \\nScenario: An automotive company automates design change approvals using workflow designer, \\nensuring regulatory compliance before releasing new vehicle models. \\n     2. New Product Development (NPD) Approval \\nScenario: A medical device company standardizes its New Product Development (NPD) workflow, \\nintegrating testing, validation, and FDA compliance. \\n     3. Supplier Onboarding & Part Qualification'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Scenario: An aerospace manufacturer ensures supplier audits, material testing, and compliance \\napprovals before integrating new suppliers into the supply chain. \\n \\nConclusion: Why Use Teamcenter Workflow Designer? \\n✔ Automates & standardizes processes, improving efficiency. \\n✔ Ensures compliance with industry regulations & quality standards. \\n✔ Enhances collaboration across departments & global teams. \\n✔ Reduces errors & improves decision-making through real-time tracking. \\n✔ Boosts product lifecycle management by optimizing workflows for faster execution. \\n9.Distinguish between Current Information Model and \\nPLM model provide detailed answer \\nChatGPT said: \\nDistinguishing Between the Current Information Model and the PLM Model \\nIntroduction \\nInformation management is crucial for engineering, manufacturing, and business processes. Two \\nprimary approaches to handling information are: \\n1. Current Information Model (CIM) – The traditional way organizations store and manage \\ndata. \\n2. Product Lifecycle Management (PLM) Model – A more advanced, structured, and \\ninterconnected way of managing product data and processes throughout the product’s \\nlifecycle. \\n \\n1. Definition of the Two Models \\n   Current Information Model (CIM): \\nThe Current Information Model (CIM) refers to the existing or legacy way organizations handle \\ninformation. This model is often siloed, lacks structured interconnectivity, and primarily focuses on \\nstoring data for specific departments or purposes without seamless integration across the product \\nlifecycle. \\n  Characteristics of CIM: \\n✔ Data is stored in separate databases. \\n✔ Limited integration between different departments (e.g., design, production, and marketing). \\n✔ Information is often stored in documents, spreadsheets, or legacy systems. \\n✔ Changes in one area don’t automatically update across the entire organization. \\n        Example: A company stores its engineering designs in CAD software, manufacturing data in ERP, \\nand supplier information in Excel files, leading to data inconsistencies.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='PLM Model: \\nThe Product Lifecycle Management (PLM) Model is a centralized, integrated, and structured \\napproach to managing product data and processes across the entire product lifecycle. It creates a \\nsingle source of truth (SSOT) for product-related information, ensuring collaboration, version \\ncontrol, and traceability. \\n  Characteristics of the PLM Model: \\n✔ Single centralized repository for all product-related data. \\n✔ Seamless integration between design, engineering, manufacturing, sales, and service. \\n✔ Version control & change management to track updates and approvals. \\n✔ Supports collaboration between teams, including suppliers and customers. \\n        Example: A PLM system like Teamcenter or Windchill stores and links CAD models, BOMs (Bill \\nof Materials), simulation data, compliance documents, and change histories in one place. \\n \\n2. Key Differences Between CIM and PLM Model \\nAspect Current Information Model (CIM) PLM Model \\nData Storage Siloed in multiple systems (CAD, ERP , Excel) Centralized database with \\nstructured access \\nData Consistency Prone to duplication and inconsistency Single source of truth (SSOT) \\nensures consistency \\nCollaboration Limited; data exchange between \\ndepartments is manual \\nSeamless collaboration across \\nteams, suppliers, and partners \\nChange \\nManagement \\nChanges must be manually communicated, \\noften leading to errors \\nAutomated workflows track and \\nmanage product changes \\nTraceability Difficult to trace product history across \\ndepartments \\nFull traceability from concept to \\nretirement \\nVersion Control Weak or non-existent; multiple versions of \\nthe same document may exist \\nRobust version control prevents \\nconflicts and data loss \\nRegulatory \\nCompliance \\nRequires manual tracking and \\ndocumentation \\nBuilt-in compliance tracking and \\nreporting \\nProcess \\nAutomation \\nMinimal automation, reliant on human \\nintervention \\nAutomated workflows, reducing \\nerrors and inefficiencies \\nCost of Errors High due to miscommunication and data \\nloss \\nLow, as errors are detected and \\nresolved early'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Aspect Current Information Model (CIM) PLM Model \\nScalability Hard to scale, as adding new features or \\nusers is complex \\nEasily scalable for large, global \\norganizations \\n \\n3. Importance of Moving from CIM to PLM Model \\nOrganizations that transition from the Current Information Model to a PLM Model experience: \\n✔ Faster time-to-market due to efficient workflows. \\n✔ Improved product quality due to better change management. \\n✔ Cost savings by reducing errors, redundancies, and rework. \\n✔ Better compliance & audit readiness for industry regulations. \\n✔ Enhanced innovation by integrating AI, IoT, and digital twins. \\n        Example: A global automotive manufacturer moves from Excel-based data storage (CIM) to \\nPLM-based digital product management. As a result, they reduce design iterations by 30% and \\nimprove collaboration between R&D, manufacturing, and suppliers. \\n \\n4. Conclusion \\n✔ The Current Information Model (CIM) is outdated, fragmented, and inefficient for modern \\nproduct development. \\n✔ The PLM Model provides an integrated, structured, and scalable approach to product data \\nmanagement, ensuring real-time collaboration, version control, and process automation. \\n✔ Transitioning to a PLM system is essential for digital transformation, competitive advantage, and \\nsustainable growth in today’s industries.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"UNIT-01 \\n2 MARKS \\n \\n1.Summarize the reasons for the domain expertise for \\nany type of data analytics. \\n \\nDomain expertise is crucial in data analytics because it provides the necessary context to understand \\ndata, identify relevant patterns, and interpret results, leading to better data quality, more \\nmeaningful insights, and ultimately, more effective decision-making.  \\nHere's a more detailed explanation of why domain expertise is so important: \\n• Data Quality Assessment: \\nDomain experts can identify anomalies, outliers, and potential biases in the data that might go \\nunnoticed by someone lacking domain knowledge, ensuring the data is reliable and accurate.  \\n• Feature Engineering: \\nDomain expertise helps identify which features are most relevant and meaningful for a particular \\nproblem, leading to more effective models and analysis.  \\n• Model Interpretation: \\nUnderstanding the domain context is crucial for interpreting the output of a data model and its \\nimplications in the real world, ensuring that the insights are actionable and relevant.  \\n• Understanding Data Context: \\nDomain knowledge helps analysts understand how data is collected, its sources, and the context in \\nwhich it exists, which is essential for proper data preprocessing and cleaning.  \\n• Identifying Actionable Insights: \\nDomain expertise allows analysts to translate data insights into practical recommendations and \\nsolutions that address real-world business issues.  \\n• Improved Communication: \\nDomain expertise facilitates better communication between data analysts and stakeholders, \\nensuring that everyone understands the findings and can make informed decisions.  \\n• Innovation and Opportunity Identification: \\nDomain experts can identify opportunities for innovation and new approaches to problem-solving \\nbased on their understanding of the industry and data.  \\n• Tailored Solutions: \\nDomain knowledge allows for the development of solutions that are tailored to the specific needs \\nand challenges of a particular industry or business.  \\n \\n \\n1. Summarize the reasons for domain expertise in any type of data analytics. \\nDomain expertise is essential in data analytics as it helps in understanding the context of data, \\nidentifying relevant trends, correlations, and anomalies, and ensuring accurate interpretation of \\nresults. Without domain knowledge, analysts may misinterpret insights, leading to incorrect \\nconclusions. Expertise in areas like healthcare, finance, or marketing ensures that data-driven \\ndecisions align with business objectives. \\nExample: In healthcare, a domain expert can help identify meaningful patterns in patient data, such \\nas risk factors for diseases, leading to better diagnosis and treatment recommendations.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"2.How the accuracy in big data is beneficial? \\n \\nData accuracy is vital to the success of all organizations—from sales to accounting and marketing to \\nhuman resources. Data informs decisions, creates impressions about an organization, and drives \\nrevenue. Reasons why data accuracy is important and a priority for the enterprise are that data \\naccuracy: \\n• Delivers better results to the organization’s users \\n• Drives more value from artificial intelligence implementations with accurate and consistent \\ndata to feed algorithms \\n• Enables better decision-making \\n• Enhances efficiency \\n• Makes it easier to achieve consistent results \\n• Mitigates risks associated with flawed data \\n• Provides confidence to users who depend on the data \\n• Reduces the need to spend time and money finding and fixing errors in the data \\n• Supports focused audience targeting and marketing efforts \\n2. How is accuracy in big data beneficial? \\nAccuracy in big data ensures better decision-making, predictive analytics, and fraud \\ndetection. High accuracy reduces errors, biases, and misinformation, leading to efficient \\nbusiness operations and improved customer experiences. \\nExample: In financial services, accurate big data analysis can detect fraudulent transactions \\nin real-time, preventing losses and enhancing security. \\n3.What are the top challenges in big data? \\nThe top challenges in big data revolve around managing its volume, velocity, \\nvariety, and veracity, along with data quality, security, and integration, as well as \\nfinding and retaining skilled talent.  \\nHere's a more detailed breakdown: \\n1. Volume, Velocity, Variety, and Veracity (The 4 Vs): \\n• Volume: \\nThe sheer amount of data generated and stored is immense, requiring robust \\nstorage and processing infrastructure.  \\n• Velocity: \\nData is generated and processed at an increasingly rapid pace, demanding real-\\ntime analytics and processing capabilities.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='• Variety: \\nBig data comes in various formats (structured, unstructured, semi-structured) \\nfrom diverse sources, requiring flexible data management and analysis tools.  \\n• Veracity: \\nData quality and accuracy are crucial, as inaccurate or incomplete data can lead \\nto flawed insights and decisions \\n3. What are the top challenges in big data? \\nBig data presents challenges such as data storage, processing speed, data \\nquality, security, and privacy. Managing the 3Vs (Volume, Velocity, Variety) is \\ndifficult, requiring advanced tools like Hadoop and Spark. Ensuring data \\nintegrity and protecting sensitive information from cyber threats is also critical. \\nExample: Social media companies face challenges in analyzing millions of posts \\nper second while filtering out fake news and maintaining user privacy. \\n \\n4,12.Distinguish between Big data and conventional \\ndata. \\n \\nTraditional Data  Big Data  \\nTraditional data is generated in \\nenterprise level. Big data is generated outside the enterprise level. \\nIts volume ranges from Gigabytes to \\nTerabytes. \\nIts volume ranges from Petabytes to Zettabytes or \\nExabytes. \\nTraditional database system deals \\nwith structured data. \\nBig data system deals with structured, semi-\\nstructured,database, and unstructured data. \\nTraditional data is generated per hour or \\nper day or more. \\nBut big data is generated more frequently mainly \\nper seconds. \\nTraditional data source is centralized \\nand it is managed in centralized form. \\nBig data source is distributed and it is managed in \\ndistributed form. \\nData integration is very easy. Data integration is very difficult.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"Traditional Data  Big Data  \\nNormal system configuration is capable \\nto process traditional data. \\nHigh system configuration is required to process \\nbig data. \\nThe size of the data is very small. The size is more than the traditional data size. \\nTraditional data base tools are required \\nto perform any data base operation. \\nSpecial kind of data base tools are required to \\nperform any database schema based operation. \\nNormal functions can manipulate data. Special kind of functions can manipulate data. \\nIts data model is strict schema based \\nand it is static. \\nIts data model is a flat schema based and it is \\ndynamic. \\nTraditional data is stable and inter \\nrelationship. Big data is not stable and unknown relationship. \\nTraditional data is in manageable \\nvolume. \\nBig data is in huge volume which becomes \\nunmanageable. \\nIt is easy to manage and manipulate the \\ndata. It is difficult to manage and manipulate the data. \\n \\n4. Distinguish between big data and conventional data. \\n• Big Data: Large, complex datasets that require specialized tools for analysis. It includes \\nstructured, unstructured, and semi-structured data from diverse sources. \\n• Conventional Data: Smaller, structured datasets stored in relational databases (RDBMS) \\nwith fixed schemas. \\nExample: \\n• Big Data: Real-time IoT sensor data from smart cities. \\n• Conventional Data: Employee records stored in an Excel sheet. \\n \\n5.Outline the role of big data analytics. \\n \\nBig data analytics plays a crucial role in modern organizations by enabling them to extract valuable \\ninsights from large datasets, leading to better decision-making, improved operations, and increased \\ncompetitiveness.  \\nHere's a more detailed outline: \\n1. Uncovering Hidden Patterns and Trends: \\n• Identifying Opportunities: \\nBig data analytics helps organizations discover hidden patterns, correlations, and market trends that \\nmight otherwise go unnoticed, allowing them to identify new opportunities for growth and \\ninnovation.  \\n• Predictive Analytics:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"By analyzing historical data, organizations can use big data analytics to predict future outcomes, \\nenabling them to make proactive decisions and mitigate potential risks.  \\n• Understanding Customer Behavior: \\nBig data analytics provides insights into customer behavior, preferences, and needs, allowing \\nbusinesses to personalize their offerings and improve customer experiences.  \\n \\n5. Outline the role of big data analytics. \\nBig data analytics helps in trend identification, predictive modeling, and real-time decision-\\nmaking. It enhances customer experiences, operational efficiency, and fraud detection. \\nExample: E-commerce companies like Amazon use big data analytics to recommend products \\nbased on user behavior. \\n \\n \\n6,15.State the four computing resources of Big Data \\nStorage. \\nThe four main computing resources for Big Data Storage are data storage, data mining, data \\nanalytics, and data visualization.  \\nHere's a more detailed explanation of each: \\n• Data Storage: \\nThis involves the methods and technologies used to store large, complex datasets, which can include \\ndata lakes, data warehouses, cloud storage, and object storage.  \\n• Data Mining: \\nThis process involves extracting valuable insights and patterns from large datasets using various \\ntechniques and algorithms.  \\n• Data Analytics: \\nThis focuses on analyzing and interpreting the extracted data to gain meaningful insights and make \\ninformed decisions.  \\n• Data Visualization: \\nThis involves presenting the analyzed data in a visual format, such as charts, graphs, and dashboards, \\nto facilitate understanding and communication of findings.  \\n \\n6. State the four computing resources of Big Data Storage. \\n1. Processing Power (CPU & GPU): Required for computing large datasets. \\n2. Storage (HDFS, Cloud Storage): Stores petabytes of structured and unstructured data. \\n3. Memory (RAM): Supports high-speed processing and real-time analytics. \\n4. Networking (High-Speed Data Transfer): Ensures seamless data movement across systems. \\nExample: Google Cloud provides high-performance computing for real-time big data processing. \\n \\n \\n7.What role does cloud computing play in Big Data \\nManagement? \\n \\nCloud Computing: The Big Data Solution \\nCloud computing offers an effective solution towards dealing with big size information \\nsets. Organizations can store their big-data efficiently manage them as well analyze them by \\nleveraging scalability provided through clouds on demand resources such as storage capacity\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='7. What role does cloud computing play in Big Data Management? \\nCloud computing provides scalability, cost-efficiency, remote accessibility, and real-time processing \\nfor big data applications. Platforms like AWS, Google Cloud, and Azure offer distributed computing \\nand AI-powered analytics. \\nExample: Netflix uses AWS cloud computing to process and recommend personalized content to \\nmillions of users. \\n \\n \\n8.List the various dimensions of growth of big data. \\n \\nThe growth of big data is often characterized by the \"5 V\\'s\": Volume, Velocity, Variety, Veracity, and \\nValue.  \\nHere\\'s a breakdown of each dimension: \\n• Volume: \\nRefers to the sheer amount of data being generated and collected, often measured in terabytes, \\npetabytes, or even exabytes.  \\n• Velocity: \\nDescribes the speed at which data is generated, collected, and processed, including real-time and \\nbatch processing.  \\n• Variety: \\nEncompasses the different types and formats of data, including structured, unstructured, and semi-\\nstructured data.  \\n• Veracity: \\nRelates to the accuracy, reliability, and trustworthiness of the data.  \\n• Value: \\nFocuses on the insights and actionable knowledge that can be derived from the data, enabling better  \\ndecision-making and business outcomes.  \\n \\n8. List the various dimensions of growth of big data. \\n1. Volume: Increase in the amount of data generated daily. \\n2. Velocity: Speed at which data is created and processed. \\n3. Variety: Diverse data types (text, images, audio, video). \\n4. Veracity: Ensuring data accuracy and reliability. \\n5. Value: Extracting meaningful insights for business impact. \\nExample: Twitter generates terabytes of text data every day, requiring advanced analytics. \\n \\n \\n9.Identify the risks involved in using big data. \\n \\nUsing big data presents risks including  \\ndata privacy breaches \\n security vulnerabilities \\n ethical concerns, \\n potential for misuse \\n managing large datasets.  \\n \\n9. Identify the risks involved in using big data. \\n• Privacy breaches: Exposure of sensitive user data. \\n• Security vulnerabilities: Risk of cyberattacks. \\n• Bias in AI models: Incorrect conclusions due to skewed data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='• Data inconsistency: Errors caused by poor-quality data. \\nExample: A social media company mishandling user data can lead to data breaches and legal \\nconsequences. \\n \\n \\n \\n10.With example explain structured data. \\nStructured data refers to information organized in a predefined format, making it easily searchable \\nand analyzable by computers, such as data in tables, databases, or spreadsheets. Examples \\ninclude customer names, addresses, dates, times, and product prices.  \\n• Spreadsheets (Excel): \\nData organized in rows and columns, similar to relational databases, but with more flexibility in \\nformatting and data types.  \\n \\n10. With an example, explain structured data. \\nStructured data is highly organized and stored in relational databases with predefined schemas. It is \\neasy to query using SQL. \\nExample: A customer database containing fields like Name, Age, Email, and Purchase History stored \\nin an SQL table. \\n \\n \\n11.State the best practices for Big Data Analytics. \\n \\n• Implement Data Quality Management Programs. \\n• Build More Scalable Infrastructures. \\n• Employ Agile Development Methodologies. \\n• Safeguard Data With Robust Security Measures. \\n• Use Data Ethically. \\n• Monitor and Optimize Continuously. \\n• Provide Workforce Skill Development. \\n11. State the best practices for Big Data Analytics. \\n1. Ensure data quality and consistency. \\n2. Use scalable and efficient storage solutions (HDFS, Cloud). \\n3. Implement security measures to protect sensitive data. \\n4. Leverage AI and ML for deeper insights. \\n5. Optimize processing speed with parallel computing. \\nExample: A bank using AI-driven fraud detection models must ensure data accuracy for reliable \\npredictions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='13.Compare data analytics and big data analytics. \\n \\nThe purpose of big data is to store huge \\nvolume of data and to process it. \\nThe purpose of data analytics is to \\nanalyze the raw data and find out \\ninsights for the information. \\nParallel computing and other complex \\nautomation tools are used to handle big \\ndata. \\nPredictive and statistical modelling with \\nrelatively simple tools used to handle \\ndata analytics. \\nBig data operations are handled by big data \\nprofessionals. \\nData analytics is performed by skilled \\ndata analysts. \\nBig data analysts need the knowledge of \\nprogramming, NoSQL databases, distributed \\nsystems, and frameworks. \\nData Analysts need the knowledge of \\nprogramming, statistics, and \\nmathematics. \\nIt supports in dealing with huge volumes of \\ndata. \\nIt supports in examining raw data and \\nrecognizing useful information. \\n \\n13. Compare data analytics and big data analytics. \\nFeature Data Analytics Big Data Analytics \\nData Size Small to Medium Very Large (Petabytes) \\nProcessing SQL, Excel Hadoop, Spark \\nComplexity Simple Queries AI, ML, Predictive \\nExample HR payroll data analysis Sentiment analysis of social media posts \\nIdentify the four computing resources of Big Data Storage. \\n \\n11,14. List out the best practices of Big Data Analytics. \\n• Define clear business objectives. \\n• Use the right technology stack (Spark, NoSQL). \\n• Ensure compliance with data privacy regulations (GDPR, HIPAA). \\n• Implement real-time monitoring for fraud detection. \\nExample: Amazon optimizes inventory using real-time big data analytics. \\n \\n16. Define big data and under what conditions it is \\ngiven that name. \\nBig Data refers to extremely large datasets that traditional databases cannot handle \\nefficiently. It is characterized by Volume, Velocity, Variety, Veracity, and Value. \\nExample: Facebook stores and processes billions of images, videos, and messages daily. \\n \\nWhy it\\'s called \"big data\":'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='The term \"big data\" is used because these datasets are so large and complex that they \\nrequire specialized technologies and tools to manage, store, and analyze them effectively.  \\n \\ncharacterized by the \"5 Vs\": volume, velocity, variety, veracity, and value.  \\n \\n• Examples: \\n• Social media posts, emails, and web traffic data.  \\n• Financial transaction records, customer databases, and inventory data.  \\n• Sensor data from IoT devices, medical records, and scientific experiments.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b01b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b33827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 138 documents into 283 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: 15 marks VDA \n",
      " \n",
      "1.Significance of Software Standards in OpenGL and How It Ensures Cross-Platform Compatibility \n",
      "Significance of Software Standards in OpenGL \n",
      "Software standards in OpenGL are essential...\n",
      "Metadata: {'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='15 marks VDA \\n \\n1.Significance of Software Standards in OpenGL and How It Ensures Cross-Platform Compatibility \\nSignificance of Software Standards in OpenGL \\nSoftware standards in OpenGL are essential for ensuring uniformity, performance, and compatibility \\nacross different computing environments. OpenGL is an open standard for graphics rendering, which \\nmeans it is maintained and updated by a consortium rather than a single company. The importance \\nof software standards in OpenGL can be explained through the following points: \\n1. Cross-Platform Consistency: \\nOpenGL provides a standardized API that enables graphics applications to run across multiple \\nplatforms, including Windows, macOS, Linux, and mobile operating systems like Android. This \\nreduces the need for platform-specific modifications and ensures consistent performance \\nacross different devices. \\n2. Hardware Abstraction: \\nOpenGL abstracts hardware details, allowing applications to interact with the GPU without'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='across different devices. \\n2. Hardware Abstraction: \\nOpenGL abstracts hardware details, allowing applications to interact with the GPU without \\nbeing tied to a specific graphics card manufacturer. This means developers do not have to \\nwrite separate code for AMD, NVIDIA, or Intel GPUs, as OpenGL provides a unified API. \\n3. Interoperability Across Vendors: \\nOpenGL follows a well-defined specification that multiple vendors adhere to. This ensures \\nthat OpenGL applications can run on different hardware implementations without significant \\nmodifications, fostering a competitive and diverse ecosystem. \\n4. Backward Compatibility: \\nOpenGL maintains backward compatibility between different versions, ensuring that \\napplications written for older OpenGL versions can still function on newer hardware with \\nminimal changes. This protects developers from frequent rewrites and ensures longevity for \\ngraphics applications. \\n5. Performance Optimization:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='minimal changes. This protects developers from frequent rewrites and ensures longevity for \\ngraphics applications. \\n5. Performance Optimization: \\nOpenGL standards define efficient rendering pipelines and data management techniques, \\nallowing applications to optimize GPU usage. Standardized API functions ensure that \\napplications use the best possible methods for rendering, irrespective of the underlying \\nhardware. \\n6. Scalability and Flexibility: \\nOpenGL is designed to support applications ranging from simple 2D graphics rendering to \\ncomplex 3D simulations and high-performance computing. It scales from embedded systems \\nto high-end gaming and visualization applications. \\n7. Adoption in Multiple Industries: \\nDue to its open nature and wide support, OpenGL is used in various industries, including \\ngaming, CAD (Computer-Aided Design), scientific visualization, virtual reality, and medical \\nimaging. This widespread adoption is made possible by standardized API guidelines.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content=\"gaming, CAD (Computer-Aided Design), scientific visualization, virtual reality, and medical \\nimaging. This widespread adoption is made possible by standardized API guidelines. \\n8. Facilitation of Open-Source Development: \\nOpenGL's standardized nature has led to the development of numerous open-source\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='graphics engines, frameworks, and libraries, fostering innovation and collaboration in the \\nsoftware development community. \\nHow OpenGL Ensures Cross-Platform Compatibility \\nOpenGL achieves cross-platform compatibility through several key mechanisms: \\n1. Platform-Independent API: \\nOpenGL provides a single, consistent API that remains the same across different operating \\nsystems. This means that the same OpenGL code can be compiled and executed on various \\nplatforms with minimal modifications. \\n2. Graphics Driver Support: \\nGPU manufacturers like NVIDIA, AMD, and Intel provide OpenGL-compliant drivers that \\nimplement the OpenGL standard for their specific hardware. These drivers handle the \\ntranslation of OpenGL API calls into optimized machine-level instructions for the GPU. \\n3. Extension Mechanism: \\nOpenGL supports an extension system that allows hardware vendors to introduce new \\nfeatures while maintaining compatibility with the core OpenGL standard. Developers can'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='3. Extension Mechanism: \\nOpenGL supports an extension system that allows hardware vendors to introduce new \\nfeatures while maintaining compatibility with the core OpenGL standard. Developers can \\nselectively use these extensions to take advantage of new hardware capabilities without \\nbreaking compatibility with older systems. \\n4. Use of Utility Libraries: \\nOpenGL does not handle system-specific tasks such as window management or input \\nhandling. Instead, libraries like GLUT (OpenGL Utility Toolkit) and GLFW (Graphics Library \\nFramework) provide cross-platform support for these functions, allowing OpenGL \\napplications to run on different operating systems seamlessly. \\n5. OpenGL Context Management: \\nOpenGL requires an OpenGL context to be created before rendering can begin. Libraries like \\nWGL (Windows), GLX (Linux), and CGL (macOS) handle the integration with the operating \\nsystem, ensuring smooth cross-platform support. \\n6. Shader Language Standardization (GLSL):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='WGL (Windows), GLX (Linux), and CGL (macOS) handle the integration with the operating \\nsystem, ensuring smooth cross-platform support. \\n6. Shader Language Standardization (GLSL): \\nOpenGL includes the OpenGL Shading Language (GLSL), which allows developers to write \\nGPU-accelerated shaders in a hardware-independent way. GLSL ensures that custom graphics \\neffects and rendering techniques can work across different GPU brands without modification. \\n7. Adherence to OpenGL Standards by the Khronos Group: \\nThe Khronos Group oversees the OpenGL specification, ensuring that all implementations \\nfollow a common standard. This prevents fragmentation and ensures a stable and \\npredictable development environment. \\n8. Support for Different Rendering Pipelines: \\nOpenGL supports both fixed-function and programmable pipelines, making it compatible \\nwith both older and modern hardware. The introduction of modern pipeline techniques like'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL supports both fixed-function and programmable pipelines, making it compatible \\nwith both older and modern hardware. The introduction of modern pipeline techniques like \\nVertex Buffer Objects (VBOs) and Framebuffer Objects (FBOs) further enhances its flexibility \\nacross different platforms. \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL’s adherence to software standards ensures that developers can build applications that work \\nseamlessly across different operating systems and hardware configurations. Its cross-platform \\ncapabilities are maintained through standardized APIs, driver support, extension mechanisms, and \\nshader language standardization. As a result, OpenGL remains one of the most widely used graphics \\nAPIs for real-time rendering, making it a crucial tool for developers across multiple industries. \\n \\n(8marks) \\n3. Advantages and Limitations of DDA and Bresenham’s Line Drawing Algorithms \\nLine drawing is a fundamental operation in computer graphics, and DDA (Digital Differential \\nAnalyzer) algorithm and Bresenham’s algorithm are two commonly used techniques for rasterizing \\nlines on a pixel-based display. Both methods work by determining which pixels should be illuminated \\nto best approximate a straight line. \\nEach algorithm has its strengths and weaknesses in terms of speed, accuracy, computational'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='to best approximate a straight line. \\nEach algorithm has its strengths and weaknesses in terms of speed, accuracy, computational \\nefficiency, and hardware implementation. Below is a detailed comparison of the advantages and \\nlimitations of both algorithms. \\n \\n1. Digital Differential Analyzer (DDA) Algorithm \\nOverview \\nThe Digital Differential Analyzer (DDA) algorithm is an incremental approach to line drawing. It \\ncalculates intermediate points between two endpoints using the slope-intercept equation of a line: \\ny=mx+cy = mx + cy=mx+c  \\nwhere m is the slope of the line. The algorithm incrementally updates x or y depending on the slope: \\n• If |m| ≤ 1, increment x and compute y. \\n• If |m| > 1, increment y and compute x. \\nThis method relies on floating-point arithmetic to determine pixel positions. \\n \\nAdvantages of DDA Algorithm \\n1. Simple and Easy to Implement \\no The logic follows the basic concept of linear interpolation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Advantages of DDA Algorithm \\n1. Simple and Easy to Implement \\no The logic follows the basic concept of linear interpolation. \\no The algorithm is easier to understand and implement compared to Bresenham’s \\nalgorithm. \\n2. Handles All Slopes Uniformly \\no DDA works for both steep and shallow slopes by adjusting the step size in either the \\nx or y direction. \\n3. Works Well with Floating-Point Arithmetic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Since DDA uses floating-point calculations, it provides higher precision in \\ndetermining pixel positions compared to purely integer-based methods. \\n4. Generalized for Other Shapes \\no The principles of DDA can be extended for circle drawing, curve generation, and \\nother geometric transformations in graphics. \\n \\nLimitations of DDA Algorithm \\n1. Computationally Expensive \\no DDA uses floating-point multiplication, division, and rounding operations, which \\nare slower than integer calculations. \\no This makes it less suitable for real-time applications and low-powered devices like \\nembedded systems. \\n2. Accumulation of Round-Off Errors \\no Since floating-point calculations are used, rounding errors accumulate over long \\nlines. \\no This can result in deviations from the true line as more pixels are plotted. \\n3. Less Efficient than Bresenham’s Algorithm \\no Requires additional multiplication and division operations in each step, making it \\ncomputationally slower than Bresenham’s algorithm.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='3. Less Efficient than Bresenham’s Algorithm \\no Requires additional multiplication and division operations in each step, making it \\ncomputationally slower than Bresenham’s algorithm. \\no Not ideal for applications that require high-speed rendering. \\n \\n2. Bresenham’s Line Drawing Algorithm \\nOverview \\nBresenham’s algorithm is an incremental decision-based approach that efficiently determines the \\nnext pixel using integer arithmetic instead of floating-point calculations. \\nIt avoids multiplication and division by using an error term (decision variable) to track the closest \\npixel. Instead of computing the exact floating-point values, it approximates the line by selecting the \\npixel that minimizes error accumulation. \\n \\nAdvantages of Bresenham’s Algorithm \\n1. More Efficient than DDA \\no Uses only integer addition and subtraction, making it significantly faster than the \\nDDA algorithm. \\no Suitable for real-time rendering and low-resource systems like embedded devices.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='2. Minimizes Round-Off Errors \\no Bresenham’s algorithm avoids floating-point calculations, eliminating accumulated \\nrounding errors found in DDA. \\no This makes it more accurate for pixel-based rendering. \\n3. Better Performance on Raster Displays \\no Since it directly selects the next pixel based on integer calculations, it optimally fits \\ninto raster display systems. \\no Works well for modern graphics hardware. \\n4. Ideal for Hardware Implementation \\no Bresenham’s algorithm is easy to implement in hardware circuits (like GPUs), making \\nit widely used in real-time rendering engines. \\n5. Reduces Computational Overhead \\no No need for complex arithmetic operations (multiplication or division), leading to \\nbetter execution speed. \\n \\nLimitations of Bresenham’s Algorithm \\n1. Limited to Straight Lines Only \\no Unlike DDA, Bresenham’s method is specifically designed for straight lines. \\no To draw curves, additional algorithms like Midpoint Circle Algorithm are needed.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='1. Limited to Straight Lines Only \\no Unlike DDA, Bresenham’s method is specifically designed for straight lines. \\no To draw curves, additional algorithms like Midpoint Circle Algorithm are needed. \\n2. Slightly More Complex Implementation \\no While more efficient, Bresenham’s algorithm is harder to understand and \\nimplement than DDA. \\n3. Does Not Support Anti-Aliasing \\no The algorithm produces jagged edges (aliasing effect) when rendering lines. \\no Additional anti-aliasing techniques like Wu’s Line Algorithm are needed for smooth \\nline rendering. \\n \\nComparison Table: DDA vs. Bresenham’s Algorithm \\nFeature DDA Algorithm Bresenham’s Algorithm \\nArithmetic Type Uses floating-point calculations Uses only integer calculations \\nExecution Speed Slower due to \\ndivision/multiplication Faster due to integer arithmetic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Feature DDA Algorithm Bresenham’s Algorithm \\nAccuracy Accumulates rounding errors More accurate, avoids rounding errors \\nEfficiency Less efficient for high-speed \\napplications Highly efficient for real-time rendering \\nComplexity Easier to implement More complex decision-making process \\nHandling of Slopes Works well for all slopes Requires different cases for steep/shallow \\nslopes \\nHardware \\nSuitability Not ideal for low-power devices Well-suited for hardware and embedded \\nsystems \\nAnti-Aliasing \\nSupport No built-in anti-aliasing Produces jagged edges (needs additional \\nsmoothing) \\n \\nConclusion \\nBoth DDA and Bresenham’s algorithm have their place in computer graphics: \\n• DDA is simple and useful for understanding basic line drawing principles but suffers from \\ncomputational inefficiency. \\n• Bresenham’s algorithm is the preferred choice for fast, accurate, and efficient rendering, \\nmaking it the standard for real-time graphics applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='computational inefficiency. \\n• Bresenham’s algorithm is the preferred choice for fast, accurate, and efficient rendering, \\nmaking it the standard for real-time graphics applications. \\nIf performance is a priority (e.g., in gaming, simulations, and embedded systems), Bresenham’s \\nalgorithm is the best choice. However, for floating-point precision and ease of implementation, \\nDDA can still be useful in applications where speed is not critical. \\n \\n \\n \\n(7 marks) \\n4.Construct homogeneous representation in computer graphics and discuss how it simplifies the \\nprocess of transformations along with its merits.(7) provide detailed answer \\nChatGPT said: \\nHomogeneous Representation in Computer Graphics \\nIn computer graphics, homogeneous coordinates are an extension of Cartesian coordinates that \\nfacilitate geometric transformations such as translation, scaling, rotation, and perspective projection. \\nHomogeneous coordinates introduce an additional dimension (w-component), making it easier to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='facilitate geometric transformations such as translation, scaling, rotation, and perspective projection. \\nHomogeneous coordinates introduce an additional dimension (w-component), making it easier to \\nexpress and perform transformations using matrix multiplication. \\nFor a 2D point (x,y)(x, y)(x,y) in Cartesian coordinates, the homogeneous representation is given as:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='How Homogeneous Coordinates Simplify Transformations \\nHomogeneous coordinates enable all transformations (translation, rotation, scaling, and perspective \\nprojection) to be expressed using matrix multiplication. In standard Cartesian coordinates, \\ntranslation cannot be performed using matrix operations alone, but in homogeneous form, all \\ntransformations become linear operations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Merits of Homogeneous Representation \\n1. Unified Matrix Representation for All Transformations \\no Rotation, scaling, translation, and projection can be performed using 4×4 matrices in \\n3D graphics, enabling efficient pipeline processing in graphics hardware. \\n2. Efficient Computation Using Matrix Multiplication \\no Since all transformations are represented as matrix multiplications, multiple \\ntransformations can be combined into a single matrix before applying them to a \\npoint or object. \\n3. Easier Composition of Transformations \\no Instead of applying transformations sequentially (which is computationally \\nexpensive), they can be pre-multiplied to obtain a composite transformation matrix. \\n4. Supports Perspective Projection \\no Homogeneous coordinates allow perspective transformations using a w-component, \\nwhich cannot be achieved using simple Cartesian coordinates. \\n5. Simplifies Complex Geometric Operations'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Homogeneous coordinates allow perspective transformations using a w-component, \\nwhich cannot be achieved using simple Cartesian coordinates. \\n5. Simplifies Complex Geometric Operations \\no Operations such as shearing, mirroring, and 3D clipping are easier to perform in \\nhomogeneous coordinates. \\n6. Optimized for Computer Graphics Pipelines \\no Modern GPU architectures and graphics APIs (OpenGL, DirectX) use homogeneous \\ncoordinates to efficiently process and render 3D objects. \\n7. Allows Infinite Points Representation \\no When w=0w = 0w=0, it represents a point at infinity, which is useful for handling \\nparallel lines and projections in computer graphics. \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 8, 'page_label': '9', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Homogeneous coordinates play a crucial role in computer graphics by simplifying transformations, \\nunifying geometric operations, and optimizing rendering efficiency. They enable complex \\ntransformations using simple matrix multiplications, making them a fundamental concept in 2D and \\n3D graphics applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(15marks) \\n7.Identify the various OpenGL functions used to create and manipulate points, lines, and filled areas \\nin 2D.(15) provide detailed answer for 15 markks \\nChatGPT said: \\nOpenGL Functions for Creating and Manipulating Points, Lines, and Filled Areas in 2D \\nOpenGL provides a set of functions to create and manipulate points, lines, and filled areas in 2D \\ngraphics. These functions are essential for rendering basic geometric shapes and building complex \\ngraphics applications. \\n \\n1. Functions for Creating and Manipulating Points \\nPoints are the simplest geometric primitives in OpenGL. A point is a single pixel on the screen. \\na) glBegin(GL_POINTS) and glEnd() \\nUsed to define and draw a set of individual points. Each point is specified using glVertex*(). \\ncpp \\nCopyEdit \\nglBegin(GL_POINTS);  \\n    glVertex2f(0.5, 0.5);  // Draw a point at (0.5, 0.5) \\n    glVertex2f(-0.5, -0.5); // Another point at (-0.5, -0.5) \\nglEnd(); \\nb) glPointSize(size) \\nControls the size of the points. \\ncpp'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glVertex2f(0.5, 0.5);  // Draw a point at (0.5, 0.5) \\n    glVertex2f(-0.5, -0.5); // Another point at (-0.5, -0.5) \\nglEnd(); \\nb) glPointSize(size) \\nControls the size of the points. \\ncpp \\nCopyEdit \\nglPointSize(5.0); // Sets point size to 5 pixels \\nc) glEnable(GL_POINT_SMOOTH) \\nEnables anti-aliasing for smooth points. \\ncpp \\nCopyEdit \\nglEnable(GL_POINT_SMOOTH); \\n \\n2. Functions for Creating and Manipulating Lines'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 10, 'page_label': '11', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Lines are fundamental graphics primitives used in drawing outlines, wireframes, and geometric \\nstructures. \\na) glBegin(GL_LINES) and glEnd() \\nDraws individual line segments between pairs of points. \\ncpp \\nCopyEdit \\nglBegin(GL_LINES); \\n    glVertex2f(-0.5, 0.0); // Start point \\n    glVertex2f(0.5, 0.0);  // End point \\nglEnd(); \\nb) glBegin(GL_LINE_STRIP) \\nDraws connected lines without closing the shape. \\ncpp \\nCopyEdit \\nglBegin(GL_LINE_STRIP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\n    glVertex2f(0.5, -0.5); \\nglEnd(); \\nc) glBegin(GL_LINE_LOOP) \\nDraws a closed polygonal shape with connected lines. \\ncpp \\nCopyEdit \\nglBegin(GL_LINE_LOOP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\nd) glLineWidth(width) \\nSets the width of the line in pixels.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 11, 'page_label': '12', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='cpp \\nCopyEdit \\nglLineWidth(3.0); // Sets the line thickness to 3 pixels \\ne) glEnable(GL_LINE_SMOOTH) \\nEnables anti-aliasing for smoother lines. \\ncpp \\nCopyEdit \\nglEnable(GL_LINE_SMOOTH); \\n \\n3. Functions for Creating and Manipulating Filled Areas (Polygons) \\nPolygons are used to represent filled areas such as rectangles, triangles, and complex shapes. \\na) glBegin(GL_TRIANGLES) \\nDraws a set of independent triangles (each group of three vertices forms a triangle). \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLES); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\nglEnd(); \\nb) glBegin(GL_TRIANGLE_STRIP) \\nDraws a connected series of triangles, reducing the number of vertices needed. \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLE_STRIP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\n    glVertex2f(0.5, 0.5); \\nglEnd(); \\nc) glBegin(GL_TRIANGLE_FAN)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Similar to a triangle strip, but all triangles share a common starting vertex. \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLE_FAN); \\n    glVertex2f(0.0, 0.5);  // Common center \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\nglEnd(); \\nd) glBegin(GL_QUADS) \\nDraws independent quadrilaterals (each four vertices form a closed shape). \\ncpp \\nCopyEdit \\nglBegin(GL_QUADS); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\ne) glBegin(GL_POLYGON) \\nDraws a general polygon with more than four vertices. \\ncpp \\nCopyEdit \\nglBegin(GL_POLYGON); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.7, 0.2); \\n    glVertex2f(0.3, 0.6); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\nf) glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Specifies whether the polygons should be drawn as filled, wireframe, or points. \\ncpp \\nCopyEdit \\nglPolygonMode(GL_FRONT_AND_BACK, GL_FILL); // Default: filled \\nglPolygonMode(GL_FRONT_AND_BACK, GL_LINE); // Wireframe mode \\nglPolygonMode(GL_FRONT_AND_BACK, GL_POINT); // Points only \\n \\n4. Additional OpenGL Functions for Manipulation \\na) glColor3f(r, g, b) – Sets Color \\nColors the primitives (values range from 0.0 to 1.0). \\ncpp \\nCopyEdit \\nglColor3f(1.0, 0.0, 0.0); // Red color \\nb) glViewport(x, y, width, height) \\nDefines the 2D rendering area on the screen. \\ncpp \\nCopyEdit \\nglViewport(0, 0, 800, 600); // Sets viewport to 800x600 pixels \\nc) glClear(GL_COLOR_BUFFER_BIT) \\nClears the screen before drawing. \\ncpp \\nCopyEdit \\nglClear(GL_COLOR_BUFFER_BIT); \\nd) glFlush() \\nForces immediate execution of OpenGL commands. \\ncpp \\nCopyEdit \\nglFlush(); \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL provides a rich set of functions for creating and manipulating points, lines, and filled \\npolygons in 2D graphics. These functions allow for efficient rendering and manipulation of shapes \\nusing vertex-based commands, matrix transformations, and rasterization techniques. By combining \\nthese basic elements, complex graphics applications can be developed efficiently. \\n4o \\nYou said: \\nfor the above question provide one whole program as a example \\nChatGPT said: \\nHere is a complete OpenGL program that demonstrates how to draw and manipulate points, lines, \\nand filled polygons in 2D using OpenGL and GLUT. \\nThis program: \\n   Draws a point \\n   Draws different types of lines (single lines, line strip, line loop) \\n   Draws filled polygons (triangles, quads, and a general polygon) \\n   Uses colors and transformations \\n \\nComplete OpenGL Program in C++ \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n \\n// Function to initialize OpenGL settings \\nvoid init() {'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Uses colors and transformations \\n \\nComplete OpenGL Program in C++ \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n \\n// Function to initialize OpenGL settings \\nvoid init() { \\n    glClearColor(1.0, 1.0, 1.0, 1.0); // White background \\n    glMatrixMode(GL_PROJECTION); \\n    gluOrtho2D(-1.0, 1.0, -1.0, 1.0); // Set coordinate system \\n} \\n \\n// Function to draw points, lines, and polygons \\nvoid display() { \\n    glClear(GL_COLOR_BUFFER_BIT); // Clear screen \\n     \\n    // **Draw a Point** \\n    glPointSize(5.0); // Set point size'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glColor3f(1.0, 0.0, 0.0); // Red color \\n    glBegin(GL_POINTS); \\n        glVertex2f(0.0, 0.8); \\n    glEnd(); \\n \\n    // **Draw a Line** \\n    glLineWidth(3.0); // Set line width \\n    glColor3f(0.0, 0.0, 1.0); // Blue color \\n    glBegin(GL_LINES); \\n        glVertex2f(-0.8, 0.5); \\n        glVertex2f(0.8, 0.5); \\n    glEnd(); \\n \\n    // **Draw a Line Strip** \\n    glColor3f(0.0, 0.7, 0.3); // Greenish color \\n    glBegin(GL_LINE_STRIP); \\n        glVertex2f(-0.8, 0.2); \\n        glVertex2f(-0.4, 0.3); \\n        glVertex2f(0.0, 0.2); \\n        glVertex2f(0.4, 0.3); \\n        glVertex2f(0.8, 0.2); \\n    glEnd(); \\n \\n    // **Draw a Triangle** \\n    glColor3f(1.0, 0.5, 0.0); // Orange color \\n    glBegin(GL_TRIANGLES); \\n        glVertex2f(-0.6, -0.2); \\n        glVertex2f(0.0, -0.2); \\n        glVertex2f(-0.3, -0.6); \\n    glEnd();'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 16, 'page_label': '17', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='// **Draw a Quad (Rectangle)** \\n    glColor3f(0.5, 0.0, 0.5); // Purple color \\n    glBegin(GL_QUADS); \\n        glVertex2f(0.2, -0.2); \\n        glVertex2f(0.6, -0.2); \\n        glVertex2f(0.6, -0.6); \\n        glVertex2f(0.2, -0.6); \\n    glEnd(); \\n \\n    // **Draw a Polygon** \\n    glColor3f(0.0, 0.7, 0.7); // Cyan color \\n    glBegin(GL_POLYGON); \\n        glVertex2f(-0.2, -0.8); \\n        glVertex2f(0.0, -0.7); \\n        glVertex2f(0.2, -0.8); \\n        glVertex2f(0.3, -0.9); \\n        glVertex2f(0.0, -1.0); \\n        glVertex2f(-0.3, -0.9); \\n    glEnd(); \\n \\n    glFlush(); // Force execution \\n} \\n \\n// Main function to initialize GLUT and start the program \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"OpenGL 2D Drawing Example\"); \\n \\n    init(); // Initialize OpenGL settings'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glutDisplayFunc(display); // Set display callback function \\n    glutMainLoop(); // Start the main event loop \\n \\n    return 0; \\n} \\n \\nExplanation of the Program \\n1. Initialization (init()) \\no Sets the background color to white. \\no Defines the coordinate system using gluOrtho2D(), setting the range from (-1,1) for \\nboth X and Y axes. \\n2. Display Function (display()) \\no Clears the screen before drawing. \\no Draws a single point at (0.0, 0.8) with glPointSize(5.0). \\no Draws a single line from (-0.8, 0.5) to (0.8, 0.5). \\no Draws a line strip, forming a connected zigzag pattern. \\no Draws a filled triangle, colored orange. \\no Draws a filled rectangle (quad), colored purple. \\no Draws a polygon, resembling a star-like shape. \\n3. GLUT Main Loop (main()) \\no Initializes GLUT. \\no Creates a 500x500 pixel window. \\no Calls init() for OpenGL settings. \\no Starts the event loop with glutMainLoop(), which continuously listens for display \\nevents. \\n \\nExpected Output'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Creates a 500x500 pixel window. \\no Calls init() for OpenGL settings. \\no Starts the event loop with glutMainLoop(), which continuously listens for display \\nevents. \\n \\nExpected Output \\nWhen you run this program, you will see:    A red point at the top \\n   A blue horizontal line in the middle \\n   A green zigzag line strip \\n   An orange triangle on the left \\n   A purple rectangle on the right \\n   A cyan star-like polygon at the bottom'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 18, 'page_label': '19', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='How to Compile and Run \\nFor Windows (MinGW) \\n1. Install MinGW and FreeGLUT. \\n2. Compile using:  \\nlua \\nCopyEdit \\ng++ filename.cpp -o output.exe -lfreeglut -lopengl32 -lglu32 \\n3. Run:  \\nlua \\nCopyEdit \\noutput.exe \\nFor Linux (g++) \\n1. Install OpenGL libraries:  \\narduino \\nCopyEdit \\nsudo apt-get install freeglut3-dev \\n2. Compile:  \\nlua \\nCopyEdit \\ng++ filename.cpp -o output -lGL -lGLU -lglut \\n3. Run:  \\nbash \\nCopyEdit \\n./output \\n \\nConclusion \\nThis program demonstrates all major OpenGL primitives used in 2D graphics, including points, lines, \\nand filled polygons. You can modify colors, shapes, and positions to experiment with different \\ngraphics rendering techniques.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(7 marks) \\n11. Build the Digital Differential Analyzer line drawing algorithm in detail.(7) provide detailed answer \\nChatGPT said: \\nDigital Differential Analyzer (DDA) Line Drawing Algorithm \\nThe Digital Differential Analyzer (DDA) algorithm is a fundamental rasterization algorithm used in \\ncomputer graphics to draw lines between two points. It is based on incremental calculations and is \\nwidely used due to its simplicity and efficiency. \\n \\nConcept of DDA Algorithm \\nThe DDA algorithm works by calculating intermediate points between the start and end points of a \\nline and plotting them pixel by pixel. It uses the equation of a straight line: \\ny=mx+cy = mx + cy=mx+c  \\nwhere m is the slope of the line, given by: \\nm=(y2−y1)(x2−x1)m = \\\\frac{(y_2 - y_1)}{(x_2 - x_1)}m=(x2−x1)(y2−y1)  \\nInstead of using floating-point arithmetic and multiplication, the algorithm increments either xxx or \\nyyy by a fixed step size and calculates the corresponding other coordinate. \\n \\nSteps of the DDA Algorithm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='yyy by a fixed step size and calculates the corresponding other coordinate. \\n \\nSteps of the DDA Algorithm \\n1. Input the start point (x1,y1)(x_1, y_1)(x1,y1) and end point (x2,y2)(x_2, y_2)(x2,y2). \\n2. Compute dx and dy: dx=x2−x1,dy=y2−y1dx = x_2 - x_1, \\\\quad dy = y_2 - y_1dx=x2−x1,dy=y2\\n−y1  \\n3. Find the step count (maximum change in either x or y): steps=max\\u2061(∣dx∣,∣dy∣)\\\\text{steps} = \\n\\\\max(|dx|, |dy|)steps=max(∣dx∣,∣dy∣)  \\n4. Compute the incremental step values: \\nXincrement=dxsteps,Yincrement=dystepsX_{\\\\text{increment}} = \\\\frac{dx}{\\\\text{steps}}, \\\\quad \\nY_{\\\\text{increment}} = \\\\frac{dy}{\\\\text{steps}}Xincrement=stepsdx,Yincrement=stepsdy  \\n5. Initialize starting position at (x1,y1)(x_1, y_1)(x1,y1). \\n6. Iterate through the step count, rounding the values to the nearest integer and plotting the \\npixel. \\n7. Repeat until the end point is reached. \\n \\nDDA Algorithm Implementation in C++ (Using OpenGL)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='pixel. \\n7. Repeat until the end point is reached. \\n \\nDDA Algorithm Implementation in C++ (Using OpenGL) \\nHere’s a complete C++ program that implements the DDA line drawing algorithm using OpenGL: \\ncpp'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='CopyEdit \\n#include <GL/glut.h> \\n#include <cmath> \\n#include <iostream> \\n \\nusing namespace std; \\n \\n// Function to implement the DDA line drawing algorithm \\nvoid drawDDA(float x1, float y1, float x2, float y2) { \\n    float dx = x2 - x1; \\n    float dy = y2 - y1; \\n     \\n    int steps = max(abs(dx), abs(dy));  // Find the number of steps \\n     \\n    float Xinc = dx / steps;  // Increment in x per step \\n    float Yinc = dy / steps;  // Increment in y per step \\n     \\n    float x = x1, y = y1; \\n     \\n    glBegin(GL_POINTS); \\n    for (int i = 0; i <= steps; i++) { \\n        glVertex2f(round(x), round(y));  // Plot the point \\n        x += Xinc;  // Increment x \\n        y += Yinc;  // Increment y \\n    } \\n    glEnd(); \\n} \\n \\n// Display function for OpenGL \\nvoid display() { \\n    glClear(GL_COLOR_BUFFER_BIT);'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 21, 'page_label': '22', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glColor3f(1.0, 0.0, 0.0);  // Red color \\n    drawDDA(-0.5, -0.5, 0.5, 0.5);  // Draw a diagonal line \\n     \\n    glFlush(); \\n} \\n \\n// Initialize OpenGL settings \\nvoid init() { \\n    glClearColor(1.0, 1.0, 1.0, 1.0);  // Set background to white \\n    glMatrixMode(GL_PROJECTION); \\n    gluOrtho2D(-1, 1, -1, 1);  // Set 2D coordinate system \\n} \\n \\n// Main function to set up GLUT and OpenGL \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"DDA Line Drawing Algorithm\"); \\n \\n    init(); \\n    glutDisplayFunc(display); \\n    glutMainLoop(); \\n    return 0; \\n} \\n \\nMerits of DDA Algorithm \\n   Fast and efficient: Uses simple addition operations instead of complex multiplications. \\n   Straightforward implementation: Easy to implement with basic calculations. \\n   Works for all slopes: Can draw lines with any inclination.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 22, 'page_label': '23', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Limitations of DDA Algorithm \\n  Floating-point rounding errors: Since floating-point numbers are used, rounding may introduce \\nsmall inaccuracies. \\n  Slower than Bresenham’s Algorithm: Uses floating-point operations which can be \\ncomputationally expensive. \\n  Not perfectly optimized for raster displays: May produce gaps or jagged lines if not rounded \\nproperly. \\n \\nConclusion \\nThe DDA algorithm is a simple and effective method for drawing lines in computer graphics. It \\nefficiently interpolates pixel positions using incremental calculations, making it widely used in \\ngraphics rendering. However, for performance-critical applications, Bresenham’s Line Algorithm is \\noften preferred due to its integer-based calculations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(8 marks) \\nApply the concept of clipping and the importance of the clipping window in computer graphics. \\nExplain how clipping is performed in OpenGL, including the types of clipping available (point, line, \\npolygon clipping).(8) provide detailed answer \\nChatGPT said: \\nClipping in Computer Graphics \\nIntroduction \\nClipping is a crucial process in computer graphics that determines which parts of objects (such as \\npoints, lines, and polygons) are visible within a designated region, called the clipping window. Any \\nportions that lie outside this window are removed to improve rendering efficiency and maintain \\ncorrect visualization. \\nClipping is essential for: \\n• Optimizing performance by eliminating unnecessary computations. \\n• Ensuring a proper display by keeping only the relevant portions of objects. \\n• Managing object visibility within a viewport or screen. \\n \\nConcept of the Clipping Window'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content=\"• Ensuring a proper display by keeping only the relevant portions of objects. \\n• Managing object visibility within a viewport or screen. \\n \\nConcept of the Clipping Window \\nA clipping window is a rectangular or defined boundary that sets the limits for rendering objects. \\nAny graphical entity outside this boundary is clipped (discarded). This boundary could represent a \\nviewport, screen area, or a defined viewing region in a graphical application. \\nThe clipping window is typically defined in: \\n• 2D space (rectangular) – Used in raster graphics for UI elements, maps, and simple \\nrendering. \\n• 3D space (viewing frustum) – Used in 3D graphics to define the camera's perspective. \\n \\nTypes of Clipping in Computer Graphics \\nClipping is applied to different types of graphical objects, and the most common types include: \\n1. Point Clipping \\n• Determines whether a point lies inside or outside the clipping region. \\n• A point (x,y)(x, y)(x,y) is visible if it satisfies the window boundaries:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='1. Point Clipping \\n• Determines whether a point lies inside or outside the clipping region. \\n• A point (x,y)(x, y)(x,y) is visible if it satisfies the window boundaries: \\nxmin≤x≤xmax,ymin≤y≤ymaxx_{\\\\text{min}} \\\\leq x \\\\leq x_{\\\\text{max}}, \\\\quad y_{\\\\text{min}} \\n\\\\leq y \\\\leq y_{\\\\text{max}}xmin≤x≤xmax,ymin≤y≤ymax  \\n• Used in applications like point-based selections (mouse clicks, scatter plots, etc.). \\n2. Line Clipping (Cohen-Sutherland & Liang-Barsky Algorithms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='• Determines whether a line is fully visible, partially visible, or completely outside the \\nclipping region. \\n• If partially visible, the intersecting points are computed, and only the visible segment is \\ndrawn. \\n• Algorithms used:  \\no Cohen-Sutherland Line Clipping – Uses region codes to classify line segments. \\no Liang-Barsky Algorithm – More efficient as it reduces computations using parametric \\nequations. \\n3. Polygon Clipping (Sutherland-Hodgman Algorithm) \\n• Used to clip polygons against the viewport. \\n• The Sutherland-Hodgman algorithm clips a polygon edge-by-edge against the clipping \\nwindow. \\n• Produces new clipped vertices to form a visible polygon. \\n• Used in maps, gaming, and rendering optimizations. \\n4. Text Clipping \\n• Ensures that only the visible portion of text is rendered within a defined region. \\n• Used in UI development (text fields, labels). \\n5. Curve Clipping \\n• Similar to line clipping but applied to curved shapes like circles, ellipses, and Bezier curves.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='• Used in UI development (text fields, labels). \\n5. Curve Clipping \\n• Similar to line clipping but applied to curved shapes like circles, ellipses, and Bezier curves. \\n• Uses bounding boxes or incremental approximations. \\n \\nClipping in OpenGL \\nOpenGL provides automatic clipping based on the defined viewing volume. The clipping process \\noccurs in Normalized Device Coordinates (NDC), meaning anything outside the canonical clipping \\nvolume (from -1 to 1 in X, Y , and Z) is discarded. \\nHow Clipping is Performed in OpenGL \\n1. Defining the Clipping Region \\nOpenGL uses glOrtho() or gluOrtho2D() for 2D clipping and glFrustum() for 3D perspective \\nclipping. \\ncpp \\nCopyEdit \\nglMatrixMode(GL_PROJECTION); \\nglLoadIdentity(); \\ngluOrtho2D(-100, 100, -100, 100);  // Defines a clipping window from (-100, -100) to (100, 100)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='2. Applying Viewport Transformation \\nThe viewport maps the clipping region to a screen coordinate system: \\ncpp \\nCopyEdit \\nglViewport(0, 0, 800, 600); \\n3. Automatic Clipping by OpenGL \\no Any primitives outside the defined view volume are automatically clipped. \\no OpenGL uses a clipping plane to discard objects outside the NDC space. \\n4. Custom Clipping Planes in OpenGL \\nOpenGL allows defining custom clipping planes using glClipPlane(): \\ncpp \\nCopyEdit \\ndouble planeEq[] = {1.0, 0.0, 0.0, -50.0};  // Clipping plane along x = 50 \\nglEnable(GL_CLIP_PLANE0); \\nglClipPlane(GL_CLIP_PLANE0, planeEq); \\n \\nExample: Line Clipping using Cohen-Sutherland Algorithm in OpenGL \\nBelow is a C++ program that implements line clipping using the Cohen-Sutherland algorithm in \\nOpenGL. \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n#include <iostream> \\n \\nusing namespace std; \\n \\n// Clipping window boundaries \\n#define X_MIN -50 \\n#define X_MAX 50 \\n#define Y_MIN -50 \\n#define Y_MAX 50'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 26, 'page_label': '27', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='// Region codes \\nconst int INSIDE = 0, LEFT = 1, RIGHT = 2, BOTTOM = 4, TOP = 8; \\n \\n// Function to compute region code for a point (x, y) \\nint computeCode(float x, float y) { \\n    int code = INSIDE; \\n    if (x < X_MIN) code |= LEFT; \\n    else if (x > X_MAX) code |= RIGHT; \\n    if (y < Y_MIN) code |= BOTTOM; \\n    else if (y > Y_MAX) code |= TOP; \\n    return code; \\n} \\n \\n// Cohen-Sutherland Line Clipping Algorithm \\nvoid cohenSutherlandClip(float x1, float y1, float x2, float y2) { \\n    int code1 = computeCode(x1, y1); \\n    int code2 = computeCode(x2, y2); \\n    bool accept = false; \\n \\n    while (true) { \\n        if ((code1 == 0) && (code2 == 0)) { \\n            accept = true; \\n            break; \\n        } else if (code1 & code2) { \\n            break; \\n        } else { \\n            int code_out; \\n            float x, y; \\n \\n            code_out = (code1 != 0) ? code1 : code2;'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='if (code_out & TOP) { \\n                x = x1 + (x2 - x1) * (Y_MAX - y1) / (y2 - y1); \\n                y = Y_MAX; \\n            } else if (code_out & BOTTOM) { \\n                x = x1 + (x2 - x1) * (Y_MIN - y1) / (y2 - y1); \\n                y = Y_MIN; \\n            } else if (code_out & RIGHT) { \\n                y = y1 + (y2 - y1) * (X_MAX - x1) / (x2 - x1); \\n                x = X_MAX; \\n            } else if (code_out & LEFT) { \\n                y = y1 + (y2 - y1) * (X_MIN - x1) / (x2 - x1); \\n                x = X_MIN; \\n            } \\n \\n            if (code_out == code1) { \\n                x1 = x; y1 = y; code1 = computeCode(x1, y1); \\n            } else { \\n                x2 = x; y2 = y; code2 = computeCode(x2, y2); \\n            } \\n        } \\n    } \\n \\n    if (accept) { \\n        glBegin(GL_LINES); \\n        glVertex2f(x1, y1); \\n        glVertex2f(x2, y2); \\n        glEnd(); \\n    } \\n} \\n \\n// Display function'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='void display() { \\n    glClear(GL_COLOR_BUFFER_BIT); \\n    glColor3f(1.0, 0.0, 0.0); \\n    cohenSutherlandClip(-70, -30, 60, 40); \\n    glFlush(); \\n} \\n \\n// Initialize OpenGL \\nvoid init() { \\n    gluOrtho2D(-100, 100, -100, 100); \\n} \\n \\n// Main function \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"Line Clipping - Cohen Sutherland\"); \\n \\n    init(); \\n    glutDisplayFunc(display); \\n    glutMainLoop(); \\n    return 0; \\n} \\n \\nConclusion \\nClipping is an essential technique in computer graphics that improves performance by removing \\nunnecessary parts of objects. OpenGL provides built-in clipping for optimized rendering, while \\nadvanced algorithms like Cohen-Sutherland and Sutherland-Hodgman offer efficient clipping \\nsolutions for lines and polygons.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. How MapReduce Computation is Executed? \\nMapReduce executes in two main stages: Map and Reduce. \\n1. Map Stage: The input data is split into chunks and processed by Mapper tasks in parallel. \\nEach Mapper processes a portion of the data and outputs key-value pairs. \\n2. Shuffle & Sort: The output of Mappers is sorted and grouped by key. The data is then \\nshuffled to appropriate Reducers. \\n3. Reduce Stage: The Reducer aggregates and processes data by key, generating the final \\noutput. \\nExample: Word count in MapReduce: \\n• Mapper: (\"Hadoop is great\") → [(Hadoop, 1), (is, 1), (great, 1)] \\n• Shuffle & Sort: Groups values by keys → [(Hadoop, [1,1,1]), (great, [1]), (is, [1,1])] \\n• Reducer: Aggregates counts → [(Hadoop, 3), (great, 1), (is, 2)] \\nMapReduce computation is executed through two main phases: a Map phase where data is \\nprocessed in parallel, and a Reduce phase where the results from the Map phase are aggregated and \\nprocessed to produce a final output.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='processed in parallel, and a Reduce phase where the results from the Map phase are aggregated and \\nprocessed to produce a final output.  \\nHere\\'s a more detailed breakdown: \\n1. Map Phase: \\n• Input: \\nThe MapReduce framework takes a large dataset as input, which is typically stored in a distributed \\nfile system like HDFS (Hadoop Distributed File System).  \\n• Splitting: \\nThe input data is divided into smaller chunks or \"splits\" that can be processed independently by \\ndifferent nodes in a cluster.  \\n• Mapping: \\nEach split is assigned to a mapper process, which applies a user-defined \"map\" function to each \\nrecord within the split.  \\n• Key-Value Pairs: \\nThe map function typically transforms the input data into key-value pairs.  \\n• Intermediate Output: \\nThe mappers produce a set of intermediate key-value pairs as output, which are then passed to the \\nshuffle and sort phase.  \\n2. Shuffle and Sort Phase: \\n• Shuffling:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Intermediate Output: \\nThe mappers produce a set of intermediate key-value pairs as output, which are then passed to the \\nshuffle and sort phase.  \\n2. Shuffle and Sort Phase: \\n• Shuffling: \\nThe intermediate key-value pairs from all mappers are shuffled and partitioned based on the keys.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Sorting: \\nThe framework sorts the shuffled key-value pairs, ensuring that all pairs with the same key are \\ngrouped together.  \\n3. Reduce Phase: \\n• Input: The sorted key-value pairs are passed to the reducer processes.  \\n• Reducing: Each reducer receives a group of key-value pairs with the same key.  \\n• Aggregation: The user-defined \"reduce\" function processes these grouped values and \\nproduces a final set of key-value pairs as output.  \\n• Output: The final output from the reducers is stored in the distributed file system.  \\nIn summary, MapReduce is a distributed computing model that breaks down large data processing \\ntasks into smaller, parallelizable tasks (the map phase) and then combines the results (the reduce \\nphase) to produce a final output.  \\n2. Show the key advantages in Hadoop. \\nHadoop offers several key advantages, including scalability for handling massive \\ndatasets, cost-effectiveness through commodity hardware, fault tolerance with'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"2. Show the key advantages in Hadoop. \\nHadoop offers several key advantages, including scalability for handling massive \\ndatasets, cost-effectiveness through commodity hardware, fault tolerance with \\ndata replication, and flexibility in handling various data types.  \\nHere's a more detailed breakdown of Hadoop's key advantages: \\n• Scalability: \\nHadoop's architecture allows for horizontal scaling, meaning you can add more \\nnodes to the cluster to handle increasing data volumes and processing needs \\nwithout significant infrastructure changes.  \\n• Cost-Effectiveness: \\nHadoop is designed to run on inexpensive, standard hardware, making it a cost-\\neffective solution compared to traditional, expensive data storage and processing \\nsystems.  \\n• Fault Tolerance: \\nHadoop replicates data across multiple nodes, ensuring that data is not lost if one \\nor more nodes fail.  \\n• Flexibility: \\nHadoop can handle various data types, including structured, semi-structured, and\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"or more nodes fail.  \\n• Flexibility: \\nHadoop can handle various data types, including structured, semi-structured, and \\nunstructured data, making it a versatile platform for diverse data analytics needs.  \\n• Data Locality: \\nHadoop's architecture allows for processing data where it resides, minimizing data \\nmovement and improving performance.  \\n• Open Source: \\nHadoop is an open-source project, meaning its source code is freely available and \\ncan be modified and distributed.  \\n• YARN (Yet Another Resource Negotiator):\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='YARN is a resource management system that enables efficient resource allocation \\nand management within the Hadoop ecosystem, allowing for better utilization of \\ncluster resources.  \\n• Data Governance and Management: \\nHadoop provides tools and features for data governance, including metadata \\nmanagement, data lineage, and security controls.  \\n• Security: \\nHadoop offers robust security features, including authentication, authorization, \\nand encryption, to protect sensitive data.  \\n• Data Lake: \\nHadoop can be used as a foundation for building data lakes, which are centralized \\nrepositories for storing diverse data formats and sources.  \\n3. What is the purpose of the NodeManager in \\nYARN? \\nThe NodeManager in YARN is a per-machine agent responsible for launching and \\nmanaging containers on a worker node, monitoring their resource usage, and reporting \\nthis information to the ResourceManager \\nThe NodeManager is responsible for: \\n• Monitoring resource usage (CPU, memory) on a node.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='this information to the ResourceManager \\nThe NodeManager is responsible for: \\n• Monitoring resource usage (CPU, memory) on a node. \\n• Managing containers for executing tasks. \\n• Reporting node health to the ResourceManager. \\nExample: In a Hadoop cluster, each server runs a NodeManager to manage local \\ncomputation tasks and communicate with the ResourceManager. \\n4. Explain the main components of YARN. \\n4. Main Components of YARN \\n1. ResourceManager (RM) – Allocates resources across the cluster. \\n2. NodeManager (NM) – Monitors nodes and manages task execution. \\n3. ApplicationMaster (AM) – Manages the execution of an application. \\n4. Container – A runtime environment that executes tasks. \\nExample: A Spark job requests resources from the RM, which assigns it to a \\nNodeManager to execute the tasks within containers. \\n5. How does MapReduce achieve parallel \\nprocessing?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 3, 'page_label': '4', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='MapReduce achieves parallel processing by dividing large datasets into smaller chunks, \\ndistributing these chunks across multiple machines for processing (the \"Map\" phase), \\nand then aggregating the results from these machines (the \"Reduce\" phase).  \\nHere\\'s a more detailed explanation: \\n• Data Splitting: \\nThe input data is split into smaller, manageable chunks, which can be processed \\nindependently by different machines.  \\n• Map Phase: \\nEach chunk of data is processed independently by a \"mapper\" function, which \\ntransforms the data into key-value pairs.  \\n• Reduce Phase: \\nThe \"reducer\" function then takes the key-value pairs generated by the mappers and \\naggregates them to produce the final output.  \\n• Parallelism: \\nThe Map and Reduce phases can be executed in parallel across multiple machines, \\nenabling efficient processing of large datasets.  \\n• Fault Tolerance: \\nMapReduce is designed to be fault-tolerant, meaning that if a machine fails, the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 3, 'page_label': '4', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='enabling efficient processing of large datasets.  \\n• Fault Tolerance: \\nMapReduce is designed to be fault-tolerant, meaning that if a machine fails, the \\nMapReduce framework can automatically reassign the tasks to other machines, \\nensuring that the job continues to run.  \\n• Hadoop Ecosystem: \\nMapReduce is a fundamental part of the Apache Hadoop ecosystem, a distributed \\ncomputing framework designed for processing large datasets.  \\n• Data Access and Storage: \\nMapReduce uses Hadoop Distributed File System (HDFS) for both input and output.  \\n6. What is the role of the \\'Mapper\\' in MapReduce? \\n• Takes input data and converts it into key-value pairs. \\n• Runs in parallel on multiple nodes. \\n• Sends intermediate output to the Reduce phase. \\nExample: In a sales dataset, the Mapper outputs (city, sales): \\n• Input: (\"New York, 200\"), (\"Los Angeles, 150\") \\n• Output: [(New York, 200), (Los Angeles, 150)]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"7.Justify the need for replication in HDFS. \\nHDFS data replication is the process of creating multiple copies (replicas) of data blocks \\nacross different DataNodes in the HDFS cluster. This redundancy ensures that if one or more \\nDataNodes fail, the data can still be accessed from the remaining replicas, providing high \\navailability and fault tolerance. \\n8. Purpose of Secondary NameNode in HDFS \\n• Periodically merges edit logs with the namespace image to prevent the NameNode \\nfrom being overloaded. \\n• Acts as a checkpointing mechanism but not a failover node. \\nExample: If the NameNode fails, the Secondary NameNode’s last checkpointed metadata \\ncan be used to restore the system. \\nIn HDFS, the secondary namenode's primary purpose is to act as a helper node for the \\nprimary namenode, performing periodic checkpoints of the namespace and merging edit \\nlogs with the fsimage to prevent the edit logs from growing too large, which can lead to \\nslow NameNode restarts.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"logs with the fsimage to prevent the edit logs from growing too large, which can lead to \\nslow NameNode restarts.  \\n9,12. Explain the steps in map reduce algorithm. \\nThe MapReduce algorithm consists of two main phases: Map and Reduce, with an \\nintermediate shuffle and sort step. The map phase processes data into key-value pairs, \\nwhich are then shuffled and sorted before the reduce phase aggregates the data.  \\nHere's a more detailed breakdown: \\n1. Input Splitting:  \\n• The input data is split into smaller chunks, which are then assigned to different \\nworker nodes in the cluster. \\n• These chunks are processed independently by the mapper functions. \\n2. Map Phase: \\n• Each mapper function receives a chunk of data as input.  \\n• The mapper processes the data and produces a set of intermediate key-value pairs.  \\n• The output of the map phase is a list of key-value pairs.  \\n3. Shuffle and Sort:  \\n• The intermediate key-value pairs are grouped and shuffled based on their keys.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• The output of the map phase is a list of key-value pairs.  \\n3. Shuffle and Sort:  \\n• The intermediate key-value pairs are grouped and shuffled based on their keys. \\n• The keys are sorted, ensuring that all occurrences of the same key are grouped \\ntogether. \\n4. Reduce Phase: \\n• The reducer functions receive the shuffled and sorted key-value pairs as input.  \\n• The reducer aggregates the values associated with the same key, producing the final \\noutput.  \\nSteps in the MapReduce Algorithm \\n1. Input Splitting – Data is divided into chunks. \\n2. Mapping – Mappers process chunks and output key-value pairs. \\n3. Shuffling & Sorting – Intermediate key-value pairs are grouped. \\n4. Reducing – Reducers aggregate data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 5, 'page_label': '6', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Final Output – Processed data is written to HDFS. \\nExample: Counting occurrences of words in a text file follows these steps. \\n11. Differentiate between HDFS and a traditional file \\nsystem. \\nFeature HDFS Traditional File System \\nStorage Type Distributed Local disk storage \\nFault Tolerance High (Replication) Low \\nScalability Linear scaling Limited by disk size \\nData Processing Optimized for batch processing General purpose \\nExample: HDFS stores and processes logs from a web service, while a traditional file \\nsystem is used for local document storage. \\n13. How partitions are shuffled in map reduce. \\n• The Partitioner determines which Reducer a key-value pair is sent to. \\n• The Shuffle Phase transfers data from Mappers to appropriate Reducers. \\n• The Sorting Phase organizes data before being processed by the Reducers. \\nExample: In a sales dataset, a partitioner ensures all sales records for a city go to the same \\nReducer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='14.Illustrate the basic structure of the MapReduce \\nprogramming model.\\n \\n15 Marks: \\n1.Describe with relevant examples the 5Vs of Big Data. \\n5Vs of Big Data with Examples (Expanded 15 Marks Answer) \\nBig Data is defined by 5 key characteristics known as the 5Vs: Volume, Velocity, Variety, \\nVeracity, and Value. These attributes help in understanding how data is generated, \\nprocessed, stored, and analyzed across various industries. \\n \\n1. Volume (Size of Data) – 3 Marks \\nDefinition: \\nVolume refers to the sheer amount of data generated every second from different sources \\nlike social media, IoT devices, and financial transactions. As organizations collect and store \\nmassive datasets, traditional storage and processing methods become inadequate. \\nExample: \\n• Social Media: Facebook processes over 4 petabytes of data daily, including posts, \\nimages, and videos. \\n• E-Commerce: Amazon and Flipkart handle billions of customer transactions every \\nyear.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Social Media: Facebook processes over 4 petabytes of data daily, including posts, \\nimages, and videos. \\n• E-Commerce: Amazon and Flipkart handle billions of customer transactions every \\nyear. \\n• Healthcare: Medical imaging data, such as MRI scans, generates terabytes of data \\nper hospital per year. \\nImpact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Requires scalable storage solutions like Hadoop Distributed File System (HDFS) and \\ncloud-based storage (AWS, Google Cloud, Azure). \\n• Traditional relational databases (RDBMS) struggle with such large data volumes, \\nleading to the adoption of NoSQL databases like Apache Cassandra and MongoDB. \\n \\n2. Velocity (Speed of Data Generation & Processing) – 3 Marks \\nDefinition: \\nVelocity represents the speed at which data is generated, collected, and processed. Some \\ndata streams require real-time processing to provide instant insights and responses. \\nExample: \\n• Stock Markets: Millions of trades per second require real-time analytics to detect \\nfluctuations. \\n• Autonomous Vehicles: Self-driving cars like Tesla process sensor data in \\nmilliseconds for quick decision-making. \\n• Google Search: Handles over 100,000 searches per second, requiring fast indexing \\nand retrieval. \\nImpact: \\n• Technologies like Apache Kafka, Apache Flink, and Spark Streaming are used for \\nreal-time data processing.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='and retrieval. \\nImpact: \\n• Technologies like Apache Kafka, Apache Flink, and Spark Streaming are used for \\nreal-time data processing. \\n• Businesses use real-time analytics for fraud detection, recommendation systems, \\nand operational efficiency. \\n \\n3. Variety (Different Data Types) – 3 Marks \\nDefinition: \\nVariety refers to the different formats and sources of data—structured, semi-structured, \\nand unstructured. Unlike traditional databases that handle only structured data, Big Data \\nsystems must process diverse formats. \\nExample: \\n• Structured Data: Relational databases storing customer details, order history \\n(MySQL, PostgreSQL). \\n• Semi-Structured Data: JSON or XML logs generated by web applications, sensors, \\nand IoT devices. \\n• Unstructured Data:  \\no YouTube processes over 500 hours of video uploads every minute. \\no Social media generates billions of posts, images, and comments daily. \\no Audio recordings from customer support call centers need sentiment \\nanalysis. \\nImpact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='o Social media generates billions of posts, images, and comments daily. \\no Audio recordings from customer support call centers need sentiment \\nanalysis. \\nImpact: \\n• NoSQL databases like MongoDB (document-based) and Apache Cassandra (column-\\nbased) allow flexible data models. \\n• AI models process unstructured data (text, images, audio) for fraud detection, \\nsentiment analysis, and medical diagnosis. \\n \\n4. Veracity (Data Accuracy & Trustworthiness) – 3 Marks \\nDefinition: \\nVeracity concerns the quality, reliability, and accuracy of data. Big Data is often messy, \\ncontaining inconsistencies, errors, or biases, making data cleaning essential.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Example: \\n• Fake News Detection: Social media platforms like Facebook use AI to filter out false \\ninformation. \\n• Financial Transactions: Banks use fraud detection models to analyze suspicious \\ntransactions and prevent fraud. \\n• Healthcare Data: Medical datasets must be cleaned and verified before being used \\nin AI-driven diagnosis models. \\nImpact: \\n• ETL (Extract, Transform, Load) pipelines clean and structure raw data before \\nanalysis. \\n• Machine learning models rely on high-quality, unbiased datasets to avoid inaccurate \\npredictions. \\n• Data governance policies ensure compliance with regulations like GDPR and HIPAA. \\n \\n5. Value (Business & Analytical Insights) – 3 Marks \\nDefinition: \\nValue is the most critical aspect of Big Data—it refers to extracting meaningful insights that \\ndrive business decisions. Without proper analysis, raw data is just a storage burden. \\nExample: \\n• Netflix’s Recommendation System: Uses machine learning on user data to suggest \\npersonalized content.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Example: \\n• Netflix’s Recommendation System: Uses machine learning on user data to suggest \\npersonalized content. \\n• Retail Industry: Walmart predicts demand based on customer purchases and adjusts \\nstock levels. \\n• Healthcare AI: Predictive analytics helps doctors diagnose diseases earlier based on \\nlarge datasets. \\nImpact: \\n• Data-driven decision-making leads to higher efficiency and revenue. \\n• AI-powered insights enable better customer experiences, fraud detection, and \\noperational automation. \\n• Companies use big data analytics tools like Hadoop, Spark, and Tableau to derive \\nbusiness intelligence. \\n \\nConclusion \\nThe 5Vs of Big Data—Volume, Velocity, Variety, Veracity, and Value—are essential for \\nhandling large-scale data efficiently. Businesses leverage advanced cloud computing, AI, \\nand NoSQL databases to process, analyze, and gain insights from Big Data. \\n2,3. Describe the importance of tools in Big Data with \\nemphasis on the trends and technology in big data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='and NoSQL databases to process, analyze, and gain insights from Big Data. \\n2,3. Describe the importance of tools in Big Data with \\nemphasis on the trends and technology in big data. \\nImportance of Big Data Tools with Trends & Technology (Expanded 15 Marks Answer) \\nThe explosion of Big Data has transformed industries, requiring advanced tools to efficiently \\nstore, process, and analyze vast datasets. Traditional databases fail to handle the Volume, \\nVariety, and Velocity of modern data, making Big Data tools essential for decision-making, \\nautomation, and real-time analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Importance of Big Data Tools – 5 Marks \\nBig Data tools offer: \\n   Efficient Data Management – Handling petabytes of structured, semi-structured, and \\nunstructured data. \\n   Scalability & Flexibility – Supporting horizontal scaling for cloud-based and on-premise \\nsystems. \\n   Real-time Processing – Enabling businesses to make instant decisions based on live data \\nstreams. \\n   Enhanced Security & Compliance – Implementing encryption and access control (GDPR, \\nHIPAA). \\n   AI & Machine Learning Integration – Powering intelligent automation and predictive \\nanalytics. \\n     Example: Facebook uses Apache Hadoop for storing and analyzing user interactions \\nacross its platform, improving targeted advertising. \\n \\n2. Key Big Data Tools & Technologies – 5 Marks \\nA. Data Storage & Management \\n  Hadoop Distributed File System (HDFS) – Scalable, fault-tolerant storage. \\n  Apache Cassandra – A NoSQL database optimized for high-speed transactions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A. Data Storage & Management \\n  Hadoop Distributed File System (HDFS) – Scalable, fault-tolerant storage. \\n  Apache Cassandra – A NoSQL database optimized for high-speed transactions. \\n  Google BigQuery – Serverless cloud warehouse for massive data analysis. \\n     Example: Spotify uses BigQuery to analyze user streaming habits and personalize \\nplaylists. \\nB. Data Processing & Analytics \\n  Apache Spark – Fast in-memory processing for large datasets. \\n  Apache Flink – Real-time stream processing for instant decision-making. \\n  TensorFlow & PyTorch – AI-driven analytics for deep learning and automation. \\n     Example: Uber uses Apache Flink to process real-time ride requests, optimizing driver \\navailability. \\nC. Data Visualization & Business Intelligence (BI) \\n  Tableau – Interactive dashboards for business reporting. \\n  Power BI – Microsoft’s AI-powered analytics tool. \\n  Google Data Studio – Cloud-based visualization platform.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Tableau – Interactive dashboards for business reporting. \\n  Power BI – Microsoft’s AI-powered analytics tool. \\n  Google Data Studio – Cloud-based visualization platform. \\n     Example: Coca-Cola uses Tableau to track supply chain efficiency and sales trends.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='3. Emerging Trends in Big Data – 5 Marks \\n        A. AI & Machine Learning Integration – Automating decision-making with predictive \\nanalytics. \\n        B. Edge Computing & IoT – Processing data closer to devices for real-time insights. \\n        C. Serverless & Cloud Computing – AWS, Azure, and Google Cloud enabling scalable \\nanalytics. \\n        D. Blockchain for Data Integrity – Securing transactions in finance and healthcare. \\n        E. Quantum Computing in Big Data – Unlocking new possibilities in large-scale data \\nprocessing. \\n     Example: Tesla leverages Edge AI & IoT to analyze autonomous driving data instantly, \\nimproving vehicle safety. \\n4,5. Illustrate the main features and evolution of big \\ndata in detail. provide answer for 15 marks \\nChatGPT said: \\nMain Features and Evolution of Big Data (15 Marks Answer) \\nBig Data refers to extremely large and complex datasets that require advanced techniques'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='ChatGPT said: \\nMain Features and Evolution of Big Data (15 Marks Answer) \\nBig Data refers to extremely large and complex datasets that require advanced techniques \\nfor processing, analyzing, and extracting insights. It has evolved significantly due to \\ntechnological advancements, increased digitalization, and the growing need for real-time \\ndecision-making. \\n \\n1. Main Features of Big Data – 5 Marks \\nBig Data is characterized by the 5Vs, which define its scale, complexity, and value: \\n1. Volume \\n• Definition: Refers to the enormous size of data generated daily from sources like \\nsocial media, IoT, and business transactions. \\n• Example: Facebook processes over 4 petabytes of data daily. \\n2. Velocity \\n• Definition: The speed at which data is generated and processed in real time. \\n• Example: Stock market trading systems process millions of transactions per second. \\n3. Variety \\n• Definition: Data exists in multiple formats (structured, semi-structured, \\nunstructured).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example: Stock market trading systems process millions of transactions per second. \\n3. Variety \\n• Definition: Data exists in multiple formats (structured, semi-structured, \\nunstructured). \\n• Example: Emails, social media posts, IoT sensor data, and videos.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Veracity \\n• Definition: The reliability and accuracy of data for meaningful decision-making. \\n• Example: Fake news detection in social media analytics. \\n5. Value \\n• Definition: Extracting meaningful insights and business advantages from data. \\n• Example: Netflix analyzes user preferences to recommend personalized content. \\n \\n2. Evolution of Big Data – 5 Marks \\nBig Data has evolved through different stages based on technological advancements: \\n1. Early Data Management (1960s - 1980s) \\n• Traditional relational databases (RDBMS) like Oracle and IBM DB2 emerged. \\n• Limited data storage and computing power. \\n2. Rise of the Internet (1990s - 2000s) \\n• Explosion of digital data with the growth of e-commerce, social media, and cloud \\ncomputing. \\n• Google developed MapReduce (2004) & Hadoop (2006) to process large datasets \\nefficiently. \\n3. Big Data Boom (2010s - Present) \\n• Technologies like Apache Spark, NoSQL, and real-time streaming gained traction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"efficiently. \\n3. Big Data Boom (2010s - Present) \\n• Technologies like Apache Spark, NoSQL, and real-time streaming gained traction. \\n• AI-driven analytics transformed industries like healthcare, finance, and marketing. \\n4. Future Trends (2025 & Beyond) \\n• Quantum computing to enhance Big Data processing. \\n• Edge computing for real-time analytics in IoT. \\n• Blockchain for secure data transactions. \\n     Example: Tesla's self-driving cars leverage Big Data to process real-time road \\nconditions. \\n \\n3. Impact of Big Data on Industries – 5 Marks \\n  Healthcare – AI-driven diagnostics (IBM Watson). \\n  Retail – Personalized recommendations (Amazon, Flipkart).\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Banking & Finance – Fraud detection (PayPal, Mastercard). \\n  Smart Cities – Traffic and energy management (Google Maps, Smart Grids). \\n \\nConclusion \\nBig Data has evolved from simple data management to AI-driven analytics, shaping \\nmodern industries. Its key features (5Vs) and technological advancements continue to \\ndrive innovation, automation, and smarter decision-making. Businesses leveraging Big \\nData will gain a competitive edge in the digital era. \\n6. Imagine you are a data scientist in 2025. How would \\nyou leverage the evolution of big data to solve a \\nspecific business problem in your industry?  \\nLeveraging Big Data Evolution to Solve a Business Problem (2025) \\nIntroduction \\nIn 2025, the evolution of big data analytics has transformed business decision-making. As \\na data scientist, I would leverage big data to solve a key problem: customer churn \\nprediction in e-commerce. Retaining customers is more profitable than acquiring new'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='a data scientist, I would leverage big data to solve a key problem: customer churn \\nprediction in e-commerce. Retaining customers is more profitable than acquiring new \\nones, and advanced data analytics can help predict and prevent churn. \\n \\n1. Evolution of Big Data & Its Role in Business \\nBig data has evolved in several ways: \\n• Scalability: Cloud-based solutions (AWS, Google BigQuery) enable real-time \\nprocessing. \\n• Speed: Technologies like Apache Spark process data faster than traditional systems. \\n• AI Integration: Machine learning & deep learning provide predictive insights. \\n• Diverse Data Sources: Data comes from IoT devices, social media, and web \\nanalytics. \\nThese advancements allow businesses to understand customer behavior, optimize \\nmarketing, and improve decision-making. \\n \\n2. How Big Data Analytics Solves Customer Churn \\na) Data Collection & Integration \\n• Structured Data: Purchase history, transaction records, and product reviews.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Unstructured Data: Customer sentiment from social media & chat support. \\n• Example: Netflix analyzes user watch history & engagement to predict \\ncancellations. \\nb) Predictive Analytics & Machine Learning \\n• Models like Random Forest, XGBoost, and Deep Learning identify churn patterns. \\n• Real-time processing (Apache Kafka, Spark) ensures quick insights. \\n• Example: Amazon uses ML to detect customers likely to switch to competitors. \\nc) AI-Driven Personalization \\n• Chatbots & recommendation engines offer real-time solutions. \\n• Sentiment analysis detects negative reviews & triggers retention strategies. \\n• Example: Spotify sends personalized playlists to keep users engaged. \\nd) Proactive Customer Retention Strategies \\n• Targeted discounts & loyalty programs for at-risk customers. \\n• Predictive alerts for sales teams to engage potential churners. \\n• Example: E-commerce platforms send discount codes to inactive users. \\n \\n3. Business Benefits & Impact'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Predictive alerts for sales teams to engage potential churners. \\n• Example: E-commerce platforms send discount codes to inactive users. \\n \\n3. Business Benefits & Impact \\n   Increased customer retention, reducing marketing costs. \\n   Higher revenue through personalized engagement strategies. \\n   Improved customer experience, enhancing brand loyalty. \\n \\nConclusion \\nBy leveraging big data analytics, AI, and real-time processing, businesses can predict \\ncustomer churn, engage users effectively, and drive growth. The evolution of big data \\nenables faster, smarter, and more personalized decision-making, ensuring a competitive \\nedge in 2025.         \\n7. Summarize the various industrial use cases of Big \\nData with examples. \\nIndustrial Use Cases of Big Data with Examples \\nIntroduction'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Big Data refers to the vast volumes of structured and unstructured data generated at high \\nvelocity from multiple sources. Organizations across industries harness big data \\ntechnologies such as Hadoop, Apache Spark, AI-driven analytics, and cloud computing to \\nextract valuable insights, improve efficiency, and make data-driven decisions. \\n \\n1. Healthcare & Pharmaceuticals \\nUse Case: Predictive Analytics and Personalized Medicine \\n• Example: IBM Watson Health leverages big data to analyze medical records and \\nassist doctors in diagnosing diseases like cancer at early stages. \\n• Impact:  \\no Enhanced patient care through early disease detection. \\no Personalized treatment plans based on patient history. \\no Faster drug discovery through AI-powered simulations. \\n \\n2. E-Commerce & Retail \\nUse Case: Customer Behavior Analysis & Demand Forecasting \\n• Example: Amazon and Flipkart use big data to analyze customer browsing and \\npurchase history, offering personalized product recommendations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example: Amazon and Flipkart use big data to analyze customer browsing and \\npurchase history, offering personalized product recommendations. \\n• Impact:  \\no Increased sales through targeted advertisements. \\no Optimized inventory management to reduce wastage. \\no Better understanding of customer preferences and trends. \\n \\n3. Finance & Banking \\nUse Case: Fraud Detection & Risk Management \\n• Example: Banks like JPMorgan Chase use machine learning algorithms to monitor \\ntransactions and detect anomalies indicating fraud. \\n• Impact:  \\no Reduction in fraudulent activities and cyber threats. \\no Enhanced customer trust through secure banking services. \\no Improved credit scoring and loan approval processes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Manufacturing & Supply Chain \\nUse Case: Predictive Maintenance & Process Optimization \\n• Example: General Electric (GE) uses IoT sensors and AI-powered analytics to predict \\nequipment failures and schedule maintenance. \\n• Impact:  \\no Reduced operational downtime and maintenance costs. \\no Increased production efficiency and output. \\no Smart inventory management and supply chain optimization. \\n \\n5. Telecommunications \\nUse Case: Network Optimization & Customer Retention \\n• Example: Vodafone and AT&T use big data to analyze call records, predict network \\ncongestion, and provide seamless connectivity. \\n• Impact:  \\no Improved network efficiency and reduced service disruptions. \\no Enhanced customer satisfaction through proactive troubleshooting. \\no Minimized customer churn by offering tailored data plans. \\n \\n6. Transportation & Smart Cities \\nUse Case: Traffic Management & Route Optimization \\n• Example: Google Maps utilizes real-time GPS data and big data analytics to provide'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Transportation & Smart Cities \\nUse Case: Traffic Management & Route Optimization \\n• Example: Google Maps utilizes real-time GPS data and big data analytics to provide \\noptimal travel routes and predict traffic conditions. \\n• Impact:  \\no Reduced traffic congestion and travel delays. \\no Efficient public transport scheduling and urban planning. \\no Safer roads through predictive accident analysis. \\n \\n7. Education & EdTech \\nUse Case: Adaptive Learning & Student Performance Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example: Platforms like Coursera and Udemy use AI-driven analytics to track \\nstudent progress and recommend personalized learning paths. \\n• Impact:  \\no Enhanced learning outcomes through customized courses. \\no Early identification of struggling students for targeted assistance. \\no Data-driven improvements in course content and teaching strategies. \\n \\n8. Entertainment & Media \\nUse Case: Content Recommendation & Audience Insights \\n• Example: Netflix and Spotify analyze user preferences to offer personalized movie, \\nmusic, and podcast recommendations. \\n• Impact:  \\no Increased viewer engagement and retention. \\no Improved content production based on audience trends. \\no Higher advertising revenue through targeted ads. \\n \\n9. Agriculture & Farming \\nUse Case: Precision Farming & Yield Prediction \\n• Example: John Deere uses big data to analyze soil conditions, weather patterns, \\nand crop health for smart farming decisions. \\n• Impact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"Use Case: Precision Farming & Yield Prediction \\n• Example: John Deere uses big data to analyze soil conditions, weather patterns, \\nand crop health for smart farming decisions. \\n• Impact:  \\no Increased crop yields through data-driven farming techniques. \\no Reduced resource wastage (water, fertilizers, and pesticides). \\no Early detection of plant diseases to prevent large-scale losses. \\n \\n10. Energy & Utilities \\nUse Case: Smart Grid Management & Energy Optimization \\n• Example: Tesla's Powerwall and smart grid systems use big data to predict energy \\ndemand and optimize electricity distribution. \\n• Impact:  \\no Reduced energy waste and improved efficiency.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='o Integration of renewable energy sources for sustainability. \\no Lower electricity costs for consumers. \\n \\nConclusion \\nBig Data is revolutionizing industries by driving efficiency, enhancing customer experience, \\nand enabling predictive decision-making. The continuous evolution of big data analytics, \\nAI, and IoT will further transform businesses, optimize operations, and shape the future of \\ndigital innovation.         \\n8. Identify the challenges of the Big Data applications \\nin Modern Data Analytics.  \\nChatGPT said: \\nChallenges of Big Data Applications in Modern Data Analytics \\nBig Data has transformed industries by enabling large-scale data processing, AI-driven \\ninsights, and real-time decision-making. However, its implementation faces significant \\nchallenges that organizations must address. \\n \\n1. Data Quality & Inconsistency \\n• Challenge: Inaccurate, incomplete, or redundant data affects analytical outcomes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='challenges that organizations must address. \\n \\n1. Data Quality & Inconsistency \\n• Challenge: Inaccurate, incomplete, or redundant data affects analytical outcomes. \\n• Impact: Poor data quality leads to unreliable insights and flawed decision-making. \\n• Solution: Implement data cleaning, validation techniques, and ETL (Extract, \\nTransform, Load) processes. \\n \\n2. Data Storage & Management \\n• Challenge: Handling massive volumes of structured and unstructured data requires \\nefficient storage solutions. \\n• Impact: High storage costs and inefficient data retrieval slow down analytics. \\n• Solution: Utilize distributed storage systems like Hadoop HDFS, Amazon S3, and \\nGoogle BigQuery. \\n \\n3. Scalability & Processing Speed \\n• Challenge: Traditional databases struggle to scale and process real-time data \\nefficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Impact: Delayed insights reduce the effectiveness of time-sensitive analytics. \\n• Solution: Implement parallel computing, cloud-based architectures, and real-time \\nprocessing tools like Apache Spark. \\n \\n4. Data Security & Privacy \\n• Challenge: Sensitive data is vulnerable to breaches, cyberattacks, and unauthorized \\naccess. \\n• Impact: Non-compliance with data regulations (GDPR, HIPAA) can result in legal \\npenalties and loss of customer trust. \\n• Solution: Use encryption, access control, and blockchain-based security \\nframeworks. \\n \\n5. Integration of Diverse Data Sources \\n• Challenge: Big Data comes from multiple sources (IoT, social media, enterprise \\nsystems), leading to integration complexities. \\n• Impact: Data silos prevent seamless analytics and decision-making. \\n• Solution: Employ data lakes, APIs, and middleware solutions for interoperability. \\n \\n6. High Computational Costs \\n• Challenge: Running machine learning models and large-scale analytics requires'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Solution: Employ data lakes, APIs, and middleware solutions for interoperability. \\n \\n6. High Computational Costs \\n• Challenge: Running machine learning models and large-scale analytics requires \\nsignificant computational power. \\n• Impact: Expensive cloud resources and infrastructure investments can limit \\nscalability. \\n• Solution: Optimize data pipelines, use edge computing, and implement cost-\\nefficient AI models. \\n \\n7. Talent Shortage & Skill Gaps \\n• Challenge: Organizations struggle to find skilled professionals in Big Data, AI, and \\ncloud technologies. \\n• Impact: Delays in project implementation and reduced efficiency in analytics. \\n• Solution: Invest in employee training, AI-driven automation, and low-code/no-code \\nanalytics platforms.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='8. Ethical & Bias Issues \\n• Challenge: AI-driven analytics can inherit biases from historical data, leading to \\nunfair outcomes. \\n• Impact: Discrimination in hiring, finance, and healthcare decisions. \\n• Solution: Ensure diverse datasets, apply bias detection algorithms, and enforce \\nethical AI guidelines. \\n \\n9. Real-time Analytics Complexity \\n• Challenge: Processing high-velocity data streams from IoT, stock markets, and social \\nmedia requires advanced real-time analytics. \\n• Impact: Delays in detecting fraud, monitoring system failures, and responding to \\ncustomer needs. \\n• Solution: Leverage real-time processing frameworks like Apache Kafka, Apache \\nFlink, and in-memory databases. \\n \\n10. Compliance & Regulatory Challenges \\n• Challenge: Governments enforce strict regulations on data usage, retention, and \\ncross-border transfers. \\n• Impact: Non-compliance results in heavy fines and operational disruptions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Challenge: Governments enforce strict regulations on data usage, retention, and \\ncross-border transfers. \\n• Impact: Non-compliance results in heavy fines and operational disruptions. \\n• Solution: Adopt transparent data governance policies, comply with GDPR, CCPA, \\nand industry standards. \\n \\nConclusion \\nAddressing these challenges is crucial for businesses to fully leverage Big Data analytics. By \\nadopting advanced AI, cloud computing, security protocols, and ethical AI practices, \\norganizations can unlock the true potential of Big Data for competitive advantage.         \\n9. Explain the big data architecture for \\nhandling large-scale social media analytics.  \\nChatGPT said: \\nBig Data Architecture for Handling Large-Scale Social Media Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Social media generates vast amounts of high-velocity, high-variety data. A robust Big Data \\nArchitecture is essential to process, analyze, and extract meaningful insights in real-time. \\n \\n1. Data Sources \\nSocial media data originates from: \\n• Platforms: Facebook, Twitter, Instagram, LinkedIn, TikTok, etc. \\n• Media Types: Text posts, images, videos, comments, reactions, hashtags. \\n• APIs & Streams: Twitter API, Facebook Graph API, YouTube Data API. \\n \\n2. Data Ingestion Layer \\nResponsible for collecting real-time and batch data. \\n• Tools: Apache Kafka, Apache Flume, AWS Kinesis, Google Pub/Sub. \\n• Process: Streams large volumes of unstructured and semi-structured data for \\nfurther processing. \\n \\n3. Storage Layer \\nStores raw, processed, and structured data efficiently. \\n• Distributed Storage: Hadoop HDFS, Amazon S3, Google Cloud Storage. \\n• NoSQL Databases: MongoDB, Cassandra for semi-structured and unstructured data. \\n• Columnar Databases: Apache HBase for fast read/write access.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• NoSQL Databases: MongoDB, Cassandra for semi-structured and unstructured data. \\n• Columnar Databases: Apache HBase for fast read/write access. \\n \\n4. Processing & Analytics Layer \\nHandles real-time and batch processing for insights extraction. \\n• Batch Processing: Apache Spark, Hadoop MapReduce for deep historical analysis. \\n• Real-time Processing: Apache Flink, Storm, or Spark Streaming for immediate trend \\ndetection. \\n• Machine Learning & AI: TensorFlow, PyTorch, Scikit-Learn for sentiment analysis, \\nuser behavior prediction. \\n \\n5. Querying & Data Access Layer \\nProvides interfaces to access, query, and manipulate data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• SQL Query Engines: Apache Hive, Presto, Google BigQuery for structured queries. \\n• Graph Databases: Neo4j for relationship analysis (e.g., influencer networks). \\n• Data Indexing: Elasticsearch for fast text searches and log analysis. \\n \\n6. Visualization & Business Intelligence Layer \\nConverts data into meaningful insights for decision-making. \\n• Tools: Tableau, Power BI, Kibana, Google Data Studio. \\n• Use Cases: Engagement metrics, hashtag trends, audience demographics, and real-\\ntime dashboards. \\n \\n7. Security & Governance Layer \\nEnsures compliance, privacy, and data integrity. \\n• Access Control: AWS IAM, Role-Based Access Control (RBAC). \\n• Data Privacy: GDPR, CCPA compliance for personal data protection. \\n• Encryption & Logging: TLS, SSL, and logging tools for secure access. \\n \\nConclusion \\nA well-structured Big Data architecture enables organizations to analyze large-scale social \\nmedia data, detect trends, enhance customer engagement, and improve business'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Conclusion \\nA well-structured Big Data architecture enables organizations to analyze large-scale social \\nmedia data, detect trends, enhance customer engagement, and improve business \\nstrategies in real time.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='CH : 2 15 Marks \\n1.If a file of 5 GB is uploaded to HDFS and the block size is 1 \\nGB, explain how HDFS will divide and store this file across \\nmultiple DataNodes. Assume replication factor of 2. \\nHDFS File Storage and Replication in a Distributed System (15 Marks) \\nWhen a 5 GB file is uploaded to HDFS with a block size of 1 GB and a replication factor of 2, \\nthe system handles the storage and replication as follows: \\n \\n1. File Splitting in HDFS \\nHDFS follows a distributed file storage system, where large files are divided into fixed-size \\nblocks. Since the block size is 1 GB, a 5 GB file will be split into 5 blocks, each of size 1 GB. \\nBlock Number Size \\nBlock 1 1 GB \\nBlock 2 1 GB \\nBlock 3 1 GB \\nBlock 4 1 GB \\nBlock 5 1 GB \\nThese blocks are then distributed across multiple DataNodes for parallel processing and \\nfault tolerance. \\n \\n2. Data Distribution Across DataNodes \\nHDFS follows a master-slave architecture, where the NameNode manages metadata, and \\nDataNodes store the actual blocks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='fault tolerance. \\n \\n2. Data Distribution Across DataNodes \\nHDFS follows a master-slave architecture, where the NameNode manages metadata, and \\nDataNodes store the actual blocks. \\nEach of the 5 blocks is stored on a different DataNode, and since the replication factor is 2, \\neach block has two copies stored on different nodes. \\nBlock Primary DataNode Replica DataNode \\nBlock 1 DataNode A DataNode D \\nBlock 2 DataNode B DataNode E \\nBlock 3 DataNode C DataNode F'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Block Primary DataNode Replica DataNode \\nBlock 4 DataNode D DataNode A \\nBlock 5 DataNode E DataNode B \\nThis replication ensures fault tolerance, meaning if one DataNode fails, another node has a \\ncopy. \\n \\n3. Advantages of HDFS Storage Strategy \\nHDFS ensures reliability, fault tolerance, and parallel processing by implementing the \\nfollowing strategies: \\n   Fault Tolerance: \\n• Since each block has 2 copies, data is not lost even if a node crashes. \\n• The NameNode monitors block health and re-replicates blocks in case of failures. \\n   Load Balancing & Scalability: \\n• Blocks are evenly distributed across multiple DataNodes, preventing overloading \\non a single machine. \\n• The system scales horizontally by adding more DataNodes when storage demand \\nincreases. \\n   Parallel Processing (MapReduce Support): \\n• Since different blocks are stored on multiple nodes, parallel processing using \\nMapReduce speeds up large-scale computations. \\n   High Availability:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Parallel Processing (MapReduce Support): \\n• Since different blocks are stored on multiple nodes, parallel processing using \\nMapReduce speeds up large-scale computations. \\n   High Availability: \\n• The NameNode tracks all block locations and helps in quick recovery if a node fails. \\n• HDFS ensures continuous access to data even during failures. \\n \\n4. Step-by-Step Process of Uploading a File to HDFS \\n1⃣ File Upload: The 5 GB file is uploaded to HDFS. \\n2⃣ Block Splitting: HDFS divides the file into 5 blocks (each 1 GB). \\n3⃣ DataNode Assignment: The NameNode assigns DataNodes to store each block. \\n4⃣ Replication: Each block is replicated twice across different DataNodes. \\n5⃣ Data Retrieval: When a client requests the file, blocks are fetched and reassembled.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Conclusion \\nHDFS follows an efficient, scalable, and fault-tolerant architecture to manage large files in \\ndistributed environments. By splitting the file into blocks, distributing them across nodes, \\nand ensuring replication, HDFS ensures high availability, reliability, and efficient processing \\nof big data workloads. \\n2. Demonstrate the working of Map Reduce algorithm \\nto find the frequency of words in a given input text. \\nprovide answer for 15 marks \\nChatGPT said: \\nDemonstration of MapReduce Algorithm for Word Frequency Calculation (15 Marks) \\n1. Introduction to MapReduce \\nMapReduce is a distributed data processing model that divides tasks into two main \\nphases: \\n  Map Phase: Processes input data and generates key-value pairs. \\n  Reduce Phase: Aggregates and summarizes results. \\nIn this example, we demonstrate how MapReduce calculates word frequency from a given \\ninput text. \\n \\n2. Input Example \\nConsider the following input text: \\n\"Big data is powerful. Big data is everywhere.\"'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='input text. \\n \\n2. Input Example \\nConsider the following input text: \\n\"Big data is powerful. Big data is everywhere.\" \\n \\n3. Step-by-Step Execution of MapReduce Algorithm \\nStep 1: Map Phase \\n• The input text is split into words. \\n• Each word is emitted as a key-value pair (word, 1). \\nInput (Text Split into Words) Mapper Output (Key-Value Pairs) \\nBig (Big, 1) \\ndata (data, 1) \\nis (is, 1)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Input (Text Split into Words) Mapper Output (Key-Value Pairs) \\npowerful. (powerful, 1) \\nBig (Big, 1) \\ndata (data, 1) \\nis (is, 1) \\neverywhere. (everywhere, 1) \\n  Intermediate Output from Mapper: \\nscss \\nCopyEdit \\n(Big, 1)   \\n(data, 1)   \\n(is, 1)   \\n(powerful, 1)   \\n(Big, 1)   \\n(data, 1)   \\n(is, 1)   \\n(everywhere, 1)   \\n \\nStep 2: Shuffle & Sort Phase \\n• The MapReduce framework groups values by keys before passing them to the \\nreducer. \\nShuffled Key-Value Pairs \\n(Big, [1, 1]) \\n(data, [1, 1]) \\n(is, [1, 1]) \\n(powerful, [1]) \\n(everywhere, [1])'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 26, 'page_label': '27', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Step 3: Reduce Phase \\n• The Reducer aggregates the counts for each word. \\nReducer Input Reducer Output \\n(Big, [1, 1]) (Big, 2) \\n(data, [1, 1]) (data, 2) \\n(is, [1, 1]) (is, 2) \\n(powerful, [1]) (powerful, 1) \\n(everywhere, [1]) (everywhere, 1) \\n  Final Output from Reducer: \\nscss \\nCopyEdit \\n(Big, 2)   \\n(data, 2)   \\n(is, 2)   \\n(powerful, 1)   \\n(everywhere, 1)   \\n \\n4. Python Implementation of MapReduce Word Count \\nHere’s a simple Python program to demonstrate word frequency using MapReduce: \\npython \\nCopyEdit \\nfrom collections import defaultdict \\n# Sample input text \\ntext = \"Big data is powerful. Big data is everywhere.\" \\n# Step 1: Mapper Function \\ndef mapper(text): \\n    words = text.lower().replace(\".\", \"\").split()  # Convert to lowercase & split \\n    return [(word, 1) for word in words]  # Emit (word, 1) pairs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='# Step 2: Shuffle & Sort (Grouping by Key) \\ndef shuffle_sort(mapped_data): \\n    grouped_data = defaultdict(list) \\n    for word, count in mapped_data: \\n        grouped_data[word].append(count) \\n    return grouped_data \\n# Step 3: Reducer Function \\ndef reducer(grouped_data): \\n    return {word: sum(counts) for word, counts in grouped_data.items()}  # Summing counts \\n# Running MapReduce \\nmapped_data = mapper(text) \\ngrouped_data = shuffle_sort(mapped_data) \\nword_frequencies = reducer(grouped_data) \\n# Display Output \\nprint(\"Word Frequencies:\", word_frequencies) \\n  Expected Output: \\nyaml \\nCopyEdit \\nWord Frequencies: {\\'big\\': 2, \\'data\\': 2, \\'is\\': 2, \\'powerful\\': 1, \\'everywhere\\': 1} \\n \\n5. Key Advantages of MapReduce for Word Count \\n   Scalability: Handles massive datasets across multiple machines. \\n   Fault Tolerance: Ensures high availability in distributed systems. \\n   Parallel Processing: Efficiently splits work across nodes. \\n   Load Balancing: Distributes tasks evenly to optimize performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Parallel Processing: Efficiently splits work across nodes. \\n   Load Balancing: Distributes tasks evenly to optimize performance. \\n \\n6. Conclusion \\nThe MapReduce algorithm efficiently counts word occurrences in large-scale datasets. It \\nsplits text into words, maps them to key-value pairs, shuffles & sorts the data, and reduces \\nit to compute the final word frequencies. This approach is widely used in big data \\nanalytics, log processing, and text mining applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='3. Explain in detail about Hadoop YARN. \\nhttps://www.geeksforgeeks.org/hadoop-yarn-architecture/ \\nHadoop YARN (Yet Another Resource Negotiator) - Detailed Explanation (15 Marks) \\n1. Introduction to Hadoop YARN \\nHadoop YARN (Yet Another Resource Negotiator) is the resource management layer in the \\nHadoop ecosystem. It allows multiple applications to share cluster resources efficiently \\nwhile ensuring scalability, fault tolerance, and high availability. \\nKey Features: \\n  Decouples resource management from job scheduling. \\n  Supports multi-tenancy and dynamic resource allocation. \\n  Enables parallel execution of applications. \\n \\n2. YARN Architecture & Components \\nHadoop YARN consists of the following key components: \\n1. ResourceManager (RM) \\n• Master daemon responsible for managing resources across the cluster. \\n• Has two main components:  \\no Scheduler: Allocates resources based on policies (FIFO, Fair, Capacity). \\no ApplicationManager: Manages application lifecycle. \\n2. NodeManager (NM)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Has two main components:  \\no Scheduler: Allocates resources based on policies (FIFO, Fair, Capacity). \\no ApplicationManager: Manages application lifecycle. \\n2. NodeManager (NM) \\n• Runs on each worker node and monitors resource usage. \\n• Communicates with ResourceManager and executes tasks in containers. \\n3. ApplicationMaster (AM) \\n• Created for each application and negotiates resources from RM. \\n• Monitors execution of tasks and handles failures. \\n4. Containers \\n• Isolated execution environments where actual tasks run. \\n• Dynamically allocated by RM based on CPU & memory requirements. \\n \\n3. Working of YARN'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Step 1: Application Submission \\n• A user submits an application (MapReduce, Spark, Hive) to YARN. \\nStep 2: Resource Allocation \\n• RM assigns containers to run the ApplicationMaster (AM) for that job. \\nStep 3: Task Execution \\n• AM requests resources from RM. \\n• NM launches containers and executes tasks. \\nStep 4: Monitoring & Completion \\n• AM monitors task execution and reports status to RM. \\n• Once completed, AM releases resources back to the cluster. \\n \\n4. Advantages of YARN \\n   Efficient Resource Utilization – Dynamic allocation of CPU & memory. \\n   Multi-Tenancy Support – Runs multiple frameworks like Spark, Tez, Flink, HBase. \\n   Scalability – Handles thousands of nodes efficiently. \\n   Fault Tolerance – Automatically reschedules failed tasks. \\n \\n5. Real-World Applications of YARN \\n  Big Data Processing – Supports MapReduce, Apache Spark, Apache Flink. \\n  Data Warehousing – Runs Apache Hive for SQL-based queries.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Real-World Applications of YARN \\n  Big Data Processing – Supports MapReduce, Apache Spark, Apache Flink. \\n  Data Warehousing – Runs Apache Hive for SQL-based queries. \\n  Machine Learning & AI – Supports ML frameworks like TensorFlow on YARN. \\n  Streaming Applications – Works with Apache Storm & Kafka for real-time analytics. \\n \\n6. Conclusion \\nHadoop YARN efficiently manages cluster resources, enabling large-scale data processing. \\nIts scalability, fault tolerance, and flexibility make it a backbone for big data applications. \\n        \\n5. Explain about Hadoop distributed file system \\narchitecture with neat diagram. \\nHadoop Distributed File System (HDFS) Architecture \\n1. Introduction to HDFS'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Hadoop Distributed File System (HDFS) is a highly scalable, fault-tolerant, and \\ndistributed storage system designed for big data applications. It enables efficient \\nstorage and processing of massive datasets across a cluster of commodity hardware. \\nHDFS is inspired by the Google File System (GFS) and follows a master-slave \\narchitecture to manage large-scale data efficiently. \\n \\n2. Key Components of HDFS \\nHDFS consists of three main components: \\n1. NameNode (Master Node) \\n• Acts as the central metadata manager of the HDFS cluster. \\n• Tracks file structure, block locations, and namespace metadata. \\n• Does not store actual data but manages file system operations. \\n• A single point of failure unless High Availability (HA) is enabled. \\n2. DataNode (Worker Nodes) \\n• Stores actual data blocks on local disks. \\n• Periodically sends heartbeat signals to the NameNode to confirm availability. \\n• Performs read, write, replication, and deletion operations based on NameNode \\ninstructions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Periodically sends heartbeat signals to the NameNode to confirm availability. \\n• Performs read, write, replication, and deletion operations based on NameNode \\ninstructions. \\n3. Secondary NameNode \\n• Not a backup NameNode but assists in maintaining system consistency. \\n• Periodically saves checkpoints of the file system metadata. \\n \\n3. HDFS File Storage and Block Replication \\n• When a file is uploaded, HDFS splits it into fixed-size blocks (default 128MB or \\n256MB). \\n• Each block is stored across multiple DataNodes for redundancy and fault tolerance. \\n• Default replication factor is 3, meaning each block is replicated on three different \\nnodes. \\n  Example: \\nA 600MB file stored in HDFS with a block size of 128MB will be divided into 5 blocks: \\nB1 (128MB), B2 (128MB), B3 (128MB), B4 (128MB), B5 (88MB) \\nEach block will be replicated three times across different DataNodes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Data Flow in HDFS \\nA. Writing Data to HDFS \\n     Client → NameNode → Assigns DataNodes → Data is split & stored with \\nreplication. \\n• The client requests the NameNode to store a file. \\n• The NameNode allocates DataNodes to store the file in blocks. \\n• Data is written in parallel across multiple DataNodes with replication. \\nB. Reading Data from HDFS \\n     Client → NameNode → Fetches block locations → Reads directly from DataNodes. \\n• The client requests the NameNode to read a file. \\n• The NameNode provides the list of DataNodes containing the requested blocks. \\n• The client retrieves data directly from the nearest DataNodes to optimize \\nperformance. \\n \\n5. HDFS Architecture Diagram \\n     Below is a neat diagram illustrating HDFS architecture: \\n(Insert a well-labeled diagram with NameNode, DataNodes, Replication, and Client \\ninteractions.) \\n \\n6. Features & Benefits of HDFS \\n   Scalability – Handles petabytes of data efficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='(Insert a well-labeled diagram with NameNode, DataNodes, Replication, and Client \\ninteractions.) \\n \\n6. Features & Benefits of HDFS \\n   Scalability – Handles petabytes of data efficiently. \\n   Fault Tolerance – Data replication ensures no data loss. \\n   High Throughput – Optimized for large-scale batch processing. \\n   Cost-Effective – Works on commodity hardware, reducing infrastructure costs. \\n   Reliability – Automatic replication and fault recovery. \\n \\n7. Real-World Applications of HDFS \\n  Social Media Platforms – Facebook, Twitter, and Instagram store vast amounts of \\nuser data. \\n  Search Engines – Google and Bing use distributed storage for indexing and ranking \\nweb pages.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='E-Commerce Platforms – Amazon and Flipkart process customer data and \\ntransaction logs. \\n  Healthcare & Finance – Fraud detection, medical imaging, and data-driven \\ndecision-making. \\n \\n8. Conclusion \\nHDFS is the backbone of the Hadoop ecosystem, providing a reliable, scalable, and \\nefficient storage solution for big data applications. Its ability to handle large datasets \\nacross distributed environments makes it a crucial technology for modern data-driven \\nenterprises.         \\nhttps://www.geeksforgeeks.org/explain-the-hadoop-distributed-file-system-hdfs-\\narchitecture-and-advantages/ \\n \\n6.How does the MapReduce programming model \\nwork? Explain with an example.  \\nChatGPT said: \\nMapReduce Programming Model \\n1. Introduction to MapReduce \\nMapReduce is a distributed computing framework in Hadoop used for processing \\nlarge-scale datasets in parallel across multiple nodes. It follows a divide-and-conquer \\napproach, breaking tasks into smaller chunks that can be executed independently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='large-scale datasets in parallel across multiple nodes. It follows a divide-and-conquer \\napproach, breaking tasks into smaller chunks that can be executed independently. \\n• Developed by Google, later implemented in Hadoop. \\n• Works on the principle of parallelism to process big data efficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Consists of two main phases: Map and Reduce. \\n \\n2. Working of MapReduce \\nA. Map Phase (Splitting & Processing) \\n• Input data is split into chunks (blocks) and processed in parallel. \\n• Each chunk is passed to a Mapper function, which processes the data and \\ngenerates intermediate key-value pairs. \\nB. Shuffle & Sort Phase (Intermediate Processing) \\n• The framework sorts and groups the key-value pairs from the Mappers. \\n• Ensures that all values associated with a specific key are grouped together before \\npassing them to the Reducer. \\nC. Reduce Phase (Aggregation & Output Generation) \\n• The grouped data is passed to the Reducer function, which processes and \\naggregates the values for each key. \\n• The final result is stored in HDFS. \\n \\n3. Example: Word Count Program Using MapReduce \\nProblem Statement: \\nFind the frequency of each word in a given input text file. \\nStep-by-Step Execution: \\n1⃣ Input Data (Stored in HDFS): \\ncsharp \\nCopyEdit \\nHello Hadoop   \\nHadoop is powerful'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Problem Statement: \\nFind the frequency of each word in a given input text file. \\nStep-by-Step Execution: \\n1⃣ Input Data (Stored in HDFS): \\ncsharp \\nCopyEdit \\nHello Hadoop   \\nHadoop is powerful   \\nHello Big Data   \\n2⃣ Map Phase (Key-Value Generation by Mapper): \\nEach line is processed by a Mapper function that splits text into words and assigns a \\ncount of 1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 34, 'page_label': '35', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Input Line Mapper Output (Key, Value) \\nHello Hadoop (Hello, 1), (Hadoop, 1) \\nHadoop is powerful (Hadoop, 1), (is, 1), (powerful, 1) \\nHello Big Data (Hello, 1), (Big, 1), (Data, 1) \\n3⃣ Shuffle & Sort Phase: \\nThe framework groups and sorts values by keys. \\nKey Grouped Values \\nBig (1) \\nData (1) \\nHadoop (1, 1) \\nHello (1, 1) \\nis (1) \\npowerful (1) \\n4⃣ Reduce Phase (Aggregation by Reducer): \\nThe Reducer function sums the values for each key. \\nKey Final Count (Reducer Output) \\nBig 1 \\nData 1 \\nHadoop 2 \\nHello 2 \\nis 1 \\npowerful 1 \\n5⃣ Final Output Stored in HDFS: \\ncsharp \\nCopyEdit \\nBig 1   \\nData 1'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Hadoop 2   \\nHello 2   \\nis 1   \\npowerful 1   \\n \\n4. Advantages of MapReduce \\n   Scalability – Can process petabytes of data efficiently. \\n   Fault Tolerance – Automatic recovery from node failures. \\n   Parallelism – Runs tasks across multiple nodes simultaneously. \\n   Ease of Use – Handles low-level distributed computing complexities. \\n \\n5. Conclusion \\nMapReduce is a highly efficient and scalable data processing model widely used in big \\ndata analytics. Its parallel processing capabilities make it suitable for handling massive \\ndatasets in real-world applications like log analysis, recommendation systems, and \\nmachine learning preprocessing.         \\n4o \\n7. Summarize the features of Hadoop and explain \\nthe functionalities of Hadoop cluster? provide \\nanswer for 15 marks  \\nChatGPT said: \\nHadoop: Features & Cluster Functionalities \\n1. Features of Hadoop \\nHadoop is an open-source framework for storing and processing large-scale data across'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='answer for 15 marks  \\nChatGPT said: \\nHadoop: Features & Cluster Functionalities \\n1. Features of Hadoop \\nHadoop is an open-source framework for storing and processing large-scale data across \\na distributed computing environment. Its key features include: \\nA. Scalability \\n• Can handle petabytes of data by adding new nodes to the cluster without affecting \\nperformance. \\nB. Fault Tolerance \\n• Uses HDFS (Hadoop Distributed File System) to replicate data across multiple \\nnodes, ensuring reliability even if nodes fail.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Distributed Processing \\n• Utilizes the MapReduce programming model for parallel execution, reducing \\nprocessing time. \\nD. Cost-Effective \\n• Runs on commodity hardware, reducing infrastructure costs. \\nE. High Availability \\n• If one node fails, data remains accessible due to multiple copies stored across \\ndifferent nodes. \\nF. Flexibility \\n• Supports structured, semi-structured, and unstructured data from logs, social \\nmedia, emails, and sensors. \\nG. Security & Authentication \\n• Kerberos-based authentication and data encryption provide secure data access. \\n \\n2. Functionalities of a Hadoop Cluster \\nA Hadoop cluster is a group of interconnected computers that work together to store \\nand process massive datasets. It consists of the following key components: \\nA. Master Node (NameNode & ResourceManager) \\n1. NameNode (HDFS Master) \\no Manages metadata (file locations, permissions, hierarchy) in HDFS. \\no Directs DataNodes on where to store and retrieve data. \\n2. ResourceManager (YARN Master)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. NameNode (HDFS Master) \\no Manages metadata (file locations, permissions, hierarchy) in HDFS. \\no Directs DataNodes on where to store and retrieve data. \\n2. ResourceManager (YARN Master) \\no Allocates resources (CPU, memory) to applications running in the cluster. \\no Manages job scheduling and execution. \\nB. Worker Nodes (DataNodes & NodeManagers) \\n1. DataNodes (HDFS Workers) \\no Store actual data in block format (default 128MB per block). \\no Send heartbeat signals to the NameNode for health monitoring. \\n2. NodeManagers (YARN Workers) \\no Execute tasks assigned by the ResourceManager. \\no Monitor resource usage and report back to the ResourceManager.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Job Execution Process in a Hadoop Cluster \\n1. Data is split into blocks and distributed across DataNodes. \\n2. The NameNode keeps track of data locations. \\n3. The ResourceManager assigns computing resources for MapReduce tasks. \\n4. Mappers process data in parallel and send intermediate results to Reducers. \\n5. Reducers aggregate data and store the final output in HDFS. \\n \\n3. Conclusion \\nHadoop’s scalability, fault tolerance, and distributed processing make it ideal for big \\ndata applications like log analysis, recommendation systems, and real-time analytics. A \\nHadoop cluster efficiently manages data storage and processing, ensuring high \\navailability and performance across large datasets.  \\n \\n \\n8. Apply the limitations of the traditional \\nMapReduce model and how YARN helps overcome \\nthem. provide answer for 15 marks \\nChatGPT said: \\nLimitations of Traditional MapReduce and How YARN Overcomes Them \\n1. Introduction to MapReduce'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='MapReduce model and how YARN helps overcome \\nthem. provide answer for 15 marks \\nChatGPT said: \\nLimitations of Traditional MapReduce and How YARN Overcomes Them \\n1. Introduction to MapReduce \\nMapReduce is a distributed processing model used in Hadoop for large-scale data \\nprocessing. It follows a two-stage process: \\n1. Map Phase: Processes input data and generates key-value pairs. \\n2. Reduce Phase: Aggregates the intermediate output to produce final results. \\nAlthough MapReduce was revolutionary in handling big data, it had several limitations \\nthat restricted its efficiency and scalability. YARN (Yet Another Resource Negotiator) \\nwas introduced in Hadoop 2.0 to overcome these challenges. \\n \\n2. Limitations of Traditional MapReduce \\nA. Scalability Issues'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Limitation: The JobTracker manages both resource allocation and job scheduling, \\nleading to bottlenecks. \\n• Impact: As clusters grow, JobTracker fails to efficiently manage resources, limiting \\nscalability. \\nB. Inefficient Resource Utilization \\n• Limitation: Each MapReduce job requires separate Map and Reduce slots, leading \\nto resource wastage. \\n• Impact: If only reduce tasks are running, map slots remain idle, reducing cluster \\nefficiency. \\nC. High Latency for Iterative Jobs \\n• Limitation: Each MapReduce job must reload data from HDFS, even if the same \\ndata is required multiple times. \\n• Impact: Machine Learning, Graph Processing, and Iterative Analytics suffer due to \\nrepeated I/O operations. \\nD. Limited Support for Real-Time and Interactive Processing \\n• Limitation: MapReduce is batch-oriented and not optimized for real-time or \\ninteractive queries. \\n• Impact: Not suitable for use cases like stream processing, ad-hoc querying, and \\nreal-time analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='interactive queries. \\n• Impact: Not suitable for use cases like stream processing, ad-hoc querying, and \\nreal-time analytics. \\nE. Single Point of Failure \\n• Limitation: The JobTracker is a single component responsible for all jobs and \\nresources. \\n• Impact: If JobTracker fails, entire Hadoop processing stops. \\nF. Lack of Multi-Tenancy \\n• Limitation: Hadoop’s MapReduce jobs run in a single execution model, restricting \\nmultiple applications. \\n• Impact: Difficult to run multiple workloads (e.g., Spark, Tez, or Storm) on the same \\ncluster. \\n \\n3. How YARN Overcomes These Limitations \\nYARN decouples job management and resource management, making Hadoop more \\nefficient and flexible. \\nA. Enhanced Scalability with ResourceManager'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Solution: YARN introduces a ResourceManager (RM) and NodeManagers (NM) \\ninstead of JobTracker. \\n• Benefit: Distributes resource management, allowing thousands of nodes to be \\nhandled efficiently. \\nB. Improved Resource Utilization via Dynamic Allocation \\n• Solution: Uses containers instead of fixed slots for Map and Reduce tasks. \\n• Benefit: Resources are dynamically assigned based on actual needs, improving \\nutilization. \\nC. Reduced Latency for Iterative and Interactive Processing \\n• Solution: YARN allows applications like Apache Spark, Tez, and HBase to run \\nalongside MapReduce. \\n• Benefit: In-memory computing reduces latency for iterative jobs like machine \\nlearning and graph analytics. \\nD. Support for Real-Time Processing and Multi-Tenancy \\n• Solution: YARN enables multiple data processing frameworks (e.g., Spark, Flink, \\nStorm) to run concurrently. \\n• Benefit: Hadoop supports real-time, batch, and interactive applications \\nsimultaneously.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Storm) to run concurrently. \\n• Benefit: Hadoop supports real-time, batch, and interactive applications \\nsimultaneously. \\nE. Fault Tolerance and High Availability \\n• Solution: JobTracker responsibilities are split into ResourceManager and \\nApplicationMaster. \\n• Benefit: Eliminates a single point of failure, making Hadoop more resilient. \\nF. Better Cluster Utilization \\n• Solution: Dynamic resource allocation prevents idle resources. \\n• Benefit: Maximizes cluster efficiency and reduces execution time for jobs. \\n \\n4. Conclusion \\nWhile traditional MapReduce had limitations like poor resource utilization, high \\nlatency, and lack of multi-tenancy, YARN provided a more efficient, scalable, and \\nflexible framework for Hadoop. By separating job scheduling and resource \\nmanagement, YARN supports multiple processing engines, better fault tolerance, and \\nimproved performance, making Hadoop suitable for modern big data workloads.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='UNIT 3 15 marks \\n1.Explain in detail about NoSQL databases and \\nschema less models . Justify the need for the same. \\nprovide answer for 15 marks \\nNoSQL Databases and Schema-less Models \\n1. Introduction to NoSQL Databases \\nNoSQL databases (Not Only SQL) are modern database management systems designed to \\nhandle large volumes of data that are often unstructured or semi-structured. Unlike \\ntraditional relational databases (RDBMS) that follow a strict schema with predefined \\ntables and relationships, NoSQL databases provide flexibility, scalability, and high \\nperformance by allowing dynamic schema and distributed storage. \\nNoSQL databases were developed to address the limitations of relational databases, \\nespecially in handling Big Data, real-time applications, and cloud computing environments. \\n \\n2. Characteristics of NoSQL Databases \\nNoSQL databases have the following key characteristics: \\n1. Schema-Less Data Model – No predefined schema is required, allowing easy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. Characteristics of NoSQL Databases \\nNoSQL databases have the following key characteristics: \\n1. Schema-Less Data Model – No predefined schema is required, allowing easy \\nmodifications to data structures. \\n2. Scalability – NoSQL databases support horizontal scaling, meaning they can expand \\nacross multiple servers. \\n3. High Availability – Designed for distributed architecture, ensuring redundancy and \\nfault tolerance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. High-Speed Read/Write Operations – Optimized for handling real-time data \\nprocessing. \\n5. Flexible Data Storage – Supports structured, semi-structured, and unstructured \\ndata formats like JSON, XML, BSON, etc. \\n \\n3. Types of NoSQL Databases and Their Applications \\nNoSQL databases are classified based on their data model: \\nA. Key-Value Stores \\n• Data is stored as key-value pairs (like a dictionary). \\n• Provides fast lookups and is ideal for caching and session management. \\n• Examples: Redis, DynamoDB, Riak. \\n• Use Case: Storing user session data in web applications. \\nB. Document Stores \\n• Stores data in document format (JSON, BSON, or XML). \\n• Supports nested structures and dynamic fields. \\n• Examples: MongoDB, CouchDB, ArangoDB. \\n• Use Case: Storing customer profiles in e-commerce platforms. \\nC. Column-Family Stores \\n• Data is stored in columns rather than rows, making it efficient for large-scale data \\nprocessing. \\n• Suitable for handling big data workloads.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Column-Family Stores \\n• Data is stored in columns rather than rows, making it efficient for large-scale data \\nprocessing. \\n• Suitable for handling big data workloads. \\n• Examples: Apache Cassandra, HBase. \\n• Use Case: Analyzing real-time user behavior in social media platforms. \\nD. Graph Databases \\n• Designed to store and analyze complex relationships between data. \\n• Used in applications requiring network analysis and recommendation systems. \\n• Examples: Neo4j, Amazon Neptune. \\n• Use Case: Social media relationship mapping and fraud detection. \\n \\n4. Understanding Schema-Less Models'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A schema-less model means that data can be stored without a predefined structure. \\nUnlike RDBMS, where each table has a fixed schema, NoSQL databases allow documents \\nor records to have different attributes, making them flexible and adaptable. \\nAdvantages of Schema-Less Models \\n✔ Flexibility – Developers can modify data structures without downtime. \\n✔ Scalability – Easier to distribute across multiple servers. \\n✔ Performance – Faster read/write operations due to denormalized storage. \\n✔ Ease of Integration – Works well with cloud-native and microservices architectures. \\n \\n5. Justification for Using NoSQL & Schema-Less Models \\nA. Big Data & Real-Time Processing \\n• Traditional relational databases struggle with high-velocity and high-volume data. \\n• NoSQL databases efficiently handle large-scale distributed data for real-time \\nanalytics. \\n• Example: Social media platforms processing millions of posts per second. \\nB. Horizontal Scalability'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• NoSQL databases efficiently handle large-scale distributed data for real-time \\nanalytics. \\n• Example: Social media platforms processing millions of posts per second. \\nB. Horizontal Scalability \\n• RDBMS scales vertically (adding resources to a single machine), which is costly. \\n• NoSQL databases scale horizontally by distributing data across multiple servers, \\nreducing costs. \\n• Example: Cloud applications like Netflix store terabytes of user data using a \\nhorizontally scalable NoSQL architecture. \\nC. Handling Unstructured Data \\n• RDBMS enforces strict data formats, making it hard to store JSON, multimedia, \\nlogs, or IoT data. \\n• NoSQL databases can handle varied data formats without restructuring. \\n• Example: Storing user-generated content in a blogging platform. \\nD. Cloud and Microservices Adoption \\n• NoSQL databases support distributed computing and microservices-based \\narchitectures. \\n• Enable high availability, auto-scaling, and fast processing in cloud environments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• NoSQL databases support distributed computing and microservices-based \\narchitectures. \\n• Enable high availability, auto-scaling, and fast processing in cloud environments. \\n• Example: Online banking systems managing millions of transactions in real time.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Comparison: NoSQL vs. RDBMS \\nFeature NoSQL RDBMS \\nSchema Schema-less, flexible Fixed schema, structured \\nScalability Horizontally scalable Vertically scalable \\nPerformance High-speed read/write Slower for large data \\nData Format JSON, XML, Key-Value, Graph Tables with rows & columns \\nUse Case Big Data, IoT, real-time apps Banking, ERP , transactional apps \\n \\n7. Real-World Applications of NoSQL Databases \\n1. Social Media Platforms – Facebook uses Apache Cassandra to manage billions of \\nposts. \\n2. E-commerce Websites – Amazon uses DynamoDB for product recommendations. \\n3. Streaming Services – Netflix stores metadata in Apache Cassandra. \\n4. Banking and Finance – Fraud detection using graph databases like Neo4j. \\n5. IoT Applications – Smart devices store sensor data in MongoDB and CouchDB. \\n \\n8. Challenges of NoSQL Databases \\n1. Lack of Standardization – No universal query language like SQL. \\n2. Complex Transactions – Not ideal for applications requiring ACID compliance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='8. Challenges of NoSQL Databases \\n1. Lack of Standardization – No universal query language like SQL. \\n2. Complex Transactions – Not ideal for applications requiring ACID compliance. \\n3. Security Risks – Schema-less design can introduce data inconsistency. \\n4. Backup & Recovery – Requires specialized tools for large-scale backups. \\n \\n9. Conclusion \\nNoSQL databases and schema-less models provide a powerful alternative to traditional \\nRDBMS, offering scalability, flexibility, and performance for modern applications. They are \\nessential for big data analytics, cloud computing, IoT, and AI-driven applications. As \\nbusinesses continue to generate vast amounts of unstructured data, NoSQL databases will \\nplay a crucial role in future data management.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. Explain the concept of High-Performance \\nArchitecture in big data processing. \\nHigh-Performance Architecture in Big Data Processing \\n1. Introduction to High-Performance Architecture \\nHigh-Performance Architecture (HPA) in big data processing refers to optimized computing \\nframeworks, storage systems, and network architectures that enable rapid, efficient, and \\nscalable data handling. With the massive growth of structured and unstructured data from \\nsources like IoT, social media, and enterprise applications, traditional architectures \\nstruggle to process data efficiently. \\nHPA leverages parallel processing, distributed computing, and advanced memory \\nmanagement to handle high-volume, high-velocity, and high-variety data. \\n \\n2. Key Components of High-Performance Big Data Architecture \\nA. Distributed Computing Frameworks \\n• Apache Hadoop: Uses HDFS (Hadoop Distributed File System) for storing and \\nMapReduce for parallel processing.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A. Distributed Computing Frameworks \\n• Apache Hadoop: Uses HDFS (Hadoop Distributed File System) for storing and \\nMapReduce for parallel processing. \\n• Apache Spark: An in-memory, distributed processing engine that performs real-\\ntime analytics much faster than Hadoop. \\nB. Scalable Storage Systems \\n• HDFS (Hadoop Distributed File System): Stores large datasets across multiple \\nnodes. \\n• NoSQL Databases (MongoDB, Cassandra): Schema-less models that offer scalability. \\n• Cloud Storage (AWS S3, Google Cloud Storage): Provides elastic storage for \\nhandling dynamic workloads. \\nC. Parallel Processing Techniques \\n• Multi-threading and Multi-core Processing: Enhances CPU utilization for faster \\ncomputations. \\n• Batch and Stream Processing:  \\no Batch Processing (Hadoop): Suitable for large-scale, historical data analysis. \\no Stream Processing (Apache Kafka, Apache Flink): Handles real-time data \\nstreams. \\nD. High-Speed Networking and Interconnects'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Software-Defined Networking (SDN): Optimizes data flow and bandwidth \\nallocation. \\n• High-Performance Clusters: Uses InfiniBand, Fiber Optics, and 5G Networks to \\nreduce latency and improve throughput. \\n \\n3. Characteristics of High-Performance Architecture \\nFeature Description \\nScalability Supports horizontal scaling to handle massive data volumes. \\nLow Latency Reduces response time using in-memory processing (Spark, Redis). \\nFault Tolerance Ensures data redundancy and system recovery (HDFS, Replication). \\nReal-time Processing Uses streaming technologies for live data analysis. \\nOptimized Resource \\nUtilization \\nEfficiently manages CPU, memory, and storage using virtualization \\nand containerization (Docker, Kubernetes). \\n \\n4. Workflow of High-Performance Big Data Processing \\n1. Data Ingestion – Capturing data from IoT, logs, transactions, and social media. \\n2. Data Storage – Using HDFS, NoSQL, or cloud-based systems for scalable storage. \\n3. Data Processing –'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Data Ingestion – Capturing data from IoT, logs, transactions, and social media. \\n2. Data Storage – Using HDFS, NoSQL, or cloud-based systems for scalable storage. \\n3. Data Processing –  \\no Batch Processing (Hadoop MapReduce) for historical data. \\no Real-time Processing (Apache Spark, Kafka) for event-driven analytics. \\n4. Data Analysis & Visualization – Using ML models, BI tools (Tableau, Power BI). \\n5. Data Output & Decision Making – Processed insights are used in AI, business \\nanalytics, and predictive modeling. \\n \\n5. Applications of High-Performance Architecture in Big Data \\n1. E-Commerce & Recommendation Systems – Amazon, Netflix use Spark for real-time \\nrecommendations. \\n2. Healthcare & Genomics – AI-driven diagnosis and genomic analysis with distributed \\ncomputing. \\n3. Financial Fraud Detection – Banks use Spark with Graph Processing for anomaly \\ndetection.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Smart Cities & IoT – Real-time traffic monitoring, energy grid management. \\n5. Cybersecurity – AI-based threat detection & log analysis using high-speed \\ncomputing. \\n \\n6. Advantages of High-Performance Big Data Architecture \\n✔ Faster Processing – In-memory computing reduces latency. \\n✔ Scalable & Cost-Effective – Cloud-based architecture adapts to workload demands. \\n✔ Improved Fault Tolerance – Distributed storage prevents single points of failure. \\n✔ Real-Time Insights – Businesses can act on live data streams. \\n✔ Optimized Hardware Utilization – Uses GPUs, TPUs, and FPGAs for AI workloads. \\n \\n7. Challenges & Future Trends \\n  Challenges: \\n• High infrastructure costs for real-time processing. \\n• Complexity in integrating multi-cloud and hybrid environments. \\n• Security & privacy concerns in distributed computing. \\n        Future Trends: \\n• AI-Driven Data Processing – Automated data pipeline optimization. \\n• Quantum Computing – Future applications for massive data analysis.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Future Trends: \\n• AI-Driven Data Processing – Automated data pipeline optimization. \\n• Quantum Computing – Future applications for massive data analysis. \\n• Edge Computing – 5G-powered low-latency processing for IoT & autonomous \\nsystems. \\n \\n8. Conclusion \\nHigh-Performance Architecture in big data processing enables organizations to process \\nmassive datasets efficiently while ensuring scalability, low latency, and real-time insights. \\nTechnologies like Apache Spark, Hadoop, NoSQL, cloud computing, and AI-powered \\nanalytics will continue shaping the next generation of big data solutions.         \\n3.Describe schema-less models in NoSQL. How do \\nthey enhance data flexibility? providde answetr for 15 \\nmarks question  \\nChatGPT said:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Schema-less Models in NoSQL and Their Role in Enhancing Data Flexibility \\n1. Introduction to Schema-less Models in NoSQL \\nA schema-less model in NoSQL databases means that the database does not enforce a \\nrigid, predefined schema like traditional relational databases (RDBMS). Instead, data can \\nbe stored in a flexible format, allowing dynamic schema evolution based on application \\nneeds. \\nThis flexibility is particularly useful in big data, real-time applications, and unstructured \\ndata storage, where data structures frequently change. \\n \\n2. Types of Schema-less NoSQL Databases \\nNoSQL databases are categorized based on how they store data: \\nA. Document Stores (e.g., MongoDB, CouchDB) \\n• Stores data in JSON, BSON, or XML formats. \\n• Each document can have different fields, structures, or types. \\n• Example:  \\njson \\nCopyEdit \\n{ \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\" } \\n{ \"name\": \"Bob\", \"email\": \"bob@example.com\", \"purchases\": [\"Laptop\", \"Phone\"] }'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example:  \\njson \\nCopyEdit \\n{ \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\" } \\n{ \"name\": \"Bob\", \"email\": \"bob@example.com\", \"purchases\": [\"Laptop\", \"Phone\"] } \\no Alice’s record has age & city, while Bob’s has email & purchases—no fixed \\nschema! \\nB. Key-Value Stores (e.g., Redis, DynamoDB) \\n• Data stored as key-value pairs. \\n• No constraints on the type of values (text, JSON, binary, etc.). \\n• Example:  \\njson \\nCopyEdit \\n\"user_101\" : { \"name\": \"Alice\", \"age\": 25 } \\n\"session_567\" : { \"lastLogin\": \"2024-03-20\", \"active\": true } \\no Different keys store different types of values without predefined schema. \\nC. Column-Family Stores (e.g., Cassandra, HBase)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Data stored in flexible columns, allowing variable attributes per row. \\n• Example:  \\nsql \\nCopyEdit \\nRow ID | Name  | Age | Email   \\n1      | Alice | 25  | -   \\n2      | Bob   | -   | bob@example.com   \\no Some rows may not have certain columns, unlike traditional RDBMS. \\nD. Graph Databases (e.g., Neo4j, ArangoDB) \\n• Stores nodes & relationships without fixed schema. \\n• Example:  \\nscss \\nCopyEdit \\n(Alice) -[:FRIENDS_WITH]-> (Bob) \\n(Bob) -[:PURCHASED]-> (\"Laptop\") \\no Flexible structures allow dynamic relationships. \\n \\n3. How Schema-less Models Enhance Data Flexibility \\n   A. Dynamic Schema Evolution \\n• NoSQL databases do not require ALTER TABLE commands. \\n• Applications can add new fields without downtime. \\n   B. Handling Unstructured & Semi-Structured Data \\n• Supports JSON, XML, key-value pairs, which can store logs, social media posts, and \\nIoT data. \\n• Example: A social media platform can store varying user details dynamically. \\n   C. Scalability & Performance Boost'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='IoT data. \\n• Example: A social media platform can store varying user details dynamically. \\n   C. Scalability & Performance Boost \\n• Schema-less NoSQL databases support horizontal scaling, ideal for big data \\napplications. \\n• Example: Cassandra scales across multiple nodes efficiently. \\n   D. Multi-format Data Storage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Unlike relational databases, which enforce uniform structure, schema-less models \\nallow storing heterogeneous data. \\n• Example: An e-commerce database can store both customer reviews (text) and \\nproduct specs (JSON) in the same collection. \\n   E. Faster Development & Agile Adaptability \\n• Developers can iterate faster without modifying schemas. \\n• Startups & AI-driven applications benefit from schema-less flexibility. \\n \\n4. Real-World Applications of Schema-less NoSQL Models \\nIndustry Use Case NoSQL Type \\nSocial Media Storing user posts, comments, likes Document Store \\n(MongoDB) \\nE-Commerce Product catalogs with varied \\nattributes Column Store (Cassandra) \\nIoT & Sensors Streaming real-time sensor data Key-Value Store (Redis) \\nFraud Detection Graph-based fraud detection Graph DB (Neo4j) \\nAI & Machine \\nLearning Storing unstructured training datasets Document Store (CouchDB) \\n \\n5. Advantages of Schema-less NoSQL Models \\nAdvantage Description'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='AI & Machine \\nLearning Storing unstructured training datasets Document Store (CouchDB) \\n \\n5. Advantages of Schema-less NoSQL Models \\nAdvantage Description \\nFlexibility Data can change dynamically without altering database \\nschema. \\nScalability NoSQL databases scale horizontally, handling massive data \\nloads. \\nPerformance Faster queries with distributed architecture (Cassandra, \\nDynamoDB). \\nMulti-format Data \\nSupport Stores structured, semi-structured, and unstructured data. \\nHigh Availability Supports replication & partitioning for data redundancy.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Challenges of Schema-less NoSQL Models \\nChallenge Solution \\nLack of ACID Compliance Use eventual consistency models for distributed transactions. \\nComplex Queries Use indexing & caching (Redis, Elasticsearch) for optimization. \\nData Integrity Issues Implement application-layer validation to ensure data quality. \\n \\n7. Conclusion \\nSchema-less models in NoSQL databases provide unmatched flexibility, scalability, and \\nadaptability for modern applications. Whether handling real-time data, unstructured \\ninformation, or high-velocity transactions, schema-less NoSQL solutions like MongoDB, \\nCassandra, Redis, and Neo4j are transforming big data storage, AI systems, and cloud \\napplications.         \\n4.Describe about document store and key value store \\nprovide answer in 15 marks detail approach \\nChatGPT said: \\nDocument Store and Key-Value Store in NoSQL \\n1. Introduction to NoSQL Databases \\nNoSQL databases are designed to handle large-scale, unstructured, or semi-structured'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='ChatGPT said: \\nDocument Store and Key-Value Store in NoSQL \\n1. Introduction to NoSQL Databases \\nNoSQL databases are designed to handle large-scale, unstructured, or semi-structured \\ndata efficiently. Among the various types, Document Stores and Key-Value Stores play a \\ncrucial role in modern applications due to their scalability, flexibility, and high \\nperformance. \\n \\n2. Document Store \\nA. Definition \\nA document store is a NoSQL database that stores data in semi-structured formats such as \\nJSON, BSON, or XML. Each document contains a self-contained dataset that can have a \\nunique structure, making document stores highly flexible and schema-less. \\nB. Features of Document Stores \\n• Schema-flexibility: No predefined schema is required. \\n• Hierarchical Storage: Stores data as nested key-value pairs inside a document. \\n• Indexing & Querying: Allows complex queries using indexes on JSON fields.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Scalability: Supports horizontal scaling via sharding and replication. \\n• Embedded Relationships: Stores arrays and nested objects within documents. \\nC. Structure of a Document Store (Example in MongoDB) \\nA JSON-like document stored in MongoDB: \\njson \\nCopyEdit \\n{ \\n  \"_id\": \"101\", \\n  \"name\": \"Alice\", \\n  \"age\": 25, \\n  \"email\": \"alice@example.com\", \\n  \"orders\": [ \\n    { \"item\": \"Laptop\", \"price\": 1000 }, \\n    { \"item\": \"Mouse\", \"price\": 50 } \\n  ] \\n} \\n• Each document contains key-value pairs that store user information dynamically. \\n• Arrays (orders) allow storing multiple related objects inside a single document. \\nD. Examples of Document Store Databases \\nDatabase Description \\nMongoDB The most popular document store with JSON-like documents. \\nCouchDB Stores data in JSON format with RESTful APIs for querying. \\nRethinkDB Real-time document storage with continuous data updates. \\nFirebase Firestore A cloud-based NoSQL document database for mobile apps. \\nE. Use Cases of Document Stores'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='RethinkDB Real-time document storage with continuous data updates. \\nFirebase Firestore A cloud-based NoSQL document database for mobile apps. \\nE. Use Cases of Document Stores \\nIndustry Application \\nE-commerce Storing customer profiles, product catalogs dynamically. \\nSocial Media Storing user posts, comments, likes in a flexible format.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Industry Application \\nHealthcare Managing patient records, prescriptions, and reports. \\nIoT & AI Handling sensor data & AI-generated unstructured datasets. \\n \\n3. Key-Value Store \\nA. Definition \\nA key-value store is the simplest form of a NoSQL database where data is stored as a \\nunique key and an associated value. These databases are highly optimized for read/write \\nspeed, making them ideal for caching and real-time applications. \\nB. Features of Key-Value Stores \\n• Simple Data Storage: Stores data as key-value pairs (similar to a dictionary). \\n• High Performance: Fast retrieval using hash indexing. \\n• Scalability: Easily scalable by distributing keys across multiple nodes. \\n• Eventual Consistency: Supports distributed caching with minimal latency. \\n• Schema-less: The value can be strings, JSON, binary, or even entire objects. \\nC. Structure of a Key-Value Store (Example in Redis) \\nA key-value pair stored in Redis: \\nsql \\nCopyEdit'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Schema-less: The value can be strings, JSON, binary, or even entire objects. \\nC. Structure of a Key-Value Store (Example in Redis) \\nA key-value pair stored in Redis: \\nsql \\nCopyEdit \\nSET user_101 \"{ \\'name\\': \\'Alice\\', \\'age\\': 25, \\'email\\': \\'alice@example.com\\' }\" \\nGET user_101 \\n• The key (\"user_101\") is mapped to a JSON-like object containing user details. \\n• Unlike document stores, key-value databases lack built-in query mechanisms and \\nrely on key lookups. \\nD. Examples of Key-Value Store Databases \\nDatabase Description \\nRedis An in-memory key-value store used for caching and real-time processing. \\nDynamoDB AWS managed NoSQL database for highly available applications. \\nRiak KV A distributed key-value store optimized for availability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Database Description \\nBerkeley DB A lightweight embedded key-value database. \\nE. Use Cases of Key-Value Stores \\nIndustry Application \\nWeb Applications Session storage, authentication tokens (e.g., JWT, OAuth). \\nGaming Storing leaderboards, player stats, real-time game states. \\nE-commerce Caching product details, frequently accessed data. \\nIoT Storing real-time sensor readings, device configurations. \\n \\n4. Comparison: Document Store vs. Key-Value Store \\nFeature Document Store Key-Value Store \\nData Format JSON, BSON, XML Key-value pairs (String, Binary, \\nJSON) \\nSchema Schema-less (Flexible structure) Schema-less (No structure) \\nQuerying Supports complex queries on nested \\nfields Only key-based retrieval \\nPerformance Fast but slower than key-value stores Ultra-fast lookup operations \\nUse Case Semi-structured data (profiles, logs, \\nproducts) Caching, real-time applications \\n \\n5. Conclusion \\nBoth document stores and key-value stores serve critical roles in modern NoSQL \\ndatabases:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Use Case Semi-structured data (profiles, logs, \\nproducts) Caching, real-time applications \\n \\n5. Conclusion \\nBoth document stores and key-value stores serve critical roles in modern NoSQL \\ndatabases: \\n• Document Stores (MongoDB, CouchDB) offer schema-flexibility and powerful \\nqueries for dynamic applications like e-commerce and social media. \\n• Key-Value Stores (Redis, DynamoDB) provide blazing-fast read/write speeds for use \\ncases like caching, gaming, and real-time analytics. \\nUnderstanding their strengths allows businesses to choose the right database based on \\napplication needs, ensuring scalability, flexibility, and high performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Explain in detail about system architecture of Hbase \\nSystem Architecture of HBase \\n1. Introduction to HBase \\nHBase is a distributed, column-oriented NoSQL database built on top of Hadoop’s HDFS \\n(Hadoop Distributed File System). It is designed to handle large-scale, real-time data \\nprocessing with high availability and scalability. \\nKey Characteristics of HBase: \\n• Modeled after Google’s Bigtable \\n• Stores structured data in tables \\n• Supports random reads and writes \\n• Provides high throughput for real-time applications \\n• Runs on top of HDFS for fault tolerance \\n \\n2. HBase System Architecture \\nHBase follows a master-slave architecture with three main components: \\nA. HBase Master Server \\nThe HBase Master Server manages and coordinates the RegionServers in the cluster. \\nFunctions of HBase Master: \\n1. Assigns regions (data partitions) to RegionServers. \\n2. Monitors RegionServers for health checks and load balancing. \\n3. Handles schema changes (table creation, deletion, and updates).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Assigns regions (data partitions) to RegionServers. \\n2. Monitors RegionServers for health checks and load balancing. \\n3. Handles schema changes (table creation, deletion, and updates). \\n4. Manages splits and merges of regions dynamically. \\n5. Coordinates with ZooKeeper for failover and recovery. \\nB. RegionServers \\nRegionServers are responsible for handling read/write requests and managing data \\nstorage. \\nFunctions of RegionServers: \\n1. Stores actual data in HBase tables. \\n2. Handles client read/write requests. \\n3. Manages regions (subsets of tables) assigned by the Master.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Caches frequently accessed data for fast retrieval. \\n5. Flushes in-memory data to HDFS for durability. \\nEach RegionServer is responsible for multiple regions, and regions grow over time until \\nthey split into smaller ones. \\nC. ZooKeeper (Coordination Service) \\nHBase relies on ZooKeeper, a distributed coordination service, to manage cluster state. \\nFunctions of ZooKeeper in HBase: \\n1. Tracks active HBase Masters and RegionServers. \\n2. Manages leader election and failover. \\n3. Maintains metadata and configuration details. \\n4. Ensures synchronization between distributed components. \\n \\n3. Storage Architecture of HBase \\nHBase organizes data in a highly optimized format for high-speed reads and writes. The \\ncore storage architecture consists of: \\nA. Tables, Rows, and Column Families \\n• Data is stored in tables, consisting of rows and column families. \\n• Each row is identified by a unique row key. \\n• Column families group related columns together for better storage efficiency.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Data is stored in tables, consisting of rows and column families. \\n• Each row is identified by a unique row key. \\n• Column families group related columns together for better storage efficiency. \\nB. HFile (Storage on HDFS) \\nHBase stores data in HFiles on HDFS for durability and scalability. \\n• Data is first written to the Write-Ahead Log (WAL) to prevent data loss. \\n• Then, it is stored in MemStore (in-memory) before being flushed to HFiles on disk. \\nC. MemStore and HFile (Data Write Path) \\n1. Client writes data to MemStore (in-memory buffer). \\n2. Data is logged in Write-Ahead Log (WAL) to ensure durability. \\n3. When MemStore reaches a threshold, it flushes data to HFiles on HDFS. \\n4. Compaction merges smaller HFiles into larger ones to optimize read performance. \\nD. Data Read Path \\n1. Client sends a read request to the RegionServer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. RegionServer checks MemStore (if data is in-memory). \\n3. If not found, it searches in HFiles (on disk in HDFS). \\n4. Uses Bloom Filters and block cache to speed up reads. \\n \\n4. HBase Write and Read Operations \\nA. Write Operation (Insert/Update Data) \\n1. Client sends a write request to the appropriate RegionServer. \\n2. Data is first written to the Write-Ahead Log (WAL). \\n3. Data is then stored in MemStore (in-memory cache). \\n4. Once MemStore is full, data is flushed to disk (HFile on HDFS). \\n5. Periodic compaction merges HFiles to optimize storage. \\nB. Read Operation (Retrieve Data) \\n1. Client sends a read request to the RegionServer. \\n2. RegionServer first checks MemStore for recent data. \\n3. If data is not found, it checks BlockCache (cached data). \\n4. If still not found, it reads from HFiles on HDFS. \\n5. Data is returned to the client. \\n \\n5. HBase vs. Traditional RDBMS \\nFeature HBase (NoSQL) Traditional RDBMS \\nData Model Column-Oriented Row-Oriented'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Data is returned to the client. \\n \\n5. HBase vs. Traditional RDBMS \\nFeature HBase (NoSQL) Traditional RDBMS \\nData Model Column-Oriented Row-Oriented \\nSchema Schema-less Fixed Schema \\nScalability Horizontally Scalable Limited Scalability \\nData Storage Distributed (HDFS) Centralized \\nRead/Write Speed High for big data Slower for large datasets \\nUse Case Real-time Big Data Processing OLTP Transactions \\n \\n6. Use Cases of HBase'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 57, 'page_label': '58', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Industry Application \\nSocial Media Storing user activity logs, messages, and posts \\nFinance Managing real-time stock market transactions \\nE-commerce Handling product catalogs and customer interactions \\nHealthcare Storing patient records and medical history \\nIoT & AI Processing sensor data from connected devices \\n \\n7. Conclusion \\nHBase is a highly scalable, real-time, column-oriented NoSQL database built for big data \\napplications. Its master-slave architecture, distributed storage on HDFS, and integration \\nwith Hadoop make it a powerful choice for applications requiring fast reads, writes, and \\nhigh availability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='(SET 1 15 marks) \\n1. Customer wants you to develop the software for the remote controlled electronic toy. \\nCustomer is not sure of all requirements for the product. She is provided with initial set of \\nrequirement with which she wants to have feel of the product. She has informed that few \\nmore requirements will be provided later. Explain observations made in this scenario and \\nwhich model will be suitable for this requirement. \\nChatGPT said: \\nObservations in the Given Scenario: \\n1. Unclear and Evolving Requirements – The customer has only provided an initial set \\nof requirements and plans to introduce more in the future. This indicates that the \\nproject scope is not fully defined at the start. \\n2. Incremental Development Expected – Since the customer wants to have a \"feel\" of \\nthe product with the available requirements, an iterative approach would be \\nbeneficial to incorporate feedback and evolving needs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='the product with the available requirements, an iterative approach would be \\nbeneficial to incorporate feedback and evolving needs. \\n3. Customer Involvement is Key – The customer will be actively engaged, providing \\nnew requirements after evaluating initial versions of the software. \\n4. Need for Flexibility – A rigid development model (like the Waterfall model) may not \\nbe suitable since new requirements will emerge, necessitating adaptability. \\n5. Prototyping is Essential – The need for an early demonstration of the product \\nsuggests that an initial prototype or working version should be developed before \\nfinalizing the full requirements. \\nSuitable Software Development Model: Incremental Model \\nThe Incremental Model is the most suitable approach for this scenario because: \\n1. Iterative Delivery – The project can be broken into multiple increments. The first \\nincrement will be developed based on the initial requirements, and subsequent'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Iterative Delivery – The project can be broken into multiple increments. The first \\nincrement will be developed based on the initial requirements, and subsequent \\nincrements will incorporate new requirements as they are provided. \\n2. Customer Feedback Integration – Each increment allows the customer to test the \\nproduct, suggest modifications, and refine requirements, ensuring better alignment \\nwith their expectations. \\n3. Risk Reduction – Since the development happens in stages, issues can be identified \\nand fixed in earlier increments, reducing the risk of major failures later. \\n4. Faster Time-to-Market – A basic version of the software can be delivered early, \\nallowing the customer to interact with it before all features are implemented. \\n5. Flexibility to Accommodate Changes – New requirements can be easily incorporated \\nin upcoming increments without disrupting the entire project.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Alternative Model: Prototype Model \\nAnother possible approach is the Prototype Model, where an initial prototype is developed \\nbased on the given requirements. The customer can review it, suggest modifications, and \\nrefine the requirements before proceeding with the final development. However, if \\ncontinuous additions to the product are expected, the Incremental Model remains the \\nbetter choice. \\nConclusion \\nSince the customer is uncertain about all requirements and expects to refine them after \\ntesting an initial version, the Incremental Model is the most suitable approach. It ensures \\nearly delivery, flexibility, and continuous customer involvement, leading to a well-refined and \\ncustomer-centric final product. \\nObservations in the Given Scenario \\n1. Unclear and Evolving Requirements \\no The customer has only provided an initial set of requirements and has \\nexplicitly mentioned that more requirements will be given later.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Unclear and Evolving Requirements \\no The customer has only provided an initial set of requirements and has \\nexplicitly mentioned that more requirements will be given later. \\no This suggests that the customer does not have a complete vision of the final \\nproduct at the beginning of development. \\no The software development process must accommodate changes and new \\nrequirements as they emerge. \\n2. Customer Wants to Experience the Product Early \\no The customer wants to \"have a feel\" of the product before finalizing \\nadditional requirements. \\no This indicates the need for early software delivery so the customer can \\ninteract with it and provide feedback. \\no A development model that allows for early prototyping or incremental \\ndelivery is required. \\n3. Continuous Customer Involvement is Needed \\no Since the customer will be providing new requirements after testing the initial \\nversion, frequent interaction between the development team and the \\ncustomer is necessary.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o Since the customer will be providing new requirements after testing the initial \\nversion, frequent interaction between the development team and the \\ncustomer is necessary. \\no The development approach should support continuous collaboration, \\nfeedback, and modifications. \\n4. Risk of Major Changes in Requirements \\no Given that the requirements are incomplete, there is a possibility of \\nsignificant changes in design, functionality, or scope during development.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o A rigid model like the Waterfall Model would be unsuitable because it does \\nnot accommodate requirement changes once development has started. \\n5. Prototype or Working Version Needed \\no Since the customer wants to experience the product before finalizing all \\nrequirements, a development approach that supports prototyping or early-\\nstage working versions of the product is essential. \\n \\nMost Suitable Software Development Model: Incremental Model \\nThe Incremental Model is the best choice for this scenario due to its flexibility, iterative \\nnature, and ability to incorporate evolving requirements. \\nExplanation of the Incremental Model \\nThe Incremental Model is a software development approach where the system is built in \\nmultiple small parts (increments). Each increment adds additional features to the existing \\nsystem based on user feedback and newly defined requirements. \\nHow the Incremental Model Works in This Scenario \\n1. First Increment'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='system based on user feedback and newly defined requirements. \\nHow the Incremental Model Works in This Scenario \\n1. First Increment \\no The development team builds the first version of the software based on the \\ninitial set of requirements. \\no This provides the customer with an early version to test and give feedback. \\n2. Customer Feedback & New Requirements \\no After testing the first increment, the customer provides additional \\nrequirements and modifications. \\no The development team incorporates these in the next increment. \\n3. Subsequent Increments \\no Each new increment builds upon the previous version, integrating newly \\nidentified requirements. \\no The process continues until the final product is complete. \\nAdvantages of Using the Incremental Model in This Case \\n   Accommodates Changing Requirements \\n• Since the customer has evolving needs, the incremental model allows new \\nrequirements to be added in later stages. \\n   Early Product Delivery'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"• A functional part of the software is delivered early, enabling the customer to interact \\nwith it and refine their expectations. \\n   Better Risk Management \\n• The project is developed in smaller parts, reducing risks compared to building the \\nentire system at once. \\n   Customer Involvement Throughout Development \\n• The customer actively participates in each increment, ensuring that the final product \\naligns with their expectations. \\n   Faster Identification and Resolution of Issues \\n• Bugs and design flaws can be detected early in the development process, reducing \\noverall project risks. \\n \\nAlternative Approach: Prototype Model \\nIf the customer is unsure about the product's functionality and wants an early \\ndemonstration before development starts, the Prototype Model could also be considered. \\nHow the Prototype Model Works \\n1. A quick prototype (basic working model) is created using initial requirements. \\n2. The customer reviews the prototype and suggests modifications.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='How the Prototype Model Works \\n1. A quick prototype (basic working model) is created using initial requirements. \\n2. The customer reviews the prototype and suggests modifications. \\n3. The prototype is refined until the customer is satisfied. \\n4. Once finalized, the actual software development begins based on the refined \\nprototype. \\n   Suitable if the customer is unclear about core functionalities. \\n   Helps in requirement gathering and validation before full-scale development. \\n   Not ideal for continuous requirement changes. \\nSince the customer in this case will be providing additional requirements throughout the \\ndevelopment, the Incremental Model remains the best approach over the Prototype Model. \\n \\nConclusion \\nFor a remote-controlled electronic toy with evolving requirements, the Incremental Model is \\nthe best choice. It enables: \\n• Early delivery of working software, \\n• Continuous customer involvement,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"• Integration of new requirements at later stages, \\n• Better risk management, \\n• Higher flexibility and adaptability to changes. \\nThus, the Incremental Model ensures a structured yet flexible development approach, \\ndelivering a product that meets the customer's evolving expectations efficiently.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='SET2(15 marks) \\n1. Customer has provided you with all the requirements and also assured there will not be \\nany change in the requirement. He expects deliverables at every stage of development. You \\nhave carried out the similar project earlier and you are sure that the project could be \\nexecuted systemically. Explain observations made in this scenario and which model will be \\nsuitable for this requirement.  \\nChatGPT said: \\nObservations in the Given Scenario \\n1. Clearly Defined and Fixed Requirements \\no The customer has provided a complete and well-defined set of requirements. \\no There will be no changes in requirements throughout the project lifecycle. \\no This eliminates the need for an iterative or adaptive model that \\naccommodates changes. \\n2. Expectation of Deliverables at Every Stage \\no The customer expects progress to be demonstrated at each stage of \\ndevelopment. \\no This suggests a need for a structured development approach with clearly'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o The customer expects progress to be demonstrated at each stage of \\ndevelopment. \\no This suggests a need for a structured development approach with clearly \\ndefined phases and deliverables at each stage. \\n3. Previous Experience with Similar Projects \\no The development team has already worked on a similar project before. \\no This indicates that the team has prior knowledge, making it easier to estimate \\ntimelines and execute the project efficiently. \\no There is low uncertainty, reducing the need for prototyping or iterative \\nrefinement. \\n4. Systematic Execution is Feasible \\no Since the project requirements are well-defined and the team is experienced, \\na well-structured development process can be followed without disruptions. \\no There is no need for frequent customer interaction beyond the predefined \\nmilestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario due to its structured, phase-wise'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='milestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario due to its structured, phase-wise \\napproach that aligns well with the given conditions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Explanation of the Waterfall Model \\nThe Waterfall Model is a linear, sequential approach where each phase is completed before \\nmoving on to the next. It follows a strict order, ensuring that deliverables are provided at the \\nend of each stage. \\nPhases of the Waterfall Model and How They Apply to This Project \\n1. Requirement Gathering and Analysis \\no Since the requirements are already fully defined and fixed, this phase will be \\nstraightforward. \\no The development team will document all requirements to ensure clarity. \\n2. System Design \\no Based on the requirements, the system architecture and design will be \\ncreated. \\no This includes database design, system interfaces, and technical specifications. \\no A deliverable (design document) can be provided to the customer at this \\nstage. \\n3. Implementation (Coding) \\no The actual development (coding) takes place based on the finalized design. \\no Modules are developed and integrated systematically.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='stage. \\n3. Implementation (Coding) \\no The actual development (coding) takes place based on the finalized design. \\no Modules are developed and integrated systematically. \\no Deliverables can include partially completed modules or milestone updates. \\n4. Testing \\no The entire system undergoes rigorous testing after implementation. \\no Unit testing, integration testing, and system testing ensure that the software \\nmeets all requirements. \\no The customer can receive a test report as a deliverable at this stage. \\n5. Deployment \\no The software is deployed for customer use. \\no Since no changes are expected, this phase will be straightforward. \\n6. Maintenance \\no Although the requirements are fixed, maintenance may still be required for \\nbug fixes and optimizations. \\no The customer will receive regular updates regarding software stability and \\nperformance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Why the Waterfall Model is the Best Choice \\n   Well-Suited for Fixed Requirements \\n• Since the customer has clearly defined requirements and no changes are expected, \\nthe Waterfall Model ensures structured execution without disruptions. \\n   Systematic Execution \\n• Each phase follows a strict order, ensuring that deliverables are provided at every \\nstage as per the customer’s expectations. \\n   Predictability and Efficiency \\n• The development team has prior experience with similar projects, making \\nestimations more accurate and execution more efficient. \\n   Minimizes Risk \\n• Since all requirements are known from the beginning, risks associated with \\nrequirement changes or scope creep are eliminated. \\n   Clear Customer Expectations \\n• The customer knows what to expect at each stage, as the Waterfall Model provides \\nformal documentation and structured progress updates. \\n \\nAlternative Approach: V-Model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='• The customer knows what to expect at each stage, as the Waterfall Model provides \\nformal documentation and structured progress updates. \\n \\nAlternative Approach: V-Model \\nIf the project requires early validation and continuous testing at every stage, the V-Model \\n(Validation & Verification Model) could also be considered. \\nHow the V-Model Works \\n• It follows the same structured approach as the Waterfall Model but integrates testing \\nat every stage. \\n• Each development phase has a corresponding testing phase, ensuring issues are \\ndetected early. \\n   Ensures high-quality software through continuous testing. \\n   Minimizes post-development defects. \\n   Not necessary if testing at the final stage is sufficient. \\nSince the scenario does not specify a need for early-stage validation, the Waterfall Model \\nremains the best choice for this project.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Conclusion \\nFor a project with fixed requirements, systematic execution, and predefined stage-wise \\ndeliverables, the Waterfall Model is the most suitable choice. It ensures: \\n• A structured development process, \\n• Predictable timelines and outcomes, \\n• Formal deliverables at every stage, \\n• Minimal risk due to well-defined requirements. \\nThus, the Waterfall Model provides a clear and efficient approach, ensuring successful \\nproject completion with minimal complications. \\n \\n                                                                        [OR] \\nObservations in the Given Scenario \\n1. Well-Defined and Fixed Requirements \\no The customer has provided a complete and unchanging set of requirements. \\no This means there is no need for continuous requirement gathering or \\niterative refinements. \\no There is little to no risk of scope creep (where additional features are \\nrequested mid-development). \\n2. Expectation of Deliverables at Every Stage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='iterative refinements. \\no There is little to no risk of scope creep (where additional features are \\nrequested mid-development). \\n2. Expectation of Deliverables at Every Stage \\no The customer wants structured updates and deliverables at each stage. \\no This suggests the need for a formal, well-documented approach where each \\nphase produces a tangible output before moving to the next. \\n3. Previous Experience with Similar Projects \\no The development team has already successfully executed a similar project. \\no This indicates that: \\n▪ The project’s complexity is understood. \\n▪ Development timelines and risks can be estimated accurately. \\n▪ There is no need for a prototype or experimental approach. \\n4. Systematic Execution is Feasible \\no Since the requirements are fixed and the development team is experienced, a \\nstructured and disciplined execution approach will work best.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o There is no need for high customer involvement beyond predefined \\nmilestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario because it follows a structured, \\nlinear, and sequential approach to development. \\n \\nDetailed Explanation of the Waterfall Model \\nThe Waterfall Model follows a step-by-step execution process, where each phase is \\ncompleted before moving to the next. It is best suited when the requirements are fixed, \\ndeliverables are expected at each stage, and the project can be executed systematically. \\nPhases of the Waterfall Model and Their Relevance to This Project \\n1. Requirement Analysis Phase \\no Since the customer has already provided a complete and unchanging set of \\nrequirements, this phase will focus on: \\n▪ Documenting the requirements formally in a Software Requirement \\nSpecification (SRS) document. \\n▪ Reviewing the requirements with the customer to ensure there are no \\ngaps.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='▪ Documenting the requirements formally in a Software Requirement \\nSpecification (SRS) document. \\n▪ Reviewing the requirements with the customer to ensure there are no \\ngaps. \\n▪ Identifying constraints and dependencies early on. \\no    Deliverable: Finalized SRS document approved by the customer. \\n2. System Design Phase \\no In this phase, the system’s architecture is designed based on the SRS. \\no It involves: \\n▪ High-level design (HLD): Defines system architecture, modules, and \\ncomponents. \\n▪ Low-level design (LLD): Specifies detailed logic for each module. \\n▪ Database design: Designing data storage and access mechanisms. \\no    Deliverable: System Design Document (SDD) with diagrams, workflows, \\nand technical specifications. \\n3. Implementation (Coding) Phase'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o The actual development (coding) of the system begins based on the approved \\ndesign. \\no Since the team has prior experience with similar projects, coding can be \\ndone efficiently with minimal risk. \\no The development is modular, meaning individual components can be tested \\nbefore integration. \\no    Deliverable: Working software modules or intermediate builds at \\npredefined milestones. \\n4. Integration and Testing Phase \\no After coding, the system is assembled and tested to ensure it meets the \\nspecified requirements. \\no Testing involves: \\n▪ Unit testing: Testing individual modules for correctness. \\n▪ Integration testing: Ensuring different modules work together. \\n▪ System testing: Verifying overall functionality and performance. \\n▪ Acceptance testing: Ensuring the system meets customer \\nexpectations. \\no Since the requirements are fixed, testing is straightforward, focusing on \\nverification rather than exploratory changes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='▪ Acceptance testing: Ensuring the system meets customer \\nexpectations. \\no Since the requirements are fixed, testing is straightforward, focusing on \\nverification rather than exploratory changes. \\no    Deliverable: Test reports and a fully tested software system. \\n5. Deployment Phase \\no The system is delivered to the customer and deployed in the intended \\nenvironment. \\no This phase includes: \\n▪ Installation and configuration on customer infrastructure. \\n▪ User training (if required). \\n▪ Final acceptance testing by the customer. \\no    Deliverable: Fully operational software deployed at the customer site. \\n6. Maintenance Phase \\no While the requirements will not change, bug fixes, performance \\noptimization, and minor updates may be required. \\no The team provides support and maintenance as per the agreement.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o    Deliverable: Software updates and maintenance reports. \\n \\nWhy the Waterfall Model is the Best Choice for This Project \\n   Well-Suited for Fixed Requirements \\n• Since the customer has already provided a complete and unchanging set of \\nrequirements, the Waterfall Model ensures that the project is executed \\nsystematically with no need for requirement adjustments. \\n   Structured Development Process \\n• The Waterfall Model follows a step-by-step approach where each phase is \\ncompleted before moving to the next. This matches the customer’s expectation of \\ndeliverables at every stage. \\n   Predictability and Efficiency \\n• The development team has prior experience with similar projects, making it easier \\nto estimate timelines, costs, and risks. \\n• Since the process is structured, project tracking is straightforward and transparent. \\n   Minimizes Risks and Errors \\n• Since testing is conducted only after implementation, defects are detected before \\ndeployment.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Minimizes Risks and Errors \\n• Since testing is conducted only after implementation, defects are detected before \\ndeployment. \\n• The project follows a systematic verification process, ensuring a high-quality final \\nproduct. \\n   Clear Customer Expectations and Milestones \\n• The customer knows exactly what to expect at each stage since the Waterfall Model \\nhas predefined phases and deliverables. \\n• There is no need for continuous feedback and iteration, reducing delays and scope \\ncreep. \\n \\nAlternative Approach: V-Model (Verification & Validation Model) \\nIf testing at every phase is a major priority, the V-Model could be considered. It is an \\nenhanced version of the Waterfall Model where each development phase has a \\ncorresponding testing phase. \\nKey Features of the V-Model: \\n• Follows the same structured approach as the Waterfall Model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='• Testing is performed after every phase rather than waiting until the end. \\n• Ensures early defect detection and improves software quality. \\n   Better for projects where high reliability is required. \\n   Reduces cost of fixing bugs by identifying them early. \\n   Not necessary if post-development testing is sufficient. \\nSince the scenario does not emphasize the need for early-stage validation, the Waterfall \\nModel remains the best choice. \\n \\nConclusion \\nFor a project where all requirements are fixed, deliverables are expected at each stage, and \\nsystematic execution is feasible, the Waterfall Model is the most appropriate choice. It \\nensures: \\n  Structured execution with clearly defined phases \\n  Predictable timelines and outcomes \\n  Minimal risk due to fixed requirements \\n  Efficient project tracking and milestone-based deliverables \\nThus, the Waterfall Model provides a well-organized and disciplined development'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Minimal risk due to fixed requirements \\n  Efficient project tracking and milestone-based deliverables \\nThus, the Waterfall Model provides a well-organized and disciplined development \\napproach, ensuring successful project completion while meeting customer expectations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"SET 1(7 & 8 marks): \\n(8 marks): \\nApply Bayesian Classification method with an example for class based modeling. provide \\ndetailed answer \\nChatGPT said: \\nApplying Bayesian Classification Method for Class-Based Modeling \\n1. Introduction to Bayesian Classification \\nBayesian Classification is based on Bayes' Theorem, which provides a probabilistic approach \\nto classification. It is widely used in machine learning, spam detection, medical diagnosis, \\nand text classification. \\nBayes' Theorem Formula \\n \\n2. Class-Based Modeling Using Bayesian Classification \\nClass-based modeling defines a set of classes representing different objects in a system, \\nwhere Bayesian classification helps predict the class of a new instance. \\nExample: Email Spam Detection (Class-Based Model with Bayesian Classification) \\nLet's consider a binary classification problem, where we classify an email as Spam (S) or Not \\nSpam (NS). \\nStep 1: Define Classes \\n• C1=C_1 =C1= Spam \\n• C2=C_2 =C2= Not Spam\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"Let's consider a binary classification problem, where we classify an email as Spam (S) or Not \\nSpam (NS). \\nStep 1: Define Classes \\n• C1=C_1 =C1= Spam \\n• C2=C_2 =C2= Not Spam \\nStep 2: Collect Training Data (Feature Extraction)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Assume we have a dataset containing words like \"free\", \"win\", \"offer\", \"urgent\", which are \\ncommon in spam emails. The dataset looks like: \\n \\n \\n \\n3. Applications of Bayesian Classification in Class-Based Modeling'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Medical Diagnosis → Classifying diseases based on symptoms. \\n2. Customer Segmentation → Predicting if a customer belongs to a premium or regular \\nclass. \\n3. Fraud Detection → Identifying fraudulent transactions in banking. \\n4. Sentiment Analysis → Classifying reviews as positive or negative. \\n \\n4. Conclusion \\nBayesian Classification provides a probabilistic and interpretable approach to class-based \\nmodeling. It is effective for spam filtering, medical diagnostics, and fraud detection, as \\ndemonstrated in the email classification example.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='(7 marks): \\n(i) Build the various architectural styles and design for software design process development.  \\nChatGPT said: \\nArchitectural Styles and Design for Software Development \\nSoftware architecture defines the structure, behavior, and interaction of software \\ncomponents. Different architectural styles are used based on system requirements like \\nscalability, maintainability, and performance. \\n \\n1. Common Architectural Styles \\n1.1 Layered Architecture \\n  Description: Organizes software into multiple layers where each layer performs a specific \\nfunction. \\n  Best for: Large enterprise applications, web applications, and software with clear \\nseparation of concerns. \\nExample: Web Application Architecture \\nLayers: \\n1. Presentation Layer (UI): Handles user interactions. \\n2. Business Logic Layer: Processes business rules. \\n3. Data Access Layer: Manages database operations. \\n4. Database Layer: Stores and retrieves data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='2. Business Logic Layer: Processes business rules. \\n3. Data Access Layer: Manages database operations. \\n4. Database Layer: Stores and retrieves data. \\n     Example Technologies: React (UI), Node.js (Business Logic), MySQL (Database). \\n \\n1.2 Client-Server Architecture \\n  Description: Divides system into client (user interface) and server (backend processing). \\n  Best for: Web applications, online banking, cloud services. \\nExample: Online Banking System \\n1. Client: Web/Mobile app sending requests. \\n2. Server: Processes transactions, verifies data. \\n3. Database: Stores user data securely. \\n     Example Technologies: JavaScript (Client), Spring Boot (Server), PostgreSQL (Database).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1.3 Microservices Architecture \\n  Description: Breaks software into independent services that communicate via APIs. \\n  Best for: Scalable cloud applications, e-commerce, streaming platforms. \\nExample: E-commerce Application \\n• User Service: Handles authentication. \\n• Product Service: Manages product catalog. \\n• Payment Service: Handles transactions. \\n     Example Technologies: Docker, Kubernetes, REST APIs, AWS. \\n \\n1.4 Event-Driven Architecture \\n  Description: Components communicate through events, ensuring asynchronous \\nprocessing. \\n  Best for: Real-time applications, IoT, stock market systems. \\nExample: IoT Smart Home System \\n1. Sensors detect motion. \\n2. Event is published to an event queue. \\n3. Security system triggers an alarm. \\n     Example Technologies: Kafka, RabbitMQ, MQTT. \\n \\n1.5 Model-View-Controller (MVC) Architecture \\n  Description: Separates application logic into Model, View, and Controller. \\n  Best for: Web applications, GUI-based applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1.5 Model-View-Controller (MVC) Architecture \\n  Description: Separates application logic into Model, View, and Controller. \\n  Best for: Web applications, GUI-based applications. \\nExample: Web Framework (Django, Laravel, ASP .NET MVC) \\n1. Model: Manages data (Database). \\n2. View: Displays information (UI). \\n3. Controller: Handles requests (Logic). \\n     Example Technologies: Django (Python), Laravel (PHP), Angular. \\n \\n1.6 Pipe-and-Filter Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Description: Data flows through filters (processing units) connected by pipes (data \\nstream). \\n  Best for: Data processing systems, AI/ML pipelines. \\nExample: Image Processing System \\n1. Filter 1: Read image file. \\n2. Filter 2: Apply color correction. \\n3. Filter 3: Save processed image. \\n     Example Technologies: Apache Spark, TensorFlow pipelines. \\n \\n1.7 Service-Oriented Architecture (SOA) \\n  Description: Uses reusable services that communicate via SOAP or REST. \\n  Best for: Enterprise applications, ERP systems. \\nExample: Airline Reservation System \\n• Flight Booking Service \\n• Payment Processing Service \\n• Notification Service \\n     Example Technologies: SOAP , XML, RESTful APIs. \\n \\nConclusion \\nChoosing the right architectural style ensures software is scalable, maintainable, and \\nefficient. Layered, Microservices, Event-Driven, and MVC are some of the most commonly \\nused architectures depending on business needs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='1.Five Phases of Product Lifecycle Development – Detailed Explanation \\nProduct Lifecycle Development (PLM) is a strategic approach that ensures a product evolves \\nefficiently from its initial concept to its end-of-life phase. This structured process helps organizations \\nstreamline product development, optimize resources, reduce costs, and improve overall product \\nquality. \\n \\n1. Concept & Ideation Phase (Idea Generation & Feasibility) \\nObjective: \\nTo generate and evaluate innovative product ideas based on market needs, technological feasibility, \\nand business goals. \\nKey Activities: \\n1. Market Research & Consumer Insights: \\no Identifying industry trends and market gaps. \\no Conducting customer surveys, focus groups, and competitor analysis. \\no Understanding pain points and unmet needs. \\n2. Brainstorming & Innovation: \\no Encouraging idea generation within R&D teams. \\no Utilizing design thinking methodologies for creative solutions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Understanding pain points and unmet needs. \\n2. Brainstorming & Innovation: \\no Encouraging idea generation within R&D teams. \\no Utilizing design thinking methodologies for creative solutions. \\no Engaging stakeholders (marketing, sales, and engineering) to validate concepts. \\n3. Technical & Economic Feasibility Study: \\no Assessing the technological feasibility of the product. \\no Performing cost analysis, return on investment (ROI), and risk assessments. \\no Evaluating sustainability and regulatory compliance. \\n4. Product Requirement Definition: \\no Drafting high-level functional and technical requirements. \\no Setting initial specifications, including size, performance, and materials. \\no Identifying potential constraints (legal, environmental, and financial). \\nOutcome: \\n• Clear Product Concept Document (PCD) with specifications. \\n• Feasibility reports and go/no-go decision for product development. \\n \\n2. Design & Development Phase (Turning the Idea into a Prototype)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Objective: \\nTo transform the concept into a tangible product through detailed design, prototyping, and testing. \\nKey Activities: \\n1. Engineering & Product Design: \\no Creating Computer-Aided Design (CAD) models for visualization. \\no Selecting materials based on durability, cost, and sustainability. \\no Conducting stress tests, fluid dynamics simulations, and thermal analysis. \\n2. Prototype Development & Testing: \\no Building physical or digital prototypes for testing. \\no Running pilot tests to validate functionality, safety, and efficiency. \\no Gathering user feedback for improvements. \\n3. Software & Electronics Development (If Applicable): \\no Developing embedded software for IoT-enabled or digital products. \\no Ensuring cybersecurity measures and firmware updates. \\n4. Bill of Materials (BOM) & Supply Chain Planning: \\no Creating a multi-level BOM to define required parts and materials. \\no Identifying suppliers and manufacturers. \\n5. Regulatory & Compliance Checks:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Creating a multi-level BOM to define required parts and materials. \\no Identifying suppliers and manufacturers. \\n5. Regulatory & Compliance Checks: \\no Ensuring adherence to industry safety standards. \\no Getting necessary certifications (e.g., ISO, CE, FDA approvals). \\nOutcome: \\n• Fully functional prototype ready for validation. \\n• Finalized engineering documentation (BOM, design blueprints, regulatory reports). \\n \\n3. Manufacturing & Production Phase (Scaling Up for Market Readiness) \\nObjective: \\nTo transition from prototype to large-scale production while optimizing quality, efficiency, and cost. \\nKey Activities: \\n1. Production Process Planning: \\no Determining manufacturing methods (e.g., 3D printing, injection molding, CNC \\nmachining).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Optimizing production workflow for minimal waste. \\n2. Factory Setup & Automation: \\no Implementing smart manufacturing technologies such as IoT and AI-driven \\nautomation. \\no Establishing quality control stations and in-line testing. \\n3. Supply Chain & Logistics Optimization: \\no Coordinating with suppliers to ensure material availability. \\no Managing inventory using Enterprise Resource Planning (ERP) systems. \\n4. Quality Control & Testing: \\no Running stress, endurance, and safety tests on mass-produced units. \\no Conducting failure analysis and corrective actions. \\n5. Packaging & Distribution Readiness: \\no Designing sustainable packaging solutions to minimize waste. \\no Establishing distribution networks for global reach. \\nOutcome: \\n• Fully manufactured product meeting all safety, compliance, and market requirements. \\n• Scalability plan for mass production. \\n \\n4. Launch & Market Deployment Phase (Bringing the Product to Customers) \\nObjective:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• Scalability plan for mass production. \\n \\n4. Launch & Market Deployment Phase (Bringing the Product to Customers) \\nObjective: \\nTo introduce the product to the market through strategic sales, marketing, and distribution efforts. \\nKey Activities: \\n1. Go-To-Market (GTM) Strategy: \\no Defining pricing strategies (premium, penetration, skimming). \\no Selecting appropriate sales channels (e-commerce, retail, B2B partnerships). \\n2. Marketing & Branding: \\no Developing advertising campaigns (digital, social media, traditional). \\no Conducting product launch events and demos. \\n3. Customer Support & Service Preparation: \\no Setting up after-sales support teams for issue resolution. \\no Creating training materials for customers and retailers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='4. Feedback Collection & Market Adaptation: \\no Monitoring customer feedback via online reviews and surveys. \\no Adjusting marketing strategies based on sales performance. \\nOutcome: \\n• Successful market penetration and brand awareness. \\n• Sales data for performance evaluation. \\n \\n5. Service, Maintenance & End-of-Life (EOL) Phase (Sustaining & Retiring the Product) \\nObjective: \\nTo maintain product quality, enhance customer experience, and plan for its eventual phase-out. \\nKey Activities: \\n1. Customer Support & Warranty Management: \\no Providing help desks, chatbots, and service centers. \\no Handling repairs, replacements, and upgrades. \\n2. Software Updates & Security Patches (For Digital Products): \\no Rolling out firmware updates to fix vulnerabilities. \\no Ensuring compatibility with evolving technologies. \\n3. Product Enhancement & Redesign (If Applicable): \\no Gathering user feedback for next-gen product development. \\no Addressing common issues for an improved version.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='3. Product Enhancement & Redesign (If Applicable): \\no Gathering user feedback for next-gen product development. \\no Addressing common issues for an improved version. \\n4. End-of-Life (EOL) Planning & Sustainable Disposal: \\no Phasing out obsolete products responsibly. \\no Implementing circular economy strategies (recycling, repurposing). \\nOutcome: \\n• Extended product lifespan through continued support and updates. \\n• Responsible retirement or next-generation product planning. \\n \\nConclusion \\nThe five-phase PLM approach ensures products are innovative, efficiently developed, successfully \\nlaunched, and well-maintained. By leveraging advanced technologies like AI, IoT, and digital twins, \\ncompanies can optimize each phase, improve customer satisfaction, and drive long-term profitability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='7. Explain the steps for implementing a PLM strategy i want the answer for 15 marks so provide a \\ndetailed answer \\nChatGPT said: \\nSteps for Implementing a Product Lifecycle Management (PLM) Strategy \\nImplementing a PLM strategy is a complex yet critical process that enables businesses to manage a \\nproduct’s lifecycle from ideation to disposal. A well-structured PLM strategy enhances collaboration, \\nefficiency, innovation, and cost-effectiveness across the product’s journey. Below are the key steps \\nfor implementing a PLM strategy, explained in detail. \\n \\n1. Define Clear Business Objectives & Goals \\nBefore implementing PLM, organizations must clearly define their strategic objectives. These goals \\nshould align with the company’s overall business strategy. \\nKey Considerations: \\n• What problems will PLM solve? \\no Reduce product development time? \\no Improve collaboration? \\no Ensure regulatory compliance? \\n• Set Measurable Goals: \\no Reduce time-to-market by 20%.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• What problems will PLM solve? \\no Reduce product development time? \\no Improve collaboration? \\no Ensure regulatory compliance? \\n• Set Measurable Goals: \\no Reduce time-to-market by 20%. \\no Improve design efficiency by 30%. \\no Increase product reuse by 40%.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA car manufacturer wants to reduce design errors and improve collaboration between global teams \\nby implementing a PLM system. \\n \\n2. Conduct a Current State Assessment & Gap Analysis \\nA thorough assessment of the current product development process helps identify existing \\nchallenges, inefficiencies, and areas for improvement. \\nKey Considerations: \\n• Analyze Existing Processes & Tools: \\no How is product data managed today? \\no Are teams using spreadsheets, emails, or outdated software? \\n• Identify Gaps & Challenges: \\no Lack of real-time collaboration between teams. \\no Difficulty in tracking design changes and approvals. \\nExample: \\nAn electronics company finds that using manual document control for product data causes frequent \\nversion mismatches and production errors. \\n \\n3. Select the Right PLM System \\nChoosing the right PLM software is crucial for effective implementation. The selection should be \\nbased on the company’s size, industry, complexity, and long-term goals.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Choosing the right PLM software is crucial for effective implementation. The selection should be \\nbased on the company’s size, industry, complexity, and long-term goals. \\nKey Considerations: \\n• PLM Features to Look for: \\no Cloud-based or on-premise? \\no Integration with ERP , CAD, and other systems? \\no User-friendly interface and accessibility? \\n• Popular PLM Solutions: \\no Siemens Teamcenter \\no PTC Windchill \\no Dassault Systèmes ENOVIA \\no Autodesk Fusion Lifecycle \\nExample:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='A medical device company selects PTC Windchill because it ensures FDA compliance and \\ntraceability for medical regulations. \\n \\n4. Develop a PLM Implementation Roadmap \\nA step-by-step implementation plan ensures smooth adoption of PLM without disrupting existing \\noperations. \\nKey Considerations: \\n• Phased Implementation vs. Full Rollout: \\no Start with a pilot program before enterprise-wide rollout. \\no Prioritize critical departments (e.g., R&D, Engineering). \\n• Implementation Timeline: \\no Phase 1: System Setup & Data Migration (3 months) \\no Phase 2: Integration with CAD & ERP (6 months) \\no Phase 3: Enterprise-wide Adoption (12 months) \\nExample: \\nAn aerospace company first implements PLM in design and engineering, then gradually expands it \\nto manufacturing and supply chain management. \\n \\n5. Migrate & Standardize Product Data \\nPLM requires clean, structured, and standardized product data for efficiency. This step involves data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='to manufacturing and supply chain management. \\n \\n5. Migrate & Standardize Product Data \\nPLM requires clean, structured, and standardized product data for efficiency. This step involves data \\nmigration from legacy systems and ensuring a single source of truth for product information. \\nKey Considerations: \\n• Identify Critical Data to Migrate: \\no CAD files, BOM (Bill of Materials), compliance records. \\no Supplier information and past project documentation. \\n• Data Standardization: \\no Use common naming conventions and classification systems. \\no Ensure version control to avoid duplication errors. \\nExample: \\nA smartphone manufacturer standardizes its BOM structure in PLM to improve supplier \\ncoordination. \\n \\n6. Integrate PLM with Existing Enterprise Systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='PLM works best when integrated with other business-critical systems like ERP (Enterprise Resource \\nPlanning), CRM (Customer Relationship Management), and MES (Manufacturing Execution \\nSystems). \\nKey Considerations: \\n• Essential Integrations: \\no PLM + ERP: Ensures smooth manufacturing & procurement. \\no PLM + CAD: Seamless design data management. \\no PLM + CRM: Tracks customer feedback for product improvements. \\n• API & Data Flow Management: \\no Ensure real-time synchronization between PLM and other systems. \\nExample: \\nA fashion brand integrates PLM with ERP to manage raw materials, supplier orders, and production \\nschedules. \\n \\n7. Define & Automate Workflow Management \\nPLM improves efficiency by automating workflows such as design approvals, change requests, and \\ncompliance checks. \\nKey Considerations: \\n• Workflow Automation Examples: \\no Automated approval process for design changes. \\no Automatic alerts for compliance violations. \\n• Role-Based Access Control:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Key Considerations: \\n• Workflow Automation Examples: \\no Automated approval process for design changes. \\no Automatic alerts for compliance violations. \\n• Role-Based Access Control: \\no Ensure employees can access only relevant product data. \\nExample: \\nA pharmaceutical company automates FDA documentation approval workflows in its PLM system. \\n \\n8. Train Employees & Change Management \\nEmployees must understand and adapt to the new PLM system. Training and support are critical for \\nsuccessful adoption. \\nKey Considerations: \\n• Employee Training: \\no Conduct PLM workshops and hands-on practice sessions. \\no Provide user manuals and on-demand video tutorials.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• Change Management Strategies: \\no Address employee resistance by demonstrating PLM benefits. \\no Use pilot programs before full-scale implementation. \\nExample: \\nA car manufacturer trains engineers, supply chain managers, and designers to effectively use PLM \\nfor collaboration. \\n \\n9. Monitor Performance & Continuous Improvement \\nPost-implementation, companies must monitor PLM system performance and make improvements \\nbased on real-time data. \\nKey Considerations: \\n• Key PLM Performance Metrics: \\no Time-to-market reduction. \\no Number of design change requests. \\no Cost savings from improved efficiency. \\n• Continuous Improvement Approach: \\no Regular feedback from employees. \\no System updates and feature enhancements. \\nExample: \\nA consumer electronics company monitors PLM efficiency and adjusts workflows to further optimize \\nproduct development cycles. \\n \\nConclusion: Why a Structured PLM Strategy is Essential'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA consumer electronics company monitors PLM efficiency and adjusts workflows to further optimize \\nproduct development cycles. \\n \\nConclusion: Why a Structured PLM Strategy is Essential \\nA well-planned PLM strategy streamlines product development, collaboration, cost efficiency, and \\ninnovation. Companies that implement PLM effectively benefit from: \\n   Reduced time-to-market (faster product launches). \\n   Better data accuracy & version control. \\n   Improved supply chain coordination. \\n   Seamless compliance & regulatory management. \\nBy following these nine steps, organizations can achieve sustainable product innovation and market \\nleadership.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='12,4. Importance of Process Consistency in \\nProduct Lifecycle Management (PLM) \\nIntroduction \\nProcess consistency is a fundamental pillar of effective Product Lifecycle Management (PLM). It \\nensures standardized workflows, minimizes errors, enhances collaboration, and improves overall \\nefficiency in product development. PLM integrates multiple processes across design, manufacturing, \\nsupply chain, and customer service, making consistency crucial for seamless operations. \\nIn this detailed explanation, we will explore why process consistency is important and how it \\ncontributes to an organization’s efficiency, compliance, quality, and innovation. \\n \\n1. Enhances Efficiency & Reduces Operational Bottlenecks \\nWhy It Matters? \\nInconsistent processes lead to delays, confusion, and inefficiencies in product development. A lack \\nof uniformity in workflows causes teams to waste time resolving discrepancies, redoing tasks, and \\nhandling miscommunications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='of uniformity in workflows causes teams to waste time resolving discrepancies, redoing tasks, and \\nhandling miscommunications. \\nBenefits of Process Consistency in Efficiency: \\n   Streamlined Workflows: Standardized procedures enable teams to follow clear steps for design, \\napprovals, and production. \\n   Faster Product Development: Consistent workflows minimize time spent on unnecessary \\nrevisions or approvals. \\n   Optimized Resource Utilization: Teams can focus on innovation rather than fixing process-\\nrelated issues.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA smartphone manufacturer follows a standardized change request approval process, reducing \\ndesign iteration delays and accelerating time-to-market. \\n \\n2. Ensures Compliance with Industry Standards & Regulations \\nWhy It Matters? \\nMany industries, such as automotive, aerospace, healthcare, and electronics, have strict regulatory \\nrequirements (ISO, FDA, CE, etc.). Without process consistency, tracking compliance documents and \\napprovals becomes chaotic. \\nBenefits of Process Consistency in Compliance: \\n   Traceability & Audit Readiness: Ensures all design changes, approvals, and certifications are \\nwell-documented. \\n   Reduces Risk of Regulatory Penalties: Prevents non-compliance issues that could lead to legal \\nactions or product recalls. \\n   Simplifies Certification Processes: Structured processes enable faster certification and approval \\nfrom regulatory bodies. \\n        Example: \\nA pharmaceutical company maintains a consistent documentation process for FDA approvals,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='from regulatory bodies. \\n        Example: \\nA pharmaceutical company maintains a consistent documentation process for FDA approvals, \\npreventing delays in drug launches. \\n \\n3. Improves Collaboration Across Departments & Supply Chains \\nWhy It Matters? \\nModern product development involves multiple departments and global supply chains. Without \\nconsistency, different teams may use varied formats, tools, and workflows, leading to \\nmiscommunication and data silos. \\nBenefits of Process Consistency in Collaboration: \\n   Clear Communication Between Teams: Design, engineering, manufacturing, and suppliers follow \\na common workflow. \\n   Seamless Data Sharing: Standardized data formats ensure compatibility across PLM, ERP, and \\nCAD systems. \\n   Faster Issue Resolution: A structured change management process ensures everyone adapts to \\nmodifications quickly. \\n        Example: \\nA car manufacturer uses a standardized PLM platform for real-time collaboration between design'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='modifications quickly. \\n        Example: \\nA car manufacturer uses a standardized PLM platform for real-time collaboration between design \\nteams in the USA, Germany, and Japan, reducing errors in component specifications. \\n \\n4. Enhances Product Quality & Reduces Errors'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Why It Matters? \\nInconsistent processes can lead to design defects, manufacturing errors, and quality control \\nfailures. Standardization ensures error-free product development by defining clear quality \\ncheckpoints. \\nBenefits of Process Consistency in Quality Control: \\n   Early Detection of Defects: Ensures rigorous testing and validation at every stage. \\n   Reduces Manufacturing Errors: A structured Bill of Materials (BOM) process eliminates \\nmisconfigurations. \\n   Improves Customer Satisfaction: Ensures products meet or exceed quality expectations. \\n        Example: \\nAn aerospace company follows a standardized engineering change order (ECO) process, reducing \\nerrors in aircraft component assembly. \\n \\n5. Facilitates Scalability & Adaptability for Future Growth \\nWhy It Matters? \\nOrganizations need to scale their operations and adapt to new technologies. Without process \\nconsistency, scaling leads to chaos, inefficiencies, and quality issues.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Why It Matters? \\nOrganizations need to scale their operations and adapt to new technologies. Without process \\nconsistency, scaling leads to chaos, inefficiencies, and quality issues. \\nBenefits of Process Consistency in Scalability: \\n   Easier Onboarding of New Employees & Suppliers: Standardized processes allow quick \\nadaptation. \\n   Smooth Expansion into New Markets: Ensures regulatory compliance and operational \\nalignment across regions. \\n   Enables Digital Transformation: Structured workflows help organizations adopt AI, IoT, and \\ncloud-based PLM solutions. \\n        Example: \\nA tech company expands its manufacturing to Asia and seamlessly integrates new suppliers due to \\nstandardized PLM workflows. \\n \\n6. Strengthens Change Management & Version Control \\nWhy It Matters? \\nFrequent product design changes are common in industries like electronics, automotive, and \\nmedical devices. A lack of process consistency leads to mismanaged updates, version confusion, \\nand costly recalls.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='medical devices. A lack of process consistency leads to mismanaged updates, version confusion, \\nand costly recalls. \\nBenefits of Process Consistency in Change Management: \\n   Clear Change Request Approvals: Prevents uncontrolled modifications to product designs. \\n   Improved Version Control: Ensures teams work on the latest design versions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Minimizes Risk of Production Errors: Prevents manufacturers from using outdated product \\nspecifications. \\n        Example: \\nA smartwatch company maintains a single version of truth in PLM, preventing outdated firmware \\nfrom being installed on new models. \\n \\n7. Enables Cost Reduction & Maximizes ROI \\nWhy It Matters? \\nInconsistent processes lead to waste, rework, and inefficiencies that increase costs. Standardized \\nworkflows optimize costs and maximize return on investment (ROI). \\nBenefits of Process Consistency in Cost Optimization: \\n   Minimizes Rework & Waste: Reduces errors in design and production. \\n   Optimizes Resource Utilization: Prevents overproduction, material waste, and redundant efforts. \\n   Reduces Product Recalls & Failures: Ensures error-free product launches. \\n        Example: \\nA home appliance manufacturer cut production costs by 15% by standardizing supply chain \\nprocurement and assembly workflows. \\n \\nConclusion: Why Process Consistency is a Game-Changer'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content=\"A home appliance manufacturer cut production costs by 15% by standardizing supply chain \\nprocurement and assembly workflows. \\n \\nConclusion: Why Process Consistency is a Game-Changer \\nKey Takeaways: \\n✔ Ensures efficiency & faster time-to-market. \\n✔ Guarantees compliance & regulatory approval. \\n✔ Improves collaboration across teams & suppliers. \\n✔ Reduces errors & enhances product quality. \\n✔ Supports scalability & future growth. \\n✔ Strengthens change management & version control. \\n✔ Optimizes costs & maximizes ROI. \\nA structured, consistent PLM process is essential for companies aiming to innovate, reduce risks, \\nand stay competitive in today's fast-paced market.         \\n \\nWould you like an industry-specific case study on how Tesla, Apple, or Boeing leverage process \\nconsistency in PLM? Let me know!        \\n3.Explain the importance of product reuse and \\nsmart parts . \\nChatGPT said:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Importance of Product Reuse and Smart Parts in PLM \\nIntroduction \\nProduct reuse and smart parts are essential strategies in Product Lifecycle Management (PLM) to \\nenhance efficiency, reduce costs, and improve product quality. Organizations in industries like \\nautomotive, aerospace, electronics, and manufacturing leverage these concepts to accelerate \\ninnovation while maintaining sustainability and profitability. \\nIn this detailed explanation, we will explore: \\n• The importance of product reuse \\n• The role of smart parts \\n• Their benefits in PLM \\n \\n1. Importance of Product Reuse in PLM \\nWhat is Product Reuse? \\nProduct reuse refers to the practice of reusing existing designs, components, or modules in new \\nproduct development instead of creating them from scratch. This strategy enhances efficiency and \\npromotes sustainability. \\nKey Benefits of Product Reuse \\n   1. Reduces Development Time and Costs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='product development instead of creating them from scratch. This strategy enhances efficiency and \\npromotes sustainability. \\nKey Benefits of Product Reuse \\n   1. Reduces Development Time and Costs \\n  Eliminates the need for reinventing the wheel by leveraging existing parts and designs. \\n  Saves time and resources on design, testing, and validation. \\n  Engineers can focus on innovation rather than repetitive tasks. \\n        Example: \\nA smartphone company reuses the battery module and camera sensors from its previous model in a \\nnew version, significantly reducing R&D time and costs. \\n \\n   2. Enhances Product Quality and Reliability \\n  Reused components are already tested and validated, reducing defects. \\n  Improves product consistency by using proven designs. \\n  Minimizes unexpected failures in production. \\n        Example: \\nAn automotive manufacturer uses the same braking system across multiple car models, ensuring \\nreliability and performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Minimizes unexpected failures in production. \\n        Example: \\nAn automotive manufacturer uses the same braking system across multiple car models, ensuring \\nreliability and performance. \\n \\n   3. Improves Supply Chain Efficiency'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Standardized parts streamline procurement and inventory management. \\n  Reduces dependency on multiple suppliers, lowering supply chain risks. \\n  Enables bulk purchasing, reducing overall costs. \\n        Example: \\nAn aerospace company standardizes fasteners, sensors, and actuators across aircraft models, \\nsimplifying supplier relationships. \\n \\n   4. Supports Sustainability and Environmental Goals \\n  Reduces material waste and lowers carbon footprint. \\n  Encourages eco-friendly manufacturing by minimizing redundant production. \\n  Promotes circular economy principles in product design. \\n        Example: \\nA consumer electronics brand reuses materials from recycled laptops to manufacture new models, \\nreducing electronic waste. \\n \\n   5. Accelerates Innovation and Scalability \\n  Speeds up new product introductions by leveraging existing modules. \\n  Enables mass customization by using a modular design approach. \\n  Ensures faster adaptation to market changes and customer demands.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Enables mass customization by using a modular design approach. \\n  Ensures faster adaptation to market changes and customer demands. \\n        Example: \\nA medical device company reuses core software algorithms from a previous imaging system to \\ndevelop a next-generation scanner with AI capabilities. \\n \\n2. Importance of Smart Parts in PLM \\nWhat are Smart Parts? \\nSmart parts are intelligent, connected components that: \\n   Contain embedded sensors & chips for real-time monitoring. \\n   Enable data-driven decision-making in manufacturing. \\n   Improve predictive maintenance and self-diagnosis. \\nKey Benefits of Smart Parts \\n   1. Enables Real-Time Monitoring & Predictive Maintenance \\n  Smart parts collect data on performance, wear, and failure. \\n  Helps in preventive maintenance, reducing unexpected breakdowns. \\n  Extends product lifespan by optimizing usage conditions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA jet engine manufacturer installs IoT-enabled sensors in engines to monitor fuel efficiency and \\npredict maintenance needs. \\n \\n   2. Enhances Product Customization & Personalization \\n  Allows dynamic adjustments in performance based on user preferences. \\n  Supports on-the-fly reconfiguration for specific applications. \\n  Improves user experience through adaptive technology. \\n        Example: \\nA smart home appliance brand designs AI-powered washing machines that adjust wash cycles \\nbased on fabric type and weight. \\n \\n   3. Strengthens Supply Chain and Inventory Management \\n  Smart parts enable automated tracking and inventory updates. \\n  Reduce waste by monitoring usage patterns in real time. \\n  Enhances supply chain visibility, preventing shortages. \\n        Example: \\nAn automotive manufacturer uses RFID-equipped smart parts to track components in real time, \\nensuring smooth production flow. \\n \\n   4. Facilitates Digital Twin Implementation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nAn automotive manufacturer uses RFID-equipped smart parts to track components in real time, \\nensuring smooth production flow. \\n \\n   4. Facilitates Digital Twin Implementation \\n  Smart parts provide real-time data for digital twin simulations. \\n  Helps in virtual prototyping, troubleshooting, and performance optimization. \\n  Reduces physical testing costs and accelerates innovation. \\n        Example: \\nA manufacturing plant creates a digital twin of its assembly line using data from smart robotic arms, \\noptimizing efficiency. \\n \\n   5. Increases Safety and Compliance \\n  Smart parts monitor temperature, pressure, and stress levels, preventing failures. \\n  Ensures compliance with industry safety regulations. \\n  Alerts operators to hazardous conditions in real time. \\n        Example: \\nAn oil & gas company uses smart pipeline sensors to detect leaks and prevent environmental \\nhazards.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Conclusion: Why Product Reuse & Smart Parts Are Game-Changers in PLM \\nKey Takeaways: \\n✔ Product Reuse reduces costs, accelerates development, and enhances sustainability. \\n✔ Smart Parts improve efficiency, enable real-time monitoring, and enhance product intelligence. \\n✔ Both strategies optimize the PLM process by reducing waste, improving quality, and ensuring \\nregulatory compliance. \\n✔ Organizations adopting reuse and smart part technologies gain a competitive advantage in the \\nglobal market. \\n        Future Outlook: \\n• AI-driven smart part analytics for predictive maintenance. \\n• Blockchain integration for tracking reusable parts in the supply chain. \\n• 3D printing & additive manufacturing for on-demand smart part production. \\nYou said: \\n11.Explain teamcenter workflow designer application \\nprovide detailed answer \\nChatGPT said: \\nTeamcenter Workflow Designer Application – A Detailed Explanation \\nIntroduction to Teamcenter Workflow Designer'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='provide detailed answer \\nChatGPT said: \\nTeamcenter Workflow Designer Application – A Detailed Explanation \\nIntroduction to Teamcenter Workflow Designer \\nTeamcenter Workflow Designer is a critical application within Siemens Teamcenter PLM (Product \\nLifecycle Management) that enables organizations to automate, standardize, and manage business \\nprocesses effectively. It provides a graphical interface for designing, configuring, and optimizing \\nworkflows, ensuring smooth collaboration between teams, departments, and external stakeholders. \\nThis tool is primarily used for: \\n   Automating approval processes (e.g., engineering change orders, document approvals). \\n   Defining roles, responsibilities, and process sequences. \\n   Ensuring process consistency across different product lifecycle stages. \\n \\nKey Features of Teamcenter Workflow Designer \\n1. Graphical Workflow Modeling \\n  Provides a drag-and-drop interface to design workflows visually.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Key Features of Teamcenter Workflow Designer \\n1. Graphical Workflow Modeling \\n  Provides a drag-and-drop interface to design workflows visually. \\n  Users can create flowcharts, task sequences, and decision points. \\n  Reduces the need for manual coding when defining process logic. \\n        Example: \\nAn engineering team sets up an ECN (Engineering Change Notification) workflow, ensuring each \\ndesign change is reviewed by the right stakeholders before implementation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='2. Role-Based Task Assignment \\n  Assigns workflow steps based on user roles and access rights. \\n  Ensures the right engineers, managers, and stakeholders review the task at the right time. \\n  Supports parallel or sequential task execution. \\n        Example: \\nA design change request is automatically routed to the design engineer, quality manager, and \\nproduction lead, ensuring compliance before approval. \\n \\n3. Workflow Triggers & Conditions \\n  Enables workflows to start based on predefined events, conditions, or user actions. \\n  Automates decisions based on data inputs, status updates, or external triggers. \\n  Ensures compliance with standard operating procedures (SOPs). \\n        Example: \\nA manufacturing defect detected in the system automatically triggers a Corrective Action Process in \\nTeamcenter. \\n \\n4. Workflow Process Customization \\n  Users can create custom scripts for advanced automation. \\n  Supports integration with third-party tools and enterprise systems (ERP, MES, etc.).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Teamcenter. \\n \\n4. Workflow Process Customization \\n  Users can create custom scripts for advanced automation. \\n  Supports integration with third-party tools and enterprise systems (ERP, MES, etc.). \\n  Provides flexibility for adapting workflows to unique business needs. \\n        Example: \\nA company integrates Teamcenter with SAP ERP , enabling automatic purchase order creation once a \\npart is approved in Teamcenter. \\n \\n5. Task Status Tracking & Monitoring \\n  Real-time visibility into workflow progress, bottlenecks, and pending tasks. \\n  Provides dashboards & reports for workflow analytics. \\n  Alerts users about pending approvals or overdue tasks. \\n        Example: \\nA project manager receives an alert when an engineering change approval is delayed, allowing them \\nto take corrective action. \\n \\nHow Teamcenter Workflow Designer Works – Step-by-Step Process \\n   Step 1: Define the Workflow Objectives'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Identify the business process to automate (e.g., design approval, change management). \\n✔ Establish workflow rules, required approvals, and role assignments. \\n        Example: \\nA company needs a New Product Introduction (NPI) workflow, ensuring design, compliance, and \\ncost approvals before production starts. \\n \\n   Step 2: Create the Workflow in Teamcenter \\n✔ Open Teamcenter Workflow Designer and select \"Create New Workflow.\" \\n✔ Define tasks, approvals, and decision gates. \\n✔ Configure dependencies (e.g., approvals must happen before production starts). \\n        Example: \\nA supplier part validation workflow includes tasks like supplier audit, material testing, and \\nprocurement approval. \\n \\n   Step 3: Assign Roles & Permissions \\n✔ Assign tasks to specific users, departments, or external stakeholders. \\n✔ Define access control to protect sensitive data. \\n        Example: \\nA regulatory compliance workflow assigns review tasks to engineering, legal, and compliance \\nteams.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Define access control to protect sensitive data. \\n        Example: \\nA regulatory compliance workflow assigns review tasks to engineering, legal, and compliance \\nteams. \\n \\n   Step 4: Configure Workflow Conditions & Triggers \\n✔ Set up automatic triggers for workflow initiation (e.g., upon document submission). \\n✔ Define decision rules (e.g., reject an engineering change if cost exceeds the budget). \\n        Example: \\nIf a cost analysis report exceeds the target budget, the workflow triggers a rework request to the \\nfinance team. \\n \\n   Step 5: Test & Validate the Workflow \\n✔ Run a test case to check for errors, inefficiencies, and bottlenecks. \\n✔ Ensure smooth integration with existing business systems. \\n        Example: \\nA manufacturing company tests an assembly process validation workflow, ensuring that all quality \\nchecks are correctly assigned before implementation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Step 6: Deploy & Monitor Workflow Execution \\n✔ Launch the workflow in a live environment. \\n✔ Track progress using Teamcenter reporting tools. \\n✔ Optimize the process based on real-time feedback and analytics. \\n        Example: \\nA project manager notices approval delays in an engineering change request workflow, prompting a \\nprocess optimization review. \\n \\nBenefits of Using Teamcenter Workflow Designer \\n        1. Increases Process Efficiency \\n✔ Automates repetitive tasks, reducing manual work. \\n✔ Eliminates process delays by ensuring real-time tracking & notifications. \\n        2. Ensures Compliance & Standardization \\n✔ Adheres to industry regulations and company policies. \\n✔ Provides an audit trail for all approvals and workflow actions. \\n        3. Improves Collaboration \\n✔ Connects multiple teams, including engineering, manufacturing, supply chain, and compliance. \\n✔ Provides role-based task assignments, ensuring the right people handle the right tasks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Connects multiple teams, including engineering, manufacturing, supply chain, and compliance. \\n✔ Provides role-based task assignments, ensuring the right people handle the right tasks. \\n        4. Enhances Decision-Making \\n✔ Provides real-time workflow visibility. \\n✔ Helps managers identify bottlenecks and inefficiencies. \\n        5. Reduces Errors & Rework \\n✔ Prevents unauthorized changes in product designs and approvals. \\n✔ Ensures each process follows a predefined sequence, avoiding skipped steps. \\n \\nUse Cases of Teamcenter Workflow Designer \\n     1. Engineering Change Management (ECM) \\nScenario: An automotive company automates design change approvals using workflow designer, \\nensuring regulatory compliance before releasing new vehicle models. \\n     2. New Product Development (NPD) Approval \\nScenario: A medical device company standardizes its New Product Development (NPD) workflow, \\nintegrating testing, validation, and FDA compliance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='2. New Product Development (NPD) Approval \\nScenario: A medical device company standardizes its New Product Development (NPD) workflow, \\nintegrating testing, validation, and FDA compliance. \\n     3. Supplier Onboarding & Part Qualification'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Scenario: An aerospace manufacturer ensures supplier audits, material testing, and compliance \\napprovals before integrating new suppliers into the supply chain. \\n \\nConclusion: Why Use Teamcenter Workflow Designer? \\n✔ Automates & standardizes processes, improving efficiency. \\n✔ Ensures compliance with industry regulations & quality standards. \\n✔ Enhances collaboration across departments & global teams. \\n✔ Reduces errors & improves decision-making through real-time tracking. \\n✔ Boosts product lifecycle management by optimizing workflows for faster execution. \\n9.Distinguish between Current Information Model and \\nPLM model provide detailed answer \\nChatGPT said: \\nDistinguishing Between the Current Information Model and the PLM Model \\nIntroduction \\nInformation management is crucial for engineering, manufacturing, and business processes. Two \\nprimary approaches to handling information are: \\n1. Current Information Model (CIM) – The traditional way organizations store and manage \\ndata.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='primary approaches to handling information are: \\n1. Current Information Model (CIM) – The traditional way organizations store and manage \\ndata. \\n2. Product Lifecycle Management (PLM) Model – A more advanced, structured, and \\ninterconnected way of managing product data and processes throughout the product’s \\nlifecycle. \\n \\n1. Definition of the Two Models \\n   Current Information Model (CIM): \\nThe Current Information Model (CIM) refers to the existing or legacy way organizations handle \\ninformation. This model is often siloed, lacks structured interconnectivity, and primarily focuses on \\nstoring data for specific departments or purposes without seamless integration across the product \\nlifecycle. \\n  Characteristics of CIM: \\n✔ Data is stored in separate databases. \\n✔ Limited integration between different departments (e.g., design, production, and marketing). \\n✔ Information is often stored in documents, spreadsheets, or legacy systems.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Limited integration between different departments (e.g., design, production, and marketing). \\n✔ Information is often stored in documents, spreadsheets, or legacy systems. \\n✔ Changes in one area don’t automatically update across the entire organization. \\n        Example: A company stores its engineering designs in CAD software, manufacturing data in ERP, \\nand supplier information in Excel files, leading to data inconsistencies.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='PLM Model: \\nThe Product Lifecycle Management (PLM) Model is a centralized, integrated, and structured \\napproach to managing product data and processes across the entire product lifecycle. It creates a \\nsingle source of truth (SSOT) for product-related information, ensuring collaboration, version \\ncontrol, and traceability. \\n  Characteristics of the PLM Model: \\n✔ Single centralized repository for all product-related data. \\n✔ Seamless integration between design, engineering, manufacturing, sales, and service. \\n✔ Version control & change management to track updates and approvals. \\n✔ Supports collaboration between teams, including suppliers and customers. \\n        Example: A PLM system like Teamcenter or Windchill stores and links CAD models, BOMs (Bill \\nof Materials), simulation data, compliance documents, and change histories in one place. \\n \\n2. Key Differences Between CIM and PLM Model \\nAspect Current Information Model (CIM) PLM Model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='of Materials), simulation data, compliance documents, and change histories in one place. \\n \\n2. Key Differences Between CIM and PLM Model \\nAspect Current Information Model (CIM) PLM Model \\nData Storage Siloed in multiple systems (CAD, ERP , Excel) Centralized database with \\nstructured access \\nData Consistency Prone to duplication and inconsistency Single source of truth (SSOT) \\nensures consistency \\nCollaboration Limited; data exchange between \\ndepartments is manual \\nSeamless collaboration across \\nteams, suppliers, and partners \\nChange \\nManagement \\nChanges must be manually communicated, \\noften leading to errors \\nAutomated workflows track and \\nmanage product changes \\nTraceability Difficult to trace product history across \\ndepartments \\nFull traceability from concept to \\nretirement \\nVersion Control Weak or non-existent; multiple versions of \\nthe same document may exist \\nRobust version control prevents \\nconflicts and data loss \\nRegulatory \\nCompliance \\nRequires manual tracking and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='the same document may exist \\nRobust version control prevents \\nconflicts and data loss \\nRegulatory \\nCompliance \\nRequires manual tracking and \\ndocumentation \\nBuilt-in compliance tracking and \\nreporting \\nProcess \\nAutomation \\nMinimal automation, reliant on human \\nintervention \\nAutomated workflows, reducing \\nerrors and inefficiencies \\nCost of Errors High due to miscommunication and data \\nloss \\nLow, as errors are detected and \\nresolved early'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Aspect Current Information Model (CIM) PLM Model \\nScalability Hard to scale, as adding new features or \\nusers is complex \\nEasily scalable for large, global \\norganizations \\n \\n3. Importance of Moving from CIM to PLM Model \\nOrganizations that transition from the Current Information Model to a PLM Model experience: \\n✔ Faster time-to-market due to efficient workflows. \\n✔ Improved product quality due to better change management. \\n✔ Cost savings by reducing errors, redundancies, and rework. \\n✔ Better compliance & audit readiness for industry regulations. \\n✔ Enhanced innovation by integrating AI, IoT, and digital twins. \\n        Example: A global automotive manufacturer moves from Excel-based data storage (CIM) to \\nPLM-based digital product management. As a result, they reduce design iterations by 30% and \\nimprove collaboration between R&D, manufacturing, and suppliers. \\n \\n4. Conclusion \\n✔ The Current Information Model (CIM) is outdated, fragmented, and inefficient for modern'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='improve collaboration between R&D, manufacturing, and suppliers. \\n \\n4. Conclusion \\n✔ The Current Information Model (CIM) is outdated, fragmented, and inefficient for modern \\nproduct development. \\n✔ The PLM Model provides an integrated, structured, and scalable approach to product data \\nmanagement, ensuring real-time collaboration, version control, and process automation. \\n✔ Transitioning to a PLM system is essential for digital transformation, competitive advantage, and \\nsustainable growth in today’s industries.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"UNIT-01 \\n2 MARKS \\n \\n1.Summarize the reasons for the domain expertise for \\nany type of data analytics. \\n \\nDomain expertise is crucial in data analytics because it provides the necessary context to understand \\ndata, identify relevant patterns, and interpret results, leading to better data quality, more \\nmeaningful insights, and ultimately, more effective decision-making.  \\nHere's a more detailed explanation of why domain expertise is so important: \\n• Data Quality Assessment: \\nDomain experts can identify anomalies, outliers, and potential biases in the data that might go \\nunnoticed by someone lacking domain knowledge, ensuring the data is reliable and accurate.  \\n• Feature Engineering: \\nDomain expertise helps identify which features are most relevant and meaningful for a particular \\nproblem, leading to more effective models and analysis.  \\n• Model Interpretation: \\nUnderstanding the domain context is crucial for interpreting the output of a data model and its\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='problem, leading to more effective models and analysis.  \\n• Model Interpretation: \\nUnderstanding the domain context is crucial for interpreting the output of a data model and its \\nimplications in the real world, ensuring that the insights are actionable and relevant.  \\n• Understanding Data Context: \\nDomain knowledge helps analysts understand how data is collected, its sources, and the context in \\nwhich it exists, which is essential for proper data preprocessing and cleaning.  \\n• Identifying Actionable Insights: \\nDomain expertise allows analysts to translate data insights into practical recommendations and \\nsolutions that address real-world business issues.  \\n• Improved Communication: \\nDomain expertise facilitates better communication between data analysts and stakeholders, \\nensuring that everyone understands the findings and can make informed decisions.  \\n• Innovation and Opportunity Identification:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='ensuring that everyone understands the findings and can make informed decisions.  \\n• Innovation and Opportunity Identification: \\nDomain experts can identify opportunities for innovation and new approaches to problem-solving \\nbased on their understanding of the industry and data.  \\n• Tailored Solutions: \\nDomain knowledge allows for the development of solutions that are tailored to the specific needs \\nand challenges of a particular industry or business.  \\n \\n \\n1. Summarize the reasons for domain expertise in any type of data analytics. \\nDomain expertise is essential in data analytics as it helps in understanding the context of data, \\nidentifying relevant trends, correlations, and anomalies, and ensuring accurate interpretation of \\nresults. Without domain knowledge, analysts may misinterpret insights, leading to incorrect \\nconclusions. Expertise in areas like healthcare, finance, or marketing ensures that data-driven \\ndecisions align with business objectives.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='conclusions. Expertise in areas like healthcare, finance, or marketing ensures that data-driven \\ndecisions align with business objectives. \\nExample: In healthcare, a domain expert can help identify meaningful patterns in patient data, such \\nas risk factors for diseases, leading to better diagnosis and treatment recommendations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='2.How the accuracy in big data is beneficial? \\n \\nData accuracy is vital to the success of all organizations—from sales to accounting and marketing to \\nhuman resources. Data informs decisions, creates impressions about an organization, and drives \\nrevenue. Reasons why data accuracy is important and a priority for the enterprise are that data \\naccuracy: \\n• Delivers better results to the organization’s users \\n• Drives more value from artificial intelligence implementations with accurate and consistent \\ndata to feed algorithms \\n• Enables better decision-making \\n• Enhances efficiency \\n• Makes it easier to achieve consistent results \\n• Mitigates risks associated with flawed data \\n• Provides confidence to users who depend on the data \\n• Reduces the need to spend time and money finding and fixing errors in the data \\n• Supports focused audience targeting and marketing efforts \\n2. How is accuracy in big data beneficial?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"• Reduces the need to spend time and money finding and fixing errors in the data \\n• Supports focused audience targeting and marketing efforts \\n2. How is accuracy in big data beneficial? \\nAccuracy in big data ensures better decision-making, predictive analytics, and fraud \\ndetection. High accuracy reduces errors, biases, and misinformation, leading to efficient \\nbusiness operations and improved customer experiences. \\nExample: In financial services, accurate big data analysis can detect fraudulent transactions \\nin real-time, preventing losses and enhancing security. \\n3.What are the top challenges in big data? \\nThe top challenges in big data revolve around managing its volume, velocity, \\nvariety, and veracity, along with data quality, security, and integration, as well as \\nfinding and retaining skilled talent.  \\nHere's a more detailed breakdown: \\n1. Volume, Velocity, Variety, and Veracity (The 4 Vs): \\n• Volume: \\nThe sheer amount of data generated and stored is immense, requiring robust\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"Here's a more detailed breakdown: \\n1. Volume, Velocity, Variety, and Veracity (The 4 Vs): \\n• Volume: \\nThe sheer amount of data generated and stored is immense, requiring robust \\nstorage and processing infrastructure.  \\n• Velocity: \\nData is generated and processed at an increasingly rapid pace, demanding real-\\ntime analytics and processing capabilities.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='• Variety: \\nBig data comes in various formats (structured, unstructured, semi-structured) \\nfrom diverse sources, requiring flexible data management and analysis tools.  \\n• Veracity: \\nData quality and accuracy are crucial, as inaccurate or incomplete data can lead \\nto flawed insights and decisions \\n3. What are the top challenges in big data? \\nBig data presents challenges such as data storage, processing speed, data \\nquality, security, and privacy. Managing the 3Vs (Volume, Velocity, Variety) is \\ndifficult, requiring advanced tools like Hadoop and Spark. Ensuring data \\nintegrity and protecting sensitive information from cyber threats is also critical. \\nExample: Social media companies face challenges in analyzing millions of posts \\nper second while filtering out fake news and maintaining user privacy. \\n \\n4,12.Distinguish between Big data and conventional \\ndata. \\n \\nTraditional Data  Big Data  \\nTraditional data is generated in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='per second while filtering out fake news and maintaining user privacy. \\n \\n4,12.Distinguish between Big data and conventional \\ndata. \\n \\nTraditional Data  Big Data  \\nTraditional data is generated in \\nenterprise level. Big data is generated outside the enterprise level. \\nIts volume ranges from Gigabytes to \\nTerabytes. \\nIts volume ranges from Petabytes to Zettabytes or \\nExabytes. \\nTraditional database system deals \\nwith structured data. \\nBig data system deals with structured, semi-\\nstructured,database, and unstructured data. \\nTraditional data is generated per hour or \\nper day or more. \\nBut big data is generated more frequently mainly \\nper seconds. \\nTraditional data source is centralized \\nand it is managed in centralized form. \\nBig data source is distributed and it is managed in \\ndistributed form. \\nData integration is very easy. Data integration is very difficult.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='Traditional Data  Big Data  \\nNormal system configuration is capable \\nto process traditional data. \\nHigh system configuration is required to process \\nbig data. \\nThe size of the data is very small. The size is more than the traditional data size. \\nTraditional data base tools are required \\nto perform any data base operation. \\nSpecial kind of data base tools are required to \\nperform any database schema based operation. \\nNormal functions can manipulate data. Special kind of functions can manipulate data. \\nIts data model is strict schema based \\nand it is static. \\nIts data model is a flat schema based and it is \\ndynamic. \\nTraditional data is stable and inter \\nrelationship. Big data is not stable and unknown relationship. \\nTraditional data is in manageable \\nvolume. \\nBig data is in huge volume which becomes \\nunmanageable. \\nIt is easy to manage and manipulate the \\ndata. It is difficult to manage and manipulate the data. \\n \\n4. Distinguish between big data and conventional data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"unmanageable. \\nIt is easy to manage and manipulate the \\ndata. It is difficult to manage and manipulate the data. \\n \\n4. Distinguish between big data and conventional data. \\n• Big Data: Large, complex datasets that require specialized tools for analysis. It includes \\nstructured, unstructured, and semi-structured data from diverse sources. \\n• Conventional Data: Smaller, structured datasets stored in relational databases (RDBMS) \\nwith fixed schemas. \\nExample: \\n• Big Data: Real-time IoT sensor data from smart cities. \\n• Conventional Data: Employee records stored in an Excel sheet. \\n \\n5.Outline the role of big data analytics. \\n \\nBig data analytics plays a crucial role in modern organizations by enabling them to extract valuable \\ninsights from large datasets, leading to better decision-making, improved operations, and increased \\ncompetitiveness.  \\nHere's a more detailed outline: \\n1. Uncovering Hidden Patterns and Trends: \\n• Identifying Opportunities:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"competitiveness.  \\nHere's a more detailed outline: \\n1. Uncovering Hidden Patterns and Trends: \\n• Identifying Opportunities: \\nBig data analytics helps organizations discover hidden patterns, correlations, and market trends that \\nmight otherwise go unnoticed, allowing them to identify new opportunities for growth and \\ninnovation.  \\n• Predictive Analytics:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"By analyzing historical data, organizations can use big data analytics to predict future outcomes, \\nenabling them to make proactive decisions and mitigate potential risks.  \\n• Understanding Customer Behavior: \\nBig data analytics provides insights into customer behavior, preferences, and needs, allowing \\nbusinesses to personalize their offerings and improve customer experiences.  \\n \\n5. Outline the role of big data analytics. \\nBig data analytics helps in trend identification, predictive modeling, and real-time decision-\\nmaking. It enhances customer experiences, operational efficiency, and fraud detection. \\nExample: E-commerce companies like Amazon use big data analytics to recommend products \\nbased on user behavior. \\n \\n \\n6,15.State the four computing resources of Big Data \\nStorage. \\nThe four main computing resources for Big Data Storage are data storage, data mining, data \\nanalytics, and data visualization.  \\nHere's a more detailed explanation of each: \\n• Data Storage:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"The four main computing resources for Big Data Storage are data storage, data mining, data \\nanalytics, and data visualization.  \\nHere's a more detailed explanation of each: \\n• Data Storage: \\nThis involves the methods and technologies used to store large, complex datasets, which can include \\ndata lakes, data warehouses, cloud storage, and object storage.  \\n• Data Mining: \\nThis process involves extracting valuable insights and patterns from large datasets using various \\ntechniques and algorithms.  \\n• Data Analytics: \\nThis focuses on analyzing and interpreting the extracted data to gain meaningful insights and make \\ninformed decisions.  \\n• Data Visualization: \\nThis involves presenting the analyzed data in a visual format, such as charts, graphs, and dashboards, \\nto facilitate understanding and communication of findings.  \\n \\n6. State the four computing resources of Big Data Storage. \\n1. Processing Power (CPU & GPU): Required for computing large datasets.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='to facilitate understanding and communication of findings.  \\n \\n6. State the four computing resources of Big Data Storage. \\n1. Processing Power (CPU & GPU): Required for computing large datasets. \\n2. Storage (HDFS, Cloud Storage): Stores petabytes of structured and unstructured data. \\n3. Memory (RAM): Supports high-speed processing and real-time analytics. \\n4. Networking (High-Speed Data Transfer): Ensures seamless data movement across systems. \\nExample: Google Cloud provides high-performance computing for real-time big data processing. \\n \\n \\n7.What role does cloud computing play in Big Data \\nManagement? \\n \\nCloud Computing: The Big Data Solution \\nCloud computing offers an effective solution towards dealing with big size information \\nsets. Organizations can store their big-data efficiently manage them as well analyze them by \\nleveraging scalability provided through clouds on demand resources such as storage capacity'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='7. What role does cloud computing play in Big Data Management? \\nCloud computing provides scalability, cost-efficiency, remote accessibility, and real-time processing \\nfor big data applications. Platforms like AWS, Google Cloud, and Azure offer distributed computing \\nand AI-powered analytics. \\nExample: Netflix uses AWS cloud computing to process and recommend personalized content to \\nmillions of users. \\n \\n \\n8.List the various dimensions of growth of big data. \\n \\nThe growth of big data is often characterized by the \"5 V\\'s\": Volume, Velocity, Variety, Veracity, and \\nValue.  \\nHere\\'s a breakdown of each dimension: \\n• Volume: \\nRefers to the sheer amount of data being generated and collected, often measured in terabytes, \\npetabytes, or even exabytes.  \\n• Velocity: \\nDescribes the speed at which data is generated, collected, and processed, including real-time and \\nbatch processing.  \\n• Variety: \\nEncompasses the different types and formats of data, including structured, unstructured, and semi-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='batch processing.  \\n• Variety: \\nEncompasses the different types and formats of data, including structured, unstructured, and semi-\\nstructured data.  \\n• Veracity: \\nRelates to the accuracy, reliability, and trustworthiness of the data.  \\n• Value: \\nFocuses on the insights and actionable knowledge that can be derived from the data, enabling better  \\ndecision-making and business outcomes.  \\n \\n8. List the various dimensions of growth of big data. \\n1. Volume: Increase in the amount of data generated daily. \\n2. Velocity: Speed at which data is created and processed. \\n3. Variety: Diverse data types (text, images, audio, video). \\n4. Veracity: Ensuring data accuracy and reliability. \\n5. Value: Extracting meaningful insights for business impact. \\nExample: Twitter generates terabytes of text data every day, requiring advanced analytics. \\n \\n \\n9.Identify the risks involved in using big data. \\n \\nUsing big data presents risks including  \\ndata privacy breaches \\n security vulnerabilities'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='9.Identify the risks involved in using big data. \\n \\nUsing big data presents risks including  \\ndata privacy breaches \\n security vulnerabilities \\n ethical concerns, \\n potential for misuse \\n managing large datasets.  \\n \\n9. Identify the risks involved in using big data. \\n• Privacy breaches: Exposure of sensitive user data. \\n• Security vulnerabilities: Risk of cyberattacks. \\n• Bias in AI models: Incorrect conclusions due to skewed data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='• Data inconsistency: Errors caused by poor-quality data. \\nExample: A social media company mishandling user data can lead to data breaches and legal \\nconsequences. \\n \\n \\n \\n10.With example explain structured data. \\nStructured data refers to information organized in a predefined format, making it easily searchable \\nand analyzable by computers, such as data in tables, databases, or spreadsheets. Examples \\ninclude customer names, addresses, dates, times, and product prices.  \\n• Spreadsheets (Excel): \\nData organized in rows and columns, similar to relational databases, but with more flexibility in \\nformatting and data types.  \\n \\n10. With an example, explain structured data. \\nStructured data is highly organized and stored in relational databases with predefined schemas. It is \\neasy to query using SQL. \\nExample: A customer database containing fields like Name, Age, Email, and Purchase History stored \\nin an SQL table. \\n \\n \\n11.State the best practices for Big Data Analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='Example: A customer database containing fields like Name, Age, Email, and Purchase History stored \\nin an SQL table. \\n \\n \\n11.State the best practices for Big Data Analytics. \\n \\n• Implement Data Quality Management Programs. \\n• Build More Scalable Infrastructures. \\n• Employ Agile Development Methodologies. \\n• Safeguard Data With Robust Security Measures. \\n• Use Data Ethically. \\n• Monitor and Optimize Continuously. \\n• Provide Workforce Skill Development. \\n11. State the best practices for Big Data Analytics. \\n1. Ensure data quality and consistency. \\n2. Use scalable and efficient storage solutions (HDFS, Cloud). \\n3. Implement security measures to protect sensitive data. \\n4. Leverage AI and ML for deeper insights. \\n5. Optimize processing speed with parallel computing. \\nExample: A bank using AI-driven fraud detection models must ensure data accuracy for reliable \\npredictions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='13.Compare data analytics and big data analytics. \\n \\nThe purpose of big data is to store huge \\nvolume of data and to process it. \\nThe purpose of data analytics is to \\nanalyze the raw data and find out \\ninsights for the information. \\nParallel computing and other complex \\nautomation tools are used to handle big \\ndata. \\nPredictive and statistical modelling with \\nrelatively simple tools used to handle \\ndata analytics. \\nBig data operations are handled by big data \\nprofessionals. \\nData analytics is performed by skilled \\ndata analysts. \\nBig data analysts need the knowledge of \\nprogramming, NoSQL databases, distributed \\nsystems, and frameworks. \\nData Analysts need the knowledge of \\nprogramming, statistics, and \\nmathematics. \\nIt supports in dealing with huge volumes of \\ndata. \\nIt supports in examining raw data and \\nrecognizing useful information. \\n \\n13. Compare data analytics and big data analytics. \\nFeature Data Analytics Big Data Analytics \\nData Size Small to Medium Very Large (Petabytes)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='recognizing useful information. \\n \\n13. Compare data analytics and big data analytics. \\nFeature Data Analytics Big Data Analytics \\nData Size Small to Medium Very Large (Petabytes) \\nProcessing SQL, Excel Hadoop, Spark \\nComplexity Simple Queries AI, ML, Predictive \\nExample HR payroll data analysis Sentiment analysis of social media posts \\nIdentify the four computing resources of Big Data Storage. \\n \\n11,14. List out the best practices of Big Data Analytics. \\n• Define clear business objectives. \\n• Use the right technology stack (Spark, NoSQL). \\n• Ensure compliance with data privacy regulations (GDPR, HIPAA). \\n• Implement real-time monitoring for fraud detection. \\nExample: Amazon optimizes inventory using real-time big data analytics. \\n \\n16. Define big data and under what conditions it is \\ngiven that name. \\nBig Data refers to extremely large datasets that traditional databases cannot handle \\nefficiently. It is characterized by Volume, Velocity, Variety, Veracity, and Value.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='given that name. \\nBig Data refers to extremely large datasets that traditional databases cannot handle \\nefficiently. It is characterized by Volume, Velocity, Variety, Veracity, and Value. \\nExample: Facebook stores and processes billions of images, videos, and messages daily. \\n \\nWhy it\\'s called \"big data\":'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='The term \"big data\" is used because these datasets are so large and complex that they \\nrequire specialized technologies and tools to manage, store, and analyze them effectively.  \\n \\ncharacterized by the \"5 Vs\": volume, velocity, variety, veracity, and value.  \\n \\n• Examples: \\n• Social media posts, emails, and web traffic data.  \\n• Financial transaction records, customer databases, and inventory data.  \\n• Sensor data from IoT devices, medical records, and scientific experiments.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045be01",
   "metadata": {},
   "source": [
    "\n",
    "##embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdebdbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jasmi\\OneDrive\\Desktop\\RAGtut\\.venv\\Lib\\site-packages\\chromadb\\execution\\expression\\operator.py:239: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  Key(\"email\").regex(r\".*@example\\.com\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6158e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jasmi\\OneDrive\\Desktop\\RAGtut\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jasmi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1a24fa6a3c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182af9b",
   "metadata": {},
   "source": [
    "\n",
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cc0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1a24fb88ec0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45569496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='15 marks VDA \\n \\n1.Significance of Software Standards in OpenGL and How It Ensures Cross-Platform Compatibility \\nSignificance of Software Standards in OpenGL \\nSoftware standards in OpenGL are essential for ensuring uniformity, performance, and compatibility \\nacross different computing environments. OpenGL is an open standard for graphics rendering, which \\nmeans it is maintained and updated by a consortium rather than a single company. The importance \\nof software standards in OpenGL can be explained through the following points: \\n1. Cross-Platform Consistency: \\nOpenGL provides a standardized API that enables graphics applications to run across multiple \\nplatforms, including Windows, macOS, Linux, and mobile operating systems like Android. This \\nreduces the need for platform-specific modifications and ensures consistent performance \\nacross different devices. \\n2. Hardware Abstraction: \\nOpenGL abstracts hardware details, allowing applications to interact with the GPU without'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='across different devices. \\n2. Hardware Abstraction: \\nOpenGL abstracts hardware details, allowing applications to interact with the GPU without \\nbeing tied to a specific graphics card manufacturer. This means developers do not have to \\nwrite separate code for AMD, NVIDIA, or Intel GPUs, as OpenGL provides a unified API. \\n3. Interoperability Across Vendors: \\nOpenGL follows a well-defined specification that multiple vendors adhere to. This ensures \\nthat OpenGL applications can run on different hardware implementations without significant \\nmodifications, fostering a competitive and diverse ecosystem. \\n4. Backward Compatibility: \\nOpenGL maintains backward compatibility between different versions, ensuring that \\napplications written for older OpenGL versions can still function on newer hardware with \\nminimal changes. This protects developers from frequent rewrites and ensures longevity for \\ngraphics applications. \\n5. Performance Optimization:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='minimal changes. This protects developers from frequent rewrites and ensures longevity for \\ngraphics applications. \\n5. Performance Optimization: \\nOpenGL standards define efficient rendering pipelines and data management techniques, \\nallowing applications to optimize GPU usage. Standardized API functions ensure that \\napplications use the best possible methods for rendering, irrespective of the underlying \\nhardware. \\n6. Scalability and Flexibility: \\nOpenGL is designed to support applications ranging from simple 2D graphics rendering to \\ncomplex 3D simulations and high-performance computing. It scales from embedded systems \\nto high-end gaming and visualization applications. \\n7. Adoption in Multiple Industries: \\nDue to its open nature and wide support, OpenGL is used in various industries, including \\ngaming, CAD (Computer-Aided Design), scientific visualization, virtual reality, and medical \\nimaging. This widespread adoption is made possible by standardized API guidelines.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 0, 'page_label': '1', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content=\"gaming, CAD (Computer-Aided Design), scientific visualization, virtual reality, and medical \\nimaging. This widespread adoption is made possible by standardized API guidelines. \\n8. Facilitation of Open-Source Development: \\nOpenGL's standardized nature has led to the development of numerous open-source\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='graphics engines, frameworks, and libraries, fostering innovation and collaboration in the \\nsoftware development community. \\nHow OpenGL Ensures Cross-Platform Compatibility \\nOpenGL achieves cross-platform compatibility through several key mechanisms: \\n1. Platform-Independent API: \\nOpenGL provides a single, consistent API that remains the same across different operating \\nsystems. This means that the same OpenGL code can be compiled and executed on various \\nplatforms with minimal modifications. \\n2. Graphics Driver Support: \\nGPU manufacturers like NVIDIA, AMD, and Intel provide OpenGL-compliant drivers that \\nimplement the OpenGL standard for their specific hardware. These drivers handle the \\ntranslation of OpenGL API calls into optimized machine-level instructions for the GPU. \\n3. Extension Mechanism: \\nOpenGL supports an extension system that allows hardware vendors to introduce new \\nfeatures while maintaining compatibility with the core OpenGL standard. Developers can'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='3. Extension Mechanism: \\nOpenGL supports an extension system that allows hardware vendors to introduce new \\nfeatures while maintaining compatibility with the core OpenGL standard. Developers can \\nselectively use these extensions to take advantage of new hardware capabilities without \\nbreaking compatibility with older systems. \\n4. Use of Utility Libraries: \\nOpenGL does not handle system-specific tasks such as window management or input \\nhandling. Instead, libraries like GLUT (OpenGL Utility Toolkit) and GLFW (Graphics Library \\nFramework) provide cross-platform support for these functions, allowing OpenGL \\napplications to run on different operating systems seamlessly. \\n5. OpenGL Context Management: \\nOpenGL requires an OpenGL context to be created before rendering can begin. Libraries like \\nWGL (Windows), GLX (Linux), and CGL (macOS) handle the integration with the operating \\nsystem, ensuring smooth cross-platform support. \\n6. Shader Language Standardization (GLSL):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='WGL (Windows), GLX (Linux), and CGL (macOS) handle the integration with the operating \\nsystem, ensuring smooth cross-platform support. \\n6. Shader Language Standardization (GLSL): \\nOpenGL includes the OpenGL Shading Language (GLSL), which allows developers to write \\nGPU-accelerated shaders in a hardware-independent way. GLSL ensures that custom graphics \\neffects and rendering techniques can work across different GPU brands without modification. \\n7. Adherence to OpenGL Standards by the Khronos Group: \\nThe Khronos Group oversees the OpenGL specification, ensuring that all implementations \\nfollow a common standard. This prevents fragmentation and ensures a stable and \\npredictable development environment. \\n8. Support for Different Rendering Pipelines: \\nOpenGL supports both fixed-function and programmable pipelines, making it compatible \\nwith both older and modern hardware. The introduction of modern pipeline techniques like'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 1, 'page_label': '2', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL supports both fixed-function and programmable pipelines, making it compatible \\nwith both older and modern hardware. The introduction of modern pipeline techniques like \\nVertex Buffer Objects (VBOs) and Framebuffer Objects (FBOs) further enhances its flexibility \\nacross different platforms. \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL’s adherence to software standards ensures that developers can build applications that work \\nseamlessly across different operating systems and hardware configurations. Its cross-platform \\ncapabilities are maintained through standardized APIs, driver support, extension mechanisms, and \\nshader language standardization. As a result, OpenGL remains one of the most widely used graphics \\nAPIs for real-time rendering, making it a crucial tool for developers across multiple industries. \\n \\n(8marks) \\n3. Advantages and Limitations of DDA and Bresenham’s Line Drawing Algorithms \\nLine drawing is a fundamental operation in computer graphics, and DDA (Digital Differential \\nAnalyzer) algorithm and Bresenham’s algorithm are two commonly used techniques for rasterizing \\nlines on a pixel-based display. Both methods work by determining which pixels should be illuminated \\nto best approximate a straight line. \\nEach algorithm has its strengths and weaknesses in terms of speed, accuracy, computational'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='to best approximate a straight line. \\nEach algorithm has its strengths and weaknesses in terms of speed, accuracy, computational \\nefficiency, and hardware implementation. Below is a detailed comparison of the advantages and \\nlimitations of both algorithms. \\n \\n1. Digital Differential Analyzer (DDA) Algorithm \\nOverview \\nThe Digital Differential Analyzer (DDA) algorithm is an incremental approach to line drawing. It \\ncalculates intermediate points between two endpoints using the slope-intercept equation of a line: \\ny=mx+cy = mx + cy=mx+c  \\nwhere m is the slope of the line. The algorithm incrementally updates x or y depending on the slope: \\n• If |m| ≤ 1, increment x and compute y. \\n• If |m| > 1, increment y and compute x. \\nThis method relies on floating-point arithmetic to determine pixel positions. \\n \\nAdvantages of DDA Algorithm \\n1. Simple and Easy to Implement \\no The logic follows the basic concept of linear interpolation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Advantages of DDA Algorithm \\n1. Simple and Easy to Implement \\no The logic follows the basic concept of linear interpolation. \\no The algorithm is easier to understand and implement compared to Bresenham’s \\nalgorithm. \\n2. Handles All Slopes Uniformly \\no DDA works for both steep and shallow slopes by adjusting the step size in either the \\nx or y direction. \\n3. Works Well with Floating-Point Arithmetic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Since DDA uses floating-point calculations, it provides higher precision in \\ndetermining pixel positions compared to purely integer-based methods. \\n4. Generalized for Other Shapes \\no The principles of DDA can be extended for circle drawing, curve generation, and \\nother geometric transformations in graphics. \\n \\nLimitations of DDA Algorithm \\n1. Computationally Expensive \\no DDA uses floating-point multiplication, division, and rounding operations, which \\nare slower than integer calculations. \\no This makes it less suitable for real-time applications and low-powered devices like \\nembedded systems. \\n2. Accumulation of Round-Off Errors \\no Since floating-point calculations are used, rounding errors accumulate over long \\nlines. \\no This can result in deviations from the true line as more pixels are plotted. \\n3. Less Efficient than Bresenham’s Algorithm \\no Requires additional multiplication and division operations in each step, making it \\ncomputationally slower than Bresenham’s algorithm.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 3, 'page_label': '4', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='3. Less Efficient than Bresenham’s Algorithm \\no Requires additional multiplication and division operations in each step, making it \\ncomputationally slower than Bresenham’s algorithm. \\no Not ideal for applications that require high-speed rendering. \\n \\n2. Bresenham’s Line Drawing Algorithm \\nOverview \\nBresenham’s algorithm is an incremental decision-based approach that efficiently determines the \\nnext pixel using integer arithmetic instead of floating-point calculations. \\nIt avoids multiplication and division by using an error term (decision variable) to track the closest \\npixel. Instead of computing the exact floating-point values, it approximates the line by selecting the \\npixel that minimizes error accumulation. \\n \\nAdvantages of Bresenham’s Algorithm \\n1. More Efficient than DDA \\no Uses only integer addition and subtraction, making it significantly faster than the \\nDDA algorithm. \\no Suitable for real-time rendering and low-resource systems like embedded devices.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='2. Minimizes Round-Off Errors \\no Bresenham’s algorithm avoids floating-point calculations, eliminating accumulated \\nrounding errors found in DDA. \\no This makes it more accurate for pixel-based rendering. \\n3. Better Performance on Raster Displays \\no Since it directly selects the next pixel based on integer calculations, it optimally fits \\ninto raster display systems. \\no Works well for modern graphics hardware. \\n4. Ideal for Hardware Implementation \\no Bresenham’s algorithm is easy to implement in hardware circuits (like GPUs), making \\nit widely used in real-time rendering engines. \\n5. Reduces Computational Overhead \\no No need for complex arithmetic operations (multiplication or division), leading to \\nbetter execution speed. \\n \\nLimitations of Bresenham’s Algorithm \\n1. Limited to Straight Lines Only \\no Unlike DDA, Bresenham’s method is specifically designed for straight lines. \\no To draw curves, additional algorithms like Midpoint Circle Algorithm are needed.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 4, 'page_label': '5', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='1. Limited to Straight Lines Only \\no Unlike DDA, Bresenham’s method is specifically designed for straight lines. \\no To draw curves, additional algorithms like Midpoint Circle Algorithm are needed. \\n2. Slightly More Complex Implementation \\no While more efficient, Bresenham’s algorithm is harder to understand and \\nimplement than DDA. \\n3. Does Not Support Anti-Aliasing \\no The algorithm produces jagged edges (aliasing effect) when rendering lines. \\no Additional anti-aliasing techniques like Wu’s Line Algorithm are needed for smooth \\nline rendering. \\n \\nComparison Table: DDA vs. Bresenham’s Algorithm \\nFeature DDA Algorithm Bresenham’s Algorithm \\nArithmetic Type Uses floating-point calculations Uses only integer calculations \\nExecution Speed Slower due to \\ndivision/multiplication Faster due to integer arithmetic'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Feature DDA Algorithm Bresenham’s Algorithm \\nAccuracy Accumulates rounding errors More accurate, avoids rounding errors \\nEfficiency Less efficient for high-speed \\napplications Highly efficient for real-time rendering \\nComplexity Easier to implement More complex decision-making process \\nHandling of Slopes Works well for all slopes Requires different cases for steep/shallow \\nslopes \\nHardware \\nSuitability Not ideal for low-power devices Well-suited for hardware and embedded \\nsystems \\nAnti-Aliasing \\nSupport No built-in anti-aliasing Produces jagged edges (needs additional \\nsmoothing) \\n \\nConclusion \\nBoth DDA and Bresenham’s algorithm have their place in computer graphics: \\n• DDA is simple and useful for understanding basic line drawing principles but suffers from \\ncomputational inefficiency. \\n• Bresenham’s algorithm is the preferred choice for fast, accurate, and efficient rendering, \\nmaking it the standard for real-time graphics applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='computational inefficiency. \\n• Bresenham’s algorithm is the preferred choice for fast, accurate, and efficient rendering, \\nmaking it the standard for real-time graphics applications. \\nIf performance is a priority (e.g., in gaming, simulations, and embedded systems), Bresenham’s \\nalgorithm is the best choice. However, for floating-point precision and ease of implementation, \\nDDA can still be useful in applications where speed is not critical. \\n \\n \\n \\n(7 marks) \\n4.Construct homogeneous representation in computer graphics and discuss how it simplifies the \\nprocess of transformations along with its merits.(7) provide detailed answer \\nChatGPT said: \\nHomogeneous Representation in Computer Graphics \\nIn computer graphics, homogeneous coordinates are an extension of Cartesian coordinates that \\nfacilitate geometric transformations such as translation, scaling, rotation, and perspective projection. \\nHomogeneous coordinates introduce an additional dimension (w-component), making it easier to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 5, 'page_label': '6', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='facilitate geometric transformations such as translation, scaling, rotation, and perspective projection. \\nHomogeneous coordinates introduce an additional dimension (w-component), making it easier to \\nexpress and perform transformations using matrix multiplication. \\nFor a 2D point (x,y)(x, y)(x,y) in Cartesian coordinates, the homogeneous representation is given as:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='How Homogeneous Coordinates Simplify Transformations \\nHomogeneous coordinates enable all transformations (translation, rotation, scaling, and perspective \\nprojection) to be expressed using matrix multiplication. In standard Cartesian coordinates, \\ntranslation cannot be performed using matrix operations alone, but in homogeneous form, all \\ntransformations become linear operations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Merits of Homogeneous Representation \\n1. Unified Matrix Representation for All Transformations \\no Rotation, scaling, translation, and projection can be performed using 4×4 matrices in \\n3D graphics, enabling efficient pipeline processing in graphics hardware. \\n2. Efficient Computation Using Matrix Multiplication \\no Since all transformations are represented as matrix multiplications, multiple \\ntransformations can be combined into a single matrix before applying them to a \\npoint or object. \\n3. Easier Composition of Transformations \\no Instead of applying transformations sequentially (which is computationally \\nexpensive), they can be pre-multiplied to obtain a composite transformation matrix. \\n4. Supports Perspective Projection \\no Homogeneous coordinates allow perspective transformations using a w-component, \\nwhich cannot be achieved using simple Cartesian coordinates. \\n5. Simplifies Complex Geometric Operations'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 7, 'page_label': '8', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Homogeneous coordinates allow perspective transformations using a w-component, \\nwhich cannot be achieved using simple Cartesian coordinates. \\n5. Simplifies Complex Geometric Operations \\no Operations such as shearing, mirroring, and 3D clipping are easier to perform in \\nhomogeneous coordinates. \\n6. Optimized for Computer Graphics Pipelines \\no Modern GPU architectures and graphics APIs (OpenGL, DirectX) use homogeneous \\ncoordinates to efficiently process and render 3D objects. \\n7. Allows Infinite Points Representation \\no When w=0w = 0w=0, it represents a point at infinity, which is useful for handling \\nparallel lines and projections in computer graphics. \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 8, 'page_label': '9', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Homogeneous coordinates play a crucial role in computer graphics by simplifying transformations, \\nunifying geometric operations, and optimizing rendering efficiency. They enable complex \\ntransformations using simple matrix multiplications, making them a fundamental concept in 2D and \\n3D graphics applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(15marks) \\n7.Identify the various OpenGL functions used to create and manipulate points, lines, and filled areas \\nin 2D.(15) provide detailed answer for 15 markks \\nChatGPT said: \\nOpenGL Functions for Creating and Manipulating Points, Lines, and Filled Areas in 2D \\nOpenGL provides a set of functions to create and manipulate points, lines, and filled areas in 2D \\ngraphics. These functions are essential for rendering basic geometric shapes and building complex \\ngraphics applications. \\n \\n1. Functions for Creating and Manipulating Points \\nPoints are the simplest geometric primitives in OpenGL. A point is a single pixel on the screen. \\na) glBegin(GL_POINTS) and glEnd() \\nUsed to define and draw a set of individual points. Each point is specified using glVertex*(). \\ncpp \\nCopyEdit \\nglBegin(GL_POINTS);  \\n    glVertex2f(0.5, 0.5);  // Draw a point at (0.5, 0.5) \\n    glVertex2f(-0.5, -0.5); // Another point at (-0.5, -0.5) \\nglEnd(); \\nb) glPointSize(size) \\nControls the size of the points. \\ncpp'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glVertex2f(0.5, 0.5);  // Draw a point at (0.5, 0.5) \\n    glVertex2f(-0.5, -0.5); // Another point at (-0.5, -0.5) \\nglEnd(); \\nb) glPointSize(size) \\nControls the size of the points. \\ncpp \\nCopyEdit \\nglPointSize(5.0); // Sets point size to 5 pixels \\nc) glEnable(GL_POINT_SMOOTH) \\nEnables anti-aliasing for smooth points. \\ncpp \\nCopyEdit \\nglEnable(GL_POINT_SMOOTH); \\n \\n2. Functions for Creating and Manipulating Lines'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 10, 'page_label': '11', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Lines are fundamental graphics primitives used in drawing outlines, wireframes, and geometric \\nstructures. \\na) glBegin(GL_LINES) and glEnd() \\nDraws individual line segments between pairs of points. \\ncpp \\nCopyEdit \\nglBegin(GL_LINES); \\n    glVertex2f(-0.5, 0.0); // Start point \\n    glVertex2f(0.5, 0.0);  // End point \\nglEnd(); \\nb) glBegin(GL_LINE_STRIP) \\nDraws connected lines without closing the shape. \\ncpp \\nCopyEdit \\nglBegin(GL_LINE_STRIP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\n    glVertex2f(0.5, -0.5); \\nglEnd(); \\nc) glBegin(GL_LINE_LOOP) \\nDraws a closed polygonal shape with connected lines. \\ncpp \\nCopyEdit \\nglBegin(GL_LINE_LOOP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\nd) glLineWidth(width) \\nSets the width of the line in pixels.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 11, 'page_label': '12', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='cpp \\nCopyEdit \\nglLineWidth(3.0); // Sets the line thickness to 3 pixels \\ne) glEnable(GL_LINE_SMOOTH) \\nEnables anti-aliasing for smoother lines. \\ncpp \\nCopyEdit \\nglEnable(GL_LINE_SMOOTH); \\n \\n3. Functions for Creating and Manipulating Filled Areas (Polygons) \\nPolygons are used to represent filled areas such as rectangles, triangles, and complex shapes. \\na) glBegin(GL_TRIANGLES) \\nDraws a set of independent triangles (each group of three vertices forms a triangle). \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLES); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\nglEnd(); \\nb) glBegin(GL_TRIANGLE_STRIP) \\nDraws a connected series of triangles, reducing the number of vertices needed. \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLE_STRIP); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.0, 0.5); \\n    glVertex2f(0.5, 0.5); \\nglEnd(); \\nc) glBegin(GL_TRIANGLE_FAN)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Similar to a triangle strip, but all triangles share a common starting vertex. \\ncpp \\nCopyEdit \\nglBegin(GL_TRIANGLE_FAN); \\n    glVertex2f(0.0, 0.5);  // Common center \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\nglEnd(); \\nd) glBegin(GL_QUADS) \\nDraws independent quadrilaterals (each four vertices form a closed shape). \\ncpp \\nCopyEdit \\nglBegin(GL_QUADS); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.5, 0.5); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\ne) glBegin(GL_POLYGON) \\nDraws a general polygon with more than four vertices. \\ncpp \\nCopyEdit \\nglBegin(GL_POLYGON); \\n    glVertex2f(-0.5, -0.5); \\n    glVertex2f(0.5, -0.5); \\n    glVertex2f(0.7, 0.2); \\n    glVertex2f(0.3, 0.6); \\n    glVertex2f(-0.5, 0.5); \\nglEnd(); \\nf) glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 13, 'page_label': '14', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Specifies whether the polygons should be drawn as filled, wireframe, or points. \\ncpp \\nCopyEdit \\nglPolygonMode(GL_FRONT_AND_BACK, GL_FILL); // Default: filled \\nglPolygonMode(GL_FRONT_AND_BACK, GL_LINE); // Wireframe mode \\nglPolygonMode(GL_FRONT_AND_BACK, GL_POINT); // Points only \\n \\n4. Additional OpenGL Functions for Manipulation \\na) glColor3f(r, g, b) – Sets Color \\nColors the primitives (values range from 0.0 to 1.0). \\ncpp \\nCopyEdit \\nglColor3f(1.0, 0.0, 0.0); // Red color \\nb) glViewport(x, y, width, height) \\nDefines the 2D rendering area on the screen. \\ncpp \\nCopyEdit \\nglViewport(0, 0, 800, 600); // Sets viewport to 800x600 pixels \\nc) glClear(GL_COLOR_BUFFER_BIT) \\nClears the screen before drawing. \\ncpp \\nCopyEdit \\nglClear(GL_COLOR_BUFFER_BIT); \\nd) glFlush() \\nForces immediate execution of OpenGL commands. \\ncpp \\nCopyEdit \\nglFlush(); \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='OpenGL provides a rich set of functions for creating and manipulating points, lines, and filled \\npolygons in 2D graphics. These functions allow for efficient rendering and manipulation of shapes \\nusing vertex-based commands, matrix transformations, and rasterization techniques. By combining \\nthese basic elements, complex graphics applications can be developed efficiently. \\n4o \\nYou said: \\nfor the above question provide one whole program as a example \\nChatGPT said: \\nHere is a complete OpenGL program that demonstrates how to draw and manipulate points, lines, \\nand filled polygons in 2D using OpenGL and GLUT. \\nThis program: \\n   Draws a point \\n   Draws different types of lines (single lines, line strip, line loop) \\n   Draws filled polygons (triangles, quads, and a general polygon) \\n   Uses colors and transformations \\n \\nComplete OpenGL Program in C++ \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n \\n// Function to initialize OpenGL settings \\nvoid init() {'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 14, 'page_label': '15', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Uses colors and transformations \\n \\nComplete OpenGL Program in C++ \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n \\n// Function to initialize OpenGL settings \\nvoid init() { \\n    glClearColor(1.0, 1.0, 1.0, 1.0); // White background \\n    glMatrixMode(GL_PROJECTION); \\n    gluOrtho2D(-1.0, 1.0, -1.0, 1.0); // Set coordinate system \\n} \\n \\n// Function to draw points, lines, and polygons \\nvoid display() { \\n    glClear(GL_COLOR_BUFFER_BIT); // Clear screen \\n     \\n    // **Draw a Point** \\n    glPointSize(5.0); // Set point size'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glColor3f(1.0, 0.0, 0.0); // Red color \\n    glBegin(GL_POINTS); \\n        glVertex2f(0.0, 0.8); \\n    glEnd(); \\n \\n    // **Draw a Line** \\n    glLineWidth(3.0); // Set line width \\n    glColor3f(0.0, 0.0, 1.0); // Blue color \\n    glBegin(GL_LINES); \\n        glVertex2f(-0.8, 0.5); \\n        glVertex2f(0.8, 0.5); \\n    glEnd(); \\n \\n    // **Draw a Line Strip** \\n    glColor3f(0.0, 0.7, 0.3); // Greenish color \\n    glBegin(GL_LINE_STRIP); \\n        glVertex2f(-0.8, 0.2); \\n        glVertex2f(-0.4, 0.3); \\n        glVertex2f(0.0, 0.2); \\n        glVertex2f(0.4, 0.3); \\n        glVertex2f(0.8, 0.2); \\n    glEnd(); \\n \\n    // **Draw a Triangle** \\n    glColor3f(1.0, 0.5, 0.0); // Orange color \\n    glBegin(GL_TRIANGLES); \\n        glVertex2f(-0.6, -0.2); \\n        glVertex2f(0.0, -0.2); \\n        glVertex2f(-0.3, -0.6); \\n    glEnd();'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 16, 'page_label': '17', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='// **Draw a Quad (Rectangle)** \\n    glColor3f(0.5, 0.0, 0.5); // Purple color \\n    glBegin(GL_QUADS); \\n        glVertex2f(0.2, -0.2); \\n        glVertex2f(0.6, -0.2); \\n        glVertex2f(0.6, -0.6); \\n        glVertex2f(0.2, -0.6); \\n    glEnd(); \\n \\n    // **Draw a Polygon** \\n    glColor3f(0.0, 0.7, 0.7); // Cyan color \\n    glBegin(GL_POLYGON); \\n        glVertex2f(-0.2, -0.8); \\n        glVertex2f(0.0, -0.7); \\n        glVertex2f(0.2, -0.8); \\n        glVertex2f(0.3, -0.9); \\n        glVertex2f(0.0, -1.0); \\n        glVertex2f(-0.3, -0.9); \\n    glEnd(); \\n \\n    glFlush(); // Force execution \\n} \\n \\n// Main function to initialize GLUT and start the program \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"OpenGL 2D Drawing Example\"); \\n \\n    init(); // Initialize OpenGL settings'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glutDisplayFunc(display); // Set display callback function \\n    glutMainLoop(); // Start the main event loop \\n \\n    return 0; \\n} \\n \\nExplanation of the Program \\n1. Initialization (init()) \\no Sets the background color to white. \\no Defines the coordinate system using gluOrtho2D(), setting the range from (-1,1) for \\nboth X and Y axes. \\n2. Display Function (display()) \\no Clears the screen before drawing. \\no Draws a single point at (0.0, 0.8) with glPointSize(5.0). \\no Draws a single line from (-0.8, 0.5) to (0.8, 0.5). \\no Draws a line strip, forming a connected zigzag pattern. \\no Draws a filled triangle, colored orange. \\no Draws a filled rectangle (quad), colored purple. \\no Draws a polygon, resembling a star-like shape. \\n3. GLUT Main Loop (main()) \\no Initializes GLUT. \\no Creates a 500x500 pixel window. \\no Calls init() for OpenGL settings. \\no Starts the event loop with glutMainLoop(), which continuously listens for display \\nevents. \\n \\nExpected Output'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 17, 'page_label': '18', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='o Creates a 500x500 pixel window. \\no Calls init() for OpenGL settings. \\no Starts the event loop with glutMainLoop(), which continuously listens for display \\nevents. \\n \\nExpected Output \\nWhen you run this program, you will see:    A red point at the top \\n   A blue horizontal line in the middle \\n   A green zigzag line strip \\n   An orange triangle on the left \\n   A purple rectangle on the right \\n   A cyan star-like polygon at the bottom'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 18, 'page_label': '19', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='How to Compile and Run \\nFor Windows (MinGW) \\n1. Install MinGW and FreeGLUT. \\n2. Compile using:  \\nlua \\nCopyEdit \\ng++ filename.cpp -o output.exe -lfreeglut -lopengl32 -lglu32 \\n3. Run:  \\nlua \\nCopyEdit \\noutput.exe \\nFor Linux (g++) \\n1. Install OpenGL libraries:  \\narduino \\nCopyEdit \\nsudo apt-get install freeglut3-dev \\n2. Compile:  \\nlua \\nCopyEdit \\ng++ filename.cpp -o output -lGL -lGLU -lglut \\n3. Run:  \\nbash \\nCopyEdit \\n./output \\n \\nConclusion \\nThis program demonstrates all major OpenGL primitives used in 2D graphics, including points, lines, \\nand filled polygons. You can modify colors, shapes, and positions to experiment with different \\ngraphics rendering techniques.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(7 marks) \\n11. Build the Digital Differential Analyzer line drawing algorithm in detail.(7) provide detailed answer \\nChatGPT said: \\nDigital Differential Analyzer (DDA) Line Drawing Algorithm \\nThe Digital Differential Analyzer (DDA) algorithm is a fundamental rasterization algorithm used in \\ncomputer graphics to draw lines between two points. It is based on incremental calculations and is \\nwidely used due to its simplicity and efficiency. \\n \\nConcept of DDA Algorithm \\nThe DDA algorithm works by calculating intermediate points between the start and end points of a \\nline and plotting them pixel by pixel. It uses the equation of a straight line: \\ny=mx+cy = mx + cy=mx+c  \\nwhere m is the slope of the line, given by: \\nm=(y2−y1)(x2−x1)m = \\\\frac{(y_2 - y_1)}{(x_2 - x_1)}m=(x2−x1)(y2−y1)  \\nInstead of using floating-point arithmetic and multiplication, the algorithm increments either xxx or \\nyyy by a fixed step size and calculates the corresponding other coordinate. \\n \\nSteps of the DDA Algorithm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='yyy by a fixed step size and calculates the corresponding other coordinate. \\n \\nSteps of the DDA Algorithm \\n1. Input the start point (x1,y1)(x_1, y_1)(x1,y1) and end point (x2,y2)(x_2, y_2)(x2,y2). \\n2. Compute dx and dy: dx=x2−x1,dy=y2−y1dx = x_2 - x_1, \\\\quad dy = y_2 - y_1dx=x2−x1,dy=y2\\n−y1  \\n3. Find the step count (maximum change in either x or y): steps=max\\u2061(∣dx∣,∣dy∣)\\\\text{steps} = \\n\\\\max(|dx|, |dy|)steps=max(∣dx∣,∣dy∣)  \\n4. Compute the incremental step values: \\nXincrement=dxsteps,Yincrement=dystepsX_{\\\\text{increment}} = \\\\frac{dx}{\\\\text{steps}}, \\\\quad \\nY_{\\\\text{increment}} = \\\\frac{dy}{\\\\text{steps}}Xincrement=stepsdx,Yincrement=stepsdy  \\n5. Initialize starting position at (x1,y1)(x_1, y_1)(x1,y1). \\n6. Iterate through the step count, rounding the values to the nearest integer and plotting the \\npixel. \\n7. Repeat until the end point is reached. \\n \\nDDA Algorithm Implementation in C++ (Using OpenGL)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 19, 'page_label': '20', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='pixel. \\n7. Repeat until the end point is reached. \\n \\nDDA Algorithm Implementation in C++ (Using OpenGL) \\nHere’s a complete C++ program that implements the DDA line drawing algorithm using OpenGL: \\ncpp'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='CopyEdit \\n#include <GL/glut.h> \\n#include <cmath> \\n#include <iostream> \\n \\nusing namespace std; \\n \\n// Function to implement the DDA line drawing algorithm \\nvoid drawDDA(float x1, float y1, float x2, float y2) { \\n    float dx = x2 - x1; \\n    float dy = y2 - y1; \\n     \\n    int steps = max(abs(dx), abs(dy));  // Find the number of steps \\n     \\n    float Xinc = dx / steps;  // Increment in x per step \\n    float Yinc = dy / steps;  // Increment in y per step \\n     \\n    float x = x1, y = y1; \\n     \\n    glBegin(GL_POINTS); \\n    for (int i = 0; i <= steps; i++) { \\n        glVertex2f(round(x), round(y));  // Plot the point \\n        x += Xinc;  // Increment x \\n        y += Yinc;  // Increment y \\n    } \\n    glEnd(); \\n} \\n \\n// Display function for OpenGL \\nvoid display() { \\n    glClear(GL_COLOR_BUFFER_BIT);'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 21, 'page_label': '22', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='glColor3f(1.0, 0.0, 0.0);  // Red color \\n    drawDDA(-0.5, -0.5, 0.5, 0.5);  // Draw a diagonal line \\n     \\n    glFlush(); \\n} \\n \\n// Initialize OpenGL settings \\nvoid init() { \\n    glClearColor(1.0, 1.0, 1.0, 1.0);  // Set background to white \\n    glMatrixMode(GL_PROJECTION); \\n    gluOrtho2D(-1, 1, -1, 1);  // Set 2D coordinate system \\n} \\n \\n// Main function to set up GLUT and OpenGL \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"DDA Line Drawing Algorithm\"); \\n \\n    init(); \\n    glutDisplayFunc(display); \\n    glutMainLoop(); \\n    return 0; \\n} \\n \\nMerits of DDA Algorithm \\n   Fast and efficient: Uses simple addition operations instead of complex multiplications. \\n   Straightforward implementation: Easy to implement with basic calculations. \\n   Works for all slopes: Can draw lines with any inclination.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 22, 'page_label': '23', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='Limitations of DDA Algorithm \\n  Floating-point rounding errors: Since floating-point numbers are used, rounding may introduce \\nsmall inaccuracies. \\n  Slower than Bresenham’s Algorithm: Uses floating-point operations which can be \\ncomputationally expensive. \\n  Not perfectly optimized for raster displays: May produce gaps or jagged lines if not rounded \\nproperly. \\n \\nConclusion \\nThe DDA algorithm is a simple and effective method for drawing lines in computer graphics. It \\nefficiently interpolates pixel positions using incremental calculations, making it widely used in \\ngraphics rendering. However, for performance-critical applications, Bresenham’s Line Algorithm is \\noften preferred due to its integer-based calculations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='(8 marks) \\nApply the concept of clipping and the importance of the clipping window in computer graphics. \\nExplain how clipping is performed in OpenGL, including the types of clipping available (point, line, \\npolygon clipping).(8) provide detailed answer \\nChatGPT said: \\nClipping in Computer Graphics \\nIntroduction \\nClipping is a crucial process in computer graphics that determines which parts of objects (such as \\npoints, lines, and polygons) are visible within a designated region, called the clipping window. Any \\nportions that lie outside this window are removed to improve rendering efficiency and maintain \\ncorrect visualization. \\nClipping is essential for: \\n• Optimizing performance by eliminating unnecessary computations. \\n• Ensuring a proper display by keeping only the relevant portions of objects. \\n• Managing object visibility within a viewport or screen. \\n \\nConcept of the Clipping Window'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content=\"• Ensuring a proper display by keeping only the relevant portions of objects. \\n• Managing object visibility within a viewport or screen. \\n \\nConcept of the Clipping Window \\nA clipping window is a rectangular or defined boundary that sets the limits for rendering objects. \\nAny graphical entity outside this boundary is clipped (discarded). This boundary could represent a \\nviewport, screen area, or a defined viewing region in a graphical application. \\nThe clipping window is typically defined in: \\n• 2D space (rectangular) – Used in raster graphics for UI elements, maps, and simple \\nrendering. \\n• 3D space (viewing frustum) – Used in 3D graphics to define the camera's perspective. \\n \\nTypes of Clipping in Computer Graphics \\nClipping is applied to different types of graphical objects, and the most common types include: \\n1. Point Clipping \\n• Determines whether a point lies inside or outside the clipping region. \\n• A point (x,y)(x, y)(x,y) is visible if it satisfies the window boundaries:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='1. Point Clipping \\n• Determines whether a point lies inside or outside the clipping region. \\n• A point (x,y)(x, y)(x,y) is visible if it satisfies the window boundaries: \\nxmin≤x≤xmax,ymin≤y≤ymaxx_{\\\\text{min}} \\\\leq x \\\\leq x_{\\\\text{max}}, \\\\quad y_{\\\\text{min}} \\n\\\\leq y \\\\leq y_{\\\\text{max}}xmin≤x≤xmax,ymin≤y≤ymax  \\n• Used in applications like point-based selections (mouse clicks, scatter plots, etc.). \\n2. Line Clipping (Cohen-Sutherland & Liang-Barsky Algorithms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='• Determines whether a line is fully visible, partially visible, or completely outside the \\nclipping region. \\n• If partially visible, the intersecting points are computed, and only the visible segment is \\ndrawn. \\n• Algorithms used:  \\no Cohen-Sutherland Line Clipping – Uses region codes to classify line segments. \\no Liang-Barsky Algorithm – More efficient as it reduces computations using parametric \\nequations. \\n3. Polygon Clipping (Sutherland-Hodgman Algorithm) \\n• Used to clip polygons against the viewport. \\n• The Sutherland-Hodgman algorithm clips a polygon edge-by-edge against the clipping \\nwindow. \\n• Produces new clipped vertices to form a visible polygon. \\n• Used in maps, gaming, and rendering optimizations. \\n4. Text Clipping \\n• Ensures that only the visible portion of text is rendered within a defined region. \\n• Used in UI development (text fields, labels). \\n5. Curve Clipping \\n• Similar to line clipping but applied to curved shapes like circles, ellipses, and Bezier curves.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 24, 'page_label': '25', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='• Used in UI development (text fields, labels). \\n5. Curve Clipping \\n• Similar to line clipping but applied to curved shapes like circles, ellipses, and Bezier curves. \\n• Uses bounding boxes or incremental approximations. \\n \\nClipping in OpenGL \\nOpenGL provides automatic clipping based on the defined viewing volume. The clipping process \\noccurs in Normalized Device Coordinates (NDC), meaning anything outside the canonical clipping \\nvolume (from -1 to 1 in X, Y , and Z) is discarded. \\nHow Clipping is Performed in OpenGL \\n1. Defining the Clipping Region \\nOpenGL uses glOrtho() or gluOrtho2D() for 2D clipping and glFrustum() for 3D perspective \\nclipping. \\ncpp \\nCopyEdit \\nglMatrixMode(GL_PROJECTION); \\nglLoadIdentity(); \\ngluOrtho2D(-100, 100, -100, 100);  // Defines a clipping window from (-100, -100) to (100, 100)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 25, 'page_label': '26', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='2. Applying Viewport Transformation \\nThe viewport maps the clipping region to a screen coordinate system: \\ncpp \\nCopyEdit \\nglViewport(0, 0, 800, 600); \\n3. Automatic Clipping by OpenGL \\no Any primitives outside the defined view volume are automatically clipped. \\no OpenGL uses a clipping plane to discard objects outside the NDC space. \\n4. Custom Clipping Planes in OpenGL \\nOpenGL allows defining custom clipping planes using glClipPlane(): \\ncpp \\nCopyEdit \\ndouble planeEq[] = {1.0, 0.0, 0.0, -50.0};  // Clipping plane along x = 50 \\nglEnable(GL_CLIP_PLANE0); \\nglClipPlane(GL_CLIP_PLANE0, planeEq); \\n \\nExample: Line Clipping using Cohen-Sutherland Algorithm in OpenGL \\nBelow is a C++ program that implements line clipping using the Cohen-Sutherland algorithm in \\nOpenGL. \\ncpp \\nCopyEdit \\n#include <GL/glut.h> \\n#include <iostream> \\n \\nusing namespace std; \\n \\n// Clipping window boundaries \\n#define X_MIN -50 \\n#define X_MAX 50 \\n#define Y_MIN -50 \\n#define Y_MAX 50'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 26, 'page_label': '27', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='// Region codes \\nconst int INSIDE = 0, LEFT = 1, RIGHT = 2, BOTTOM = 4, TOP = 8; \\n \\n// Function to compute region code for a point (x, y) \\nint computeCode(float x, float y) { \\n    int code = INSIDE; \\n    if (x < X_MIN) code |= LEFT; \\n    else if (x > X_MAX) code |= RIGHT; \\n    if (y < Y_MIN) code |= BOTTOM; \\n    else if (y > Y_MAX) code |= TOP; \\n    return code; \\n} \\n \\n// Cohen-Sutherland Line Clipping Algorithm \\nvoid cohenSutherlandClip(float x1, float y1, float x2, float y2) { \\n    int code1 = computeCode(x1, y1); \\n    int code2 = computeCode(x2, y2); \\n    bool accept = false; \\n \\n    while (true) { \\n        if ((code1 == 0) && (code2 == 0)) { \\n            accept = true; \\n            break; \\n        } else if (code1 & code2) { \\n            break; \\n        } else { \\n            int code_out; \\n            float x, y; \\n \\n            code_out = (code1 != 0) ? code1 : code2;'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='if (code_out & TOP) { \\n                x = x1 + (x2 - x1) * (Y_MAX - y1) / (y2 - y1); \\n                y = Y_MAX; \\n            } else if (code_out & BOTTOM) { \\n                x = x1 + (x2 - x1) * (Y_MIN - y1) / (y2 - y1); \\n                y = Y_MIN; \\n            } else if (code_out & RIGHT) { \\n                y = y1 + (y2 - y1) * (X_MAX - x1) / (x2 - x1); \\n                x = X_MAX; \\n            } else if (code_out & LEFT) { \\n                y = y1 + (y2 - y1) * (X_MIN - x1) / (x2 - x1); \\n                x = X_MIN; \\n            } \\n \\n            if (code_out == code1) { \\n                x1 = x; y1 = y; code1 = computeCode(x1, y1); \\n            } else { \\n                x2 = x; y2 = y; code2 = computeCode(x2, y2); \\n            } \\n        } \\n    } \\n \\n    if (accept) { \\n        glBegin(GL_LINES); \\n        glVertex2f(x1, y1); \\n        glVertex2f(x2, y2); \\n        glEnd(); \\n    } \\n} \\n \\n// Display function'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-22T08:58:16+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-22T08:58:16+05:30', 'source': '..\\\\data\\\\pdf\\\\15 marks VDA.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29', 'source_file': '15 marks VDA.pdf', 'file_type': 'pdf'}, page_content='void display() { \\n    glClear(GL_COLOR_BUFFER_BIT); \\n    glColor3f(1.0, 0.0, 0.0); \\n    cohenSutherlandClip(-70, -30, 60, 40); \\n    glFlush(); \\n} \\n \\n// Initialize OpenGL \\nvoid init() { \\n    gluOrtho2D(-100, 100, -100, 100); \\n} \\n \\n// Main function \\nint main(int argc, char** argv) { \\n    glutInit(&argc, argv); \\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); \\n    glutInitWindowSize(500, 500); \\n    glutCreateWindow(\"Line Clipping - Cohen Sutherland\"); \\n \\n    init(); \\n    glutDisplayFunc(display); \\n    glutMainLoop(); \\n    return 0; \\n} \\n \\nConclusion \\nClipping is an essential technique in computer graphics that improves performance by removing \\nunnecessary parts of objects. OpenGL provides built-in clipping for optimized rendering, while \\nadvanced algorithms like Cohen-Sutherland and Sutherland-Hodgman offer efficient clipping \\nsolutions for lines and polygons.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. How MapReduce Computation is Executed? \\nMapReduce executes in two main stages: Map and Reduce. \\n1. Map Stage: The input data is split into chunks and processed by Mapper tasks in parallel. \\nEach Mapper processes a portion of the data and outputs key-value pairs. \\n2. Shuffle & Sort: The output of Mappers is sorted and grouped by key. The data is then \\nshuffled to appropriate Reducers. \\n3. Reduce Stage: The Reducer aggregates and processes data by key, generating the final \\noutput. \\nExample: Word count in MapReduce: \\n• Mapper: (\"Hadoop is great\") → [(Hadoop, 1), (is, 1), (great, 1)] \\n• Shuffle & Sort: Groups values by keys → [(Hadoop, [1,1,1]), (great, [1]), (is, [1,1])] \\n• Reducer: Aggregates counts → [(Hadoop, 3), (great, 1), (is, 2)] \\nMapReduce computation is executed through two main phases: a Map phase where data is \\nprocessed in parallel, and a Reduce phase where the results from the Map phase are aggregated and \\nprocessed to produce a final output.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='processed in parallel, and a Reduce phase where the results from the Map phase are aggregated and \\nprocessed to produce a final output.  \\nHere\\'s a more detailed breakdown: \\n1. Map Phase: \\n• Input: \\nThe MapReduce framework takes a large dataset as input, which is typically stored in a distributed \\nfile system like HDFS (Hadoop Distributed File System).  \\n• Splitting: \\nThe input data is divided into smaller chunks or \"splits\" that can be processed independently by \\ndifferent nodes in a cluster.  \\n• Mapping: \\nEach split is assigned to a mapper process, which applies a user-defined \"map\" function to each \\nrecord within the split.  \\n• Key-Value Pairs: \\nThe map function typically transforms the input data into key-value pairs.  \\n• Intermediate Output: \\nThe mappers produce a set of intermediate key-value pairs as output, which are then passed to the \\nshuffle and sort phase.  \\n2. Shuffle and Sort Phase: \\n• Shuffling:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Intermediate Output: \\nThe mappers produce a set of intermediate key-value pairs as output, which are then passed to the \\nshuffle and sort phase.  \\n2. Shuffle and Sort Phase: \\n• Shuffling: \\nThe intermediate key-value pairs from all mappers are shuffled and partitioned based on the keys.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Sorting: \\nThe framework sorts the shuffled key-value pairs, ensuring that all pairs with the same key are \\ngrouped together.  \\n3. Reduce Phase: \\n• Input: The sorted key-value pairs are passed to the reducer processes.  \\n• Reducing: Each reducer receives a group of key-value pairs with the same key.  \\n• Aggregation: The user-defined \"reduce\" function processes these grouped values and \\nproduces a final set of key-value pairs as output.  \\n• Output: The final output from the reducers is stored in the distributed file system.  \\nIn summary, MapReduce is a distributed computing model that breaks down large data processing \\ntasks into smaller, parallelizable tasks (the map phase) and then combines the results (the reduce \\nphase) to produce a final output.  \\n2. Show the key advantages in Hadoop. \\nHadoop offers several key advantages, including scalability for handling massive \\ndatasets, cost-effectiveness through commodity hardware, fault tolerance with'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"2. Show the key advantages in Hadoop. \\nHadoop offers several key advantages, including scalability for handling massive \\ndatasets, cost-effectiveness through commodity hardware, fault tolerance with \\ndata replication, and flexibility in handling various data types.  \\nHere's a more detailed breakdown of Hadoop's key advantages: \\n• Scalability: \\nHadoop's architecture allows for horizontal scaling, meaning you can add more \\nnodes to the cluster to handle increasing data volumes and processing needs \\nwithout significant infrastructure changes.  \\n• Cost-Effectiveness: \\nHadoop is designed to run on inexpensive, standard hardware, making it a cost-\\neffective solution compared to traditional, expensive data storage and processing \\nsystems.  \\n• Fault Tolerance: \\nHadoop replicates data across multiple nodes, ensuring that data is not lost if one \\nor more nodes fail.  \\n• Flexibility: \\nHadoop can handle various data types, including structured, semi-structured, and\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 1, 'page_label': '2', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"or more nodes fail.  \\n• Flexibility: \\nHadoop can handle various data types, including structured, semi-structured, and \\nunstructured data, making it a versatile platform for diverse data analytics needs.  \\n• Data Locality: \\nHadoop's architecture allows for processing data where it resides, minimizing data \\nmovement and improving performance.  \\n• Open Source: \\nHadoop is an open-source project, meaning its source code is freely available and \\ncan be modified and distributed.  \\n• YARN (Yet Another Resource Negotiator):\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='YARN is a resource management system that enables efficient resource allocation \\nand management within the Hadoop ecosystem, allowing for better utilization of \\ncluster resources.  \\n• Data Governance and Management: \\nHadoop provides tools and features for data governance, including metadata \\nmanagement, data lineage, and security controls.  \\n• Security: \\nHadoop offers robust security features, including authentication, authorization, \\nand encryption, to protect sensitive data.  \\n• Data Lake: \\nHadoop can be used as a foundation for building data lakes, which are centralized \\nrepositories for storing diverse data formats and sources.  \\n3. What is the purpose of the NodeManager in \\nYARN? \\nThe NodeManager in YARN is a per-machine agent responsible for launching and \\nmanaging containers on a worker node, monitoring their resource usage, and reporting \\nthis information to the ResourceManager \\nThe NodeManager is responsible for: \\n• Monitoring resource usage (CPU, memory) on a node.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 2, 'page_label': '3', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='this information to the ResourceManager \\nThe NodeManager is responsible for: \\n• Monitoring resource usage (CPU, memory) on a node. \\n• Managing containers for executing tasks. \\n• Reporting node health to the ResourceManager. \\nExample: In a Hadoop cluster, each server runs a NodeManager to manage local \\ncomputation tasks and communicate with the ResourceManager. \\n4. Explain the main components of YARN. \\n4. Main Components of YARN \\n1. ResourceManager (RM) – Allocates resources across the cluster. \\n2. NodeManager (NM) – Monitors nodes and manages task execution. \\n3. ApplicationMaster (AM) – Manages the execution of an application. \\n4. Container – A runtime environment that executes tasks. \\nExample: A Spark job requests resources from the RM, which assigns it to a \\nNodeManager to execute the tasks within containers. \\n5. How does MapReduce achieve parallel \\nprocessing?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 3, 'page_label': '4', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='MapReduce achieves parallel processing by dividing large datasets into smaller chunks, \\ndistributing these chunks across multiple machines for processing (the \"Map\" phase), \\nand then aggregating the results from these machines (the \"Reduce\" phase).  \\nHere\\'s a more detailed explanation: \\n• Data Splitting: \\nThe input data is split into smaller, manageable chunks, which can be processed \\nindependently by different machines.  \\n• Map Phase: \\nEach chunk of data is processed independently by a \"mapper\" function, which \\ntransforms the data into key-value pairs.  \\n• Reduce Phase: \\nThe \"reducer\" function then takes the key-value pairs generated by the mappers and \\naggregates them to produce the final output.  \\n• Parallelism: \\nThe Map and Reduce phases can be executed in parallel across multiple machines, \\nenabling efficient processing of large datasets.  \\n• Fault Tolerance: \\nMapReduce is designed to be fault-tolerant, meaning that if a machine fails, the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 3, 'page_label': '4', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='enabling efficient processing of large datasets.  \\n• Fault Tolerance: \\nMapReduce is designed to be fault-tolerant, meaning that if a machine fails, the \\nMapReduce framework can automatically reassign the tasks to other machines, \\nensuring that the job continues to run.  \\n• Hadoop Ecosystem: \\nMapReduce is a fundamental part of the Apache Hadoop ecosystem, a distributed \\ncomputing framework designed for processing large datasets.  \\n• Data Access and Storage: \\nMapReduce uses Hadoop Distributed File System (HDFS) for both input and output.  \\n6. What is the role of the \\'Mapper\\' in MapReduce? \\n• Takes input data and converts it into key-value pairs. \\n• Runs in parallel on multiple nodes. \\n• Sends intermediate output to the Reduce phase. \\nExample: In a sales dataset, the Mapper outputs (city, sales): \\n• Input: (\"New York, 200\"), (\"Los Angeles, 150\") \\n• Output: [(New York, 200), (Los Angeles, 150)]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"7.Justify the need for replication in HDFS. \\nHDFS data replication is the process of creating multiple copies (replicas) of data blocks \\nacross different DataNodes in the HDFS cluster. This redundancy ensures that if one or more \\nDataNodes fail, the data can still be accessed from the remaining replicas, providing high \\navailability and fault tolerance. \\n8. Purpose of Secondary NameNode in HDFS \\n• Periodically merges edit logs with the namespace image to prevent the NameNode \\nfrom being overloaded. \\n• Acts as a checkpointing mechanism but not a failover node. \\nExample: If the NameNode fails, the Secondary NameNode’s last checkpointed metadata \\ncan be used to restore the system. \\nIn HDFS, the secondary namenode's primary purpose is to act as a helper node for the \\nprimary namenode, performing periodic checkpoints of the namespace and merging edit \\nlogs with the fsimage to prevent the edit logs from growing too large, which can lead to \\nslow NameNode restarts.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"logs with the fsimage to prevent the edit logs from growing too large, which can lead to \\nslow NameNode restarts.  \\n9,12. Explain the steps in map reduce algorithm. \\nThe MapReduce algorithm consists of two main phases: Map and Reduce, with an \\nintermediate shuffle and sort step. The map phase processes data into key-value pairs, \\nwhich are then shuffled and sorted before the reduce phase aggregates the data.  \\nHere's a more detailed breakdown: \\n1. Input Splitting:  \\n• The input data is split into smaller chunks, which are then assigned to different \\nworker nodes in the cluster. \\n• These chunks are processed independently by the mapper functions. \\n2. Map Phase: \\n• Each mapper function receives a chunk of data as input.  \\n• The mapper processes the data and produces a set of intermediate key-value pairs.  \\n• The output of the map phase is a list of key-value pairs.  \\n3. Shuffle and Sort:  \\n• The intermediate key-value pairs are grouped and shuffled based on their keys.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 4, 'page_label': '5', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• The output of the map phase is a list of key-value pairs.  \\n3. Shuffle and Sort:  \\n• The intermediate key-value pairs are grouped and shuffled based on their keys. \\n• The keys are sorted, ensuring that all occurrences of the same key are grouped \\ntogether. \\n4. Reduce Phase: \\n• The reducer functions receive the shuffled and sorted key-value pairs as input.  \\n• The reducer aggregates the values associated with the same key, producing the final \\noutput.  \\nSteps in the MapReduce Algorithm \\n1. Input Splitting – Data is divided into chunks. \\n2. Mapping – Mappers process chunks and output key-value pairs. \\n3. Shuffling & Sorting – Intermediate key-value pairs are grouped. \\n4. Reducing – Reducers aggregate data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 5, 'page_label': '6', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Final Output – Processed data is written to HDFS. \\nExample: Counting occurrences of words in a text file follows these steps. \\n11. Differentiate between HDFS and a traditional file \\nsystem. \\nFeature HDFS Traditional File System \\nStorage Type Distributed Local disk storage \\nFault Tolerance High (Replication) Low \\nScalability Linear scaling Limited by disk size \\nData Processing Optimized for batch processing General purpose \\nExample: HDFS stores and processes logs from a web service, while a traditional file \\nsystem is used for local document storage. \\n13. How partitions are shuffled in map reduce. \\n• The Partitioner determines which Reducer a key-value pair is sent to. \\n• The Shuffle Phase transfers data from Mappers to appropriate Reducers. \\n• The Sorting Phase organizes data before being processed by the Reducers. \\nExample: In a sales dataset, a partitioner ensures all sales records for a city go to the same \\nReducer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='14.Illustrate the basic structure of the MapReduce \\nprogramming model.\\n \\n15 Marks: \\n1.Describe with relevant examples the 5Vs of Big Data. \\n5Vs of Big Data with Examples (Expanded 15 Marks Answer) \\nBig Data is defined by 5 key characteristics known as the 5Vs: Volume, Velocity, Variety, \\nVeracity, and Value. These attributes help in understanding how data is generated, \\nprocessed, stored, and analyzed across various industries. \\n \\n1. Volume (Size of Data) – 3 Marks \\nDefinition: \\nVolume refers to the sheer amount of data generated every second from different sources \\nlike social media, IoT devices, and financial transactions. As organizations collect and store \\nmassive datasets, traditional storage and processing methods become inadequate. \\nExample: \\n• Social Media: Facebook processes over 4 petabytes of data daily, including posts, \\nimages, and videos. \\n• E-Commerce: Amazon and Flipkart handle billions of customer transactions every \\nyear.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 6, 'page_label': '7', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Social Media: Facebook processes over 4 petabytes of data daily, including posts, \\nimages, and videos. \\n• E-Commerce: Amazon and Flipkart handle billions of customer transactions every \\nyear. \\n• Healthcare: Medical imaging data, such as MRI scans, generates terabytes of data \\nper hospital per year. \\nImpact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Requires scalable storage solutions like Hadoop Distributed File System (HDFS) and \\ncloud-based storage (AWS, Google Cloud, Azure). \\n• Traditional relational databases (RDBMS) struggle with such large data volumes, \\nleading to the adoption of NoSQL databases like Apache Cassandra and MongoDB. \\n \\n2. Velocity (Speed of Data Generation & Processing) – 3 Marks \\nDefinition: \\nVelocity represents the speed at which data is generated, collected, and processed. Some \\ndata streams require real-time processing to provide instant insights and responses. \\nExample: \\n• Stock Markets: Millions of trades per second require real-time analytics to detect \\nfluctuations. \\n• Autonomous Vehicles: Self-driving cars like Tesla process sensor data in \\nmilliseconds for quick decision-making. \\n• Google Search: Handles over 100,000 searches per second, requiring fast indexing \\nand retrieval. \\nImpact: \\n• Technologies like Apache Kafka, Apache Flink, and Spark Streaming are used for \\nreal-time data processing.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='and retrieval. \\nImpact: \\n• Technologies like Apache Kafka, Apache Flink, and Spark Streaming are used for \\nreal-time data processing. \\n• Businesses use real-time analytics for fraud detection, recommendation systems, \\nand operational efficiency. \\n \\n3. Variety (Different Data Types) – 3 Marks \\nDefinition: \\nVariety refers to the different formats and sources of data—structured, semi-structured, \\nand unstructured. Unlike traditional databases that handle only structured data, Big Data \\nsystems must process diverse formats. \\nExample: \\n• Structured Data: Relational databases storing customer details, order history \\n(MySQL, PostgreSQL). \\n• Semi-Structured Data: JSON or XML logs generated by web applications, sensors, \\nand IoT devices. \\n• Unstructured Data:  \\no YouTube processes over 500 hours of video uploads every minute. \\no Social media generates billions of posts, images, and comments daily. \\no Audio recordings from customer support call centers need sentiment \\nanalysis. \\nImpact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 7, 'page_label': '8', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='o Social media generates billions of posts, images, and comments daily. \\no Audio recordings from customer support call centers need sentiment \\nanalysis. \\nImpact: \\n• NoSQL databases like MongoDB (document-based) and Apache Cassandra (column-\\nbased) allow flexible data models. \\n• AI models process unstructured data (text, images, audio) for fraud detection, \\nsentiment analysis, and medical diagnosis. \\n \\n4. Veracity (Data Accuracy & Trustworthiness) – 3 Marks \\nDefinition: \\nVeracity concerns the quality, reliability, and accuracy of data. Big Data is often messy, \\ncontaining inconsistencies, errors, or biases, making data cleaning essential.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Example: \\n• Fake News Detection: Social media platforms like Facebook use AI to filter out false \\ninformation. \\n• Financial Transactions: Banks use fraud detection models to analyze suspicious \\ntransactions and prevent fraud. \\n• Healthcare Data: Medical datasets must be cleaned and verified before being used \\nin AI-driven diagnosis models. \\nImpact: \\n• ETL (Extract, Transform, Load) pipelines clean and structure raw data before \\nanalysis. \\n• Machine learning models rely on high-quality, unbiased datasets to avoid inaccurate \\npredictions. \\n• Data governance policies ensure compliance with regulations like GDPR and HIPAA. \\n \\n5. Value (Business & Analytical Insights) – 3 Marks \\nDefinition: \\nValue is the most critical aspect of Big Data—it refers to extracting meaningful insights that \\ndrive business decisions. Without proper analysis, raw data is just a storage burden. \\nExample: \\n• Netflix’s Recommendation System: Uses machine learning on user data to suggest \\npersonalized content.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Example: \\n• Netflix’s Recommendation System: Uses machine learning on user data to suggest \\npersonalized content. \\n• Retail Industry: Walmart predicts demand based on customer purchases and adjusts \\nstock levels. \\n• Healthcare AI: Predictive analytics helps doctors diagnose diseases earlier based on \\nlarge datasets. \\nImpact: \\n• Data-driven decision-making leads to higher efficiency and revenue. \\n• AI-powered insights enable better customer experiences, fraud detection, and \\noperational automation. \\n• Companies use big data analytics tools like Hadoop, Spark, and Tableau to derive \\nbusiness intelligence. \\n \\nConclusion \\nThe 5Vs of Big Data—Volume, Velocity, Variety, Veracity, and Value—are essential for \\nhandling large-scale data efficiently. Businesses leverage advanced cloud computing, AI, \\nand NoSQL databases to process, analyze, and gain insights from Big Data. \\n2,3. Describe the importance of tools in Big Data with \\nemphasis on the trends and technology in big data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 8, 'page_label': '9', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='and NoSQL databases to process, analyze, and gain insights from Big Data. \\n2,3. Describe the importance of tools in Big Data with \\nemphasis on the trends and technology in big data. \\nImportance of Big Data Tools with Trends & Technology (Expanded 15 Marks Answer) \\nThe explosion of Big Data has transformed industries, requiring advanced tools to efficiently \\nstore, process, and analyze vast datasets. Traditional databases fail to handle the Volume, \\nVariety, and Velocity of modern data, making Big Data tools essential for decision-making, \\nautomation, and real-time analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Importance of Big Data Tools – 5 Marks \\nBig Data tools offer: \\n   Efficient Data Management – Handling petabytes of structured, semi-structured, and \\nunstructured data. \\n   Scalability & Flexibility – Supporting horizontal scaling for cloud-based and on-premise \\nsystems. \\n   Real-time Processing – Enabling businesses to make instant decisions based on live data \\nstreams. \\n   Enhanced Security & Compliance – Implementing encryption and access control (GDPR, \\nHIPAA). \\n   AI & Machine Learning Integration – Powering intelligent automation and predictive \\nanalytics. \\n     Example: Facebook uses Apache Hadoop for storing and analyzing user interactions \\nacross its platform, improving targeted advertising. \\n \\n2. Key Big Data Tools & Technologies – 5 Marks \\nA. Data Storage & Management \\n  Hadoop Distributed File System (HDFS) – Scalable, fault-tolerant storage. \\n  Apache Cassandra – A NoSQL database optimized for high-speed transactions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A. Data Storage & Management \\n  Hadoop Distributed File System (HDFS) – Scalable, fault-tolerant storage. \\n  Apache Cassandra – A NoSQL database optimized for high-speed transactions. \\n  Google BigQuery – Serverless cloud warehouse for massive data analysis. \\n     Example: Spotify uses BigQuery to analyze user streaming habits and personalize \\nplaylists. \\nB. Data Processing & Analytics \\n  Apache Spark – Fast in-memory processing for large datasets. \\n  Apache Flink – Real-time stream processing for instant decision-making. \\n  TensorFlow & PyTorch – AI-driven analytics for deep learning and automation. \\n     Example: Uber uses Apache Flink to process real-time ride requests, optimizing driver \\navailability. \\nC. Data Visualization & Business Intelligence (BI) \\n  Tableau – Interactive dashboards for business reporting. \\n  Power BI – Microsoft’s AI-powered analytics tool. \\n  Google Data Studio – Cloud-based visualization platform.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 9, 'page_label': '10', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Tableau – Interactive dashboards for business reporting. \\n  Power BI – Microsoft’s AI-powered analytics tool. \\n  Google Data Studio – Cloud-based visualization platform. \\n     Example: Coca-Cola uses Tableau to track supply chain efficiency and sales trends.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='3. Emerging Trends in Big Data – 5 Marks \\n        A. AI & Machine Learning Integration – Automating decision-making with predictive \\nanalytics. \\n        B. Edge Computing & IoT – Processing data closer to devices for real-time insights. \\n        C. Serverless & Cloud Computing – AWS, Azure, and Google Cloud enabling scalable \\nanalytics. \\n        D. Blockchain for Data Integrity – Securing transactions in finance and healthcare. \\n        E. Quantum Computing in Big Data – Unlocking new possibilities in large-scale data \\nprocessing. \\n     Example: Tesla leverages Edge AI & IoT to analyze autonomous driving data instantly, \\nimproving vehicle safety. \\n4,5. Illustrate the main features and evolution of big \\ndata in detail. provide answer for 15 marks \\nChatGPT said: \\nMain Features and Evolution of Big Data (15 Marks Answer) \\nBig Data refers to extremely large and complex datasets that require advanced techniques'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='ChatGPT said: \\nMain Features and Evolution of Big Data (15 Marks Answer) \\nBig Data refers to extremely large and complex datasets that require advanced techniques \\nfor processing, analyzing, and extracting insights. It has evolved significantly due to \\ntechnological advancements, increased digitalization, and the growing need for real-time \\ndecision-making. \\n \\n1. Main Features of Big Data – 5 Marks \\nBig Data is characterized by the 5Vs, which define its scale, complexity, and value: \\n1. Volume \\n• Definition: Refers to the enormous size of data generated daily from sources like \\nsocial media, IoT, and business transactions. \\n• Example: Facebook processes over 4 petabytes of data daily. \\n2. Velocity \\n• Definition: The speed at which data is generated and processed in real time. \\n• Example: Stock market trading systems process millions of transactions per second. \\n3. Variety \\n• Definition: Data exists in multiple formats (structured, semi-structured, \\nunstructured).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 10, 'page_label': '11', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example: Stock market trading systems process millions of transactions per second. \\n3. Variety \\n• Definition: Data exists in multiple formats (structured, semi-structured, \\nunstructured). \\n• Example: Emails, social media posts, IoT sensor data, and videos.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Veracity \\n• Definition: The reliability and accuracy of data for meaningful decision-making. \\n• Example: Fake news detection in social media analytics. \\n5. Value \\n• Definition: Extracting meaningful insights and business advantages from data. \\n• Example: Netflix analyzes user preferences to recommend personalized content. \\n \\n2. Evolution of Big Data – 5 Marks \\nBig Data has evolved through different stages based on technological advancements: \\n1. Early Data Management (1960s - 1980s) \\n• Traditional relational databases (RDBMS) like Oracle and IBM DB2 emerged. \\n• Limited data storage and computing power. \\n2. Rise of the Internet (1990s - 2000s) \\n• Explosion of digital data with the growth of e-commerce, social media, and cloud \\ncomputing. \\n• Google developed MapReduce (2004) & Hadoop (2006) to process large datasets \\nefficiently. \\n3. Big Data Boom (2010s - Present) \\n• Technologies like Apache Spark, NoSQL, and real-time streaming gained traction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 11, 'page_label': '12', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"efficiently. \\n3. Big Data Boom (2010s - Present) \\n• Technologies like Apache Spark, NoSQL, and real-time streaming gained traction. \\n• AI-driven analytics transformed industries like healthcare, finance, and marketing. \\n4. Future Trends (2025 & Beyond) \\n• Quantum computing to enhance Big Data processing. \\n• Edge computing for real-time analytics in IoT. \\n• Blockchain for secure data transactions. \\n     Example: Tesla's self-driving cars leverage Big Data to process real-time road \\nconditions. \\n \\n3. Impact of Big Data on Industries – 5 Marks \\n  Healthcare – AI-driven diagnostics (IBM Watson). \\n  Retail – Personalized recommendations (Amazon, Flipkart).\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Banking & Finance – Fraud detection (PayPal, Mastercard). \\n  Smart Cities – Traffic and energy management (Google Maps, Smart Grids). \\n \\nConclusion \\nBig Data has evolved from simple data management to AI-driven analytics, shaping \\nmodern industries. Its key features (5Vs) and technological advancements continue to \\ndrive innovation, automation, and smarter decision-making. Businesses leveraging Big \\nData will gain a competitive edge in the digital era. \\n6. Imagine you are a data scientist in 2025. How would \\nyou leverage the evolution of big data to solve a \\nspecific business problem in your industry?  \\nLeveraging Big Data Evolution to Solve a Business Problem (2025) \\nIntroduction \\nIn 2025, the evolution of big data analytics has transformed business decision-making. As \\na data scientist, I would leverage big data to solve a key problem: customer churn \\nprediction in e-commerce. Retaining customers is more profitable than acquiring new'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 12, 'page_label': '13', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='a data scientist, I would leverage big data to solve a key problem: customer churn \\nprediction in e-commerce. Retaining customers is more profitable than acquiring new \\nones, and advanced data analytics can help predict and prevent churn. \\n \\n1. Evolution of Big Data & Its Role in Business \\nBig data has evolved in several ways: \\n• Scalability: Cloud-based solutions (AWS, Google BigQuery) enable real-time \\nprocessing. \\n• Speed: Technologies like Apache Spark process data faster than traditional systems. \\n• AI Integration: Machine learning & deep learning provide predictive insights. \\n• Diverse Data Sources: Data comes from IoT devices, social media, and web \\nanalytics. \\nThese advancements allow businesses to understand customer behavior, optimize \\nmarketing, and improve decision-making. \\n \\n2. How Big Data Analytics Solves Customer Churn \\na) Data Collection & Integration \\n• Structured Data: Purchase history, transaction records, and product reviews.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Unstructured Data: Customer sentiment from social media & chat support. \\n• Example: Netflix analyzes user watch history & engagement to predict \\ncancellations. \\nb) Predictive Analytics & Machine Learning \\n• Models like Random Forest, XGBoost, and Deep Learning identify churn patterns. \\n• Real-time processing (Apache Kafka, Spark) ensures quick insights. \\n• Example: Amazon uses ML to detect customers likely to switch to competitors. \\nc) AI-Driven Personalization \\n• Chatbots & recommendation engines offer real-time solutions. \\n• Sentiment analysis detects negative reviews & triggers retention strategies. \\n• Example: Spotify sends personalized playlists to keep users engaged. \\nd) Proactive Customer Retention Strategies \\n• Targeted discounts & loyalty programs for at-risk customers. \\n• Predictive alerts for sales teams to engage potential churners. \\n• Example: E-commerce platforms send discount codes to inactive users. \\n \\n3. Business Benefits & Impact'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 13, 'page_label': '14', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Predictive alerts for sales teams to engage potential churners. \\n• Example: E-commerce platforms send discount codes to inactive users. \\n \\n3. Business Benefits & Impact \\n   Increased customer retention, reducing marketing costs. \\n   Higher revenue through personalized engagement strategies. \\n   Improved customer experience, enhancing brand loyalty. \\n \\nConclusion \\nBy leveraging big data analytics, AI, and real-time processing, businesses can predict \\ncustomer churn, engage users effectively, and drive growth. The evolution of big data \\nenables faster, smarter, and more personalized decision-making, ensuring a competitive \\nedge in 2025.         \\n7. Summarize the various industrial use cases of Big \\nData with examples. \\nIndustrial Use Cases of Big Data with Examples \\nIntroduction'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Big Data refers to the vast volumes of structured and unstructured data generated at high \\nvelocity from multiple sources. Organizations across industries harness big data \\ntechnologies such as Hadoop, Apache Spark, AI-driven analytics, and cloud computing to \\nextract valuable insights, improve efficiency, and make data-driven decisions. \\n \\n1. Healthcare & Pharmaceuticals \\nUse Case: Predictive Analytics and Personalized Medicine \\n• Example: IBM Watson Health leverages big data to analyze medical records and \\nassist doctors in diagnosing diseases like cancer at early stages. \\n• Impact:  \\no Enhanced patient care through early disease detection. \\no Personalized treatment plans based on patient history. \\no Faster drug discovery through AI-powered simulations. \\n \\n2. E-Commerce & Retail \\nUse Case: Customer Behavior Analysis & Demand Forecasting \\n• Example: Amazon and Flipkart use big data to analyze customer browsing and \\npurchase history, offering personalized product recommendations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 14, 'page_label': '15', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example: Amazon and Flipkart use big data to analyze customer browsing and \\npurchase history, offering personalized product recommendations. \\n• Impact:  \\no Increased sales through targeted advertisements. \\no Optimized inventory management to reduce wastage. \\no Better understanding of customer preferences and trends. \\n \\n3. Finance & Banking \\nUse Case: Fraud Detection & Risk Management \\n• Example: Banks like JPMorgan Chase use machine learning algorithms to monitor \\ntransactions and detect anomalies indicating fraud. \\n• Impact:  \\no Reduction in fraudulent activities and cyber threats. \\no Enhanced customer trust through secure banking services. \\no Improved credit scoring and loan approval processes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Manufacturing & Supply Chain \\nUse Case: Predictive Maintenance & Process Optimization \\n• Example: General Electric (GE) uses IoT sensors and AI-powered analytics to predict \\nequipment failures and schedule maintenance. \\n• Impact:  \\no Reduced operational downtime and maintenance costs. \\no Increased production efficiency and output. \\no Smart inventory management and supply chain optimization. \\n \\n5. Telecommunications \\nUse Case: Network Optimization & Customer Retention \\n• Example: Vodafone and AT&T use big data to analyze call records, predict network \\ncongestion, and provide seamless connectivity. \\n• Impact:  \\no Improved network efficiency and reduced service disruptions. \\no Enhanced customer satisfaction through proactive troubleshooting. \\no Minimized customer churn by offering tailored data plans. \\n \\n6. Transportation & Smart Cities \\nUse Case: Traffic Management & Route Optimization \\n• Example: Google Maps utilizes real-time GPS data and big data analytics to provide'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 15, 'page_label': '16', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Transportation & Smart Cities \\nUse Case: Traffic Management & Route Optimization \\n• Example: Google Maps utilizes real-time GPS data and big data analytics to provide \\noptimal travel routes and predict traffic conditions. \\n• Impact:  \\no Reduced traffic congestion and travel delays. \\no Efficient public transport scheduling and urban planning. \\no Safer roads through predictive accident analysis. \\n \\n7. Education & EdTech \\nUse Case: Adaptive Learning & Student Performance Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example: Platforms like Coursera and Udemy use AI-driven analytics to track \\nstudent progress and recommend personalized learning paths. \\n• Impact:  \\no Enhanced learning outcomes through customized courses. \\no Early identification of struggling students for targeted assistance. \\no Data-driven improvements in course content and teaching strategies. \\n \\n8. Entertainment & Media \\nUse Case: Content Recommendation & Audience Insights \\n• Example: Netflix and Spotify analyze user preferences to offer personalized movie, \\nmusic, and podcast recommendations. \\n• Impact:  \\no Increased viewer engagement and retention. \\no Improved content production based on audience trends. \\no Higher advertising revenue through targeted ads. \\n \\n9. Agriculture & Farming \\nUse Case: Precision Farming & Yield Prediction \\n• Example: John Deere uses big data to analyze soil conditions, weather patterns, \\nand crop health for smart farming decisions. \\n• Impact:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content=\"Use Case: Precision Farming & Yield Prediction \\n• Example: John Deere uses big data to analyze soil conditions, weather patterns, \\nand crop health for smart farming decisions. \\n• Impact:  \\no Increased crop yields through data-driven farming techniques. \\no Reduced resource wastage (water, fertilizers, and pesticides). \\no Early detection of plant diseases to prevent large-scale losses. \\n \\n10. Energy & Utilities \\nUse Case: Smart Grid Management & Energy Optimization \\n• Example: Tesla's Powerwall and smart grid systems use big data to predict energy \\ndemand and optimize electricity distribution. \\n• Impact:  \\no Reduced energy waste and improved efficiency.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='o Integration of renewable energy sources for sustainability. \\no Lower electricity costs for consumers. \\n \\nConclusion \\nBig Data is revolutionizing industries by driving efficiency, enhancing customer experience, \\nand enabling predictive decision-making. The continuous evolution of big data analytics, \\nAI, and IoT will further transform businesses, optimize operations, and shape the future of \\ndigital innovation.         \\n8. Identify the challenges of the Big Data applications \\nin Modern Data Analytics.  \\nChatGPT said: \\nChallenges of Big Data Applications in Modern Data Analytics \\nBig Data has transformed industries by enabling large-scale data processing, AI-driven \\ninsights, and real-time decision-making. However, its implementation faces significant \\nchallenges that organizations must address. \\n \\n1. Data Quality & Inconsistency \\n• Challenge: Inaccurate, incomplete, or redundant data affects analytical outcomes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 17, 'page_label': '18', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='challenges that organizations must address. \\n \\n1. Data Quality & Inconsistency \\n• Challenge: Inaccurate, incomplete, or redundant data affects analytical outcomes. \\n• Impact: Poor data quality leads to unreliable insights and flawed decision-making. \\n• Solution: Implement data cleaning, validation techniques, and ETL (Extract, \\nTransform, Load) processes. \\n \\n2. Data Storage & Management \\n• Challenge: Handling massive volumes of structured and unstructured data requires \\nefficient storage solutions. \\n• Impact: High storage costs and inefficient data retrieval slow down analytics. \\n• Solution: Utilize distributed storage systems like Hadoop HDFS, Amazon S3, and \\nGoogle BigQuery. \\n \\n3. Scalability & Processing Speed \\n• Challenge: Traditional databases struggle to scale and process real-time data \\nefficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Impact: Delayed insights reduce the effectiveness of time-sensitive analytics. \\n• Solution: Implement parallel computing, cloud-based architectures, and real-time \\nprocessing tools like Apache Spark. \\n \\n4. Data Security & Privacy \\n• Challenge: Sensitive data is vulnerable to breaches, cyberattacks, and unauthorized \\naccess. \\n• Impact: Non-compliance with data regulations (GDPR, HIPAA) can result in legal \\npenalties and loss of customer trust. \\n• Solution: Use encryption, access control, and blockchain-based security \\nframeworks. \\n \\n5. Integration of Diverse Data Sources \\n• Challenge: Big Data comes from multiple sources (IoT, social media, enterprise \\nsystems), leading to integration complexities. \\n• Impact: Data silos prevent seamless analytics and decision-making. \\n• Solution: Employ data lakes, APIs, and middleware solutions for interoperability. \\n \\n6. High Computational Costs \\n• Challenge: Running machine learning models and large-scale analytics requires'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 18, 'page_label': '19', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Solution: Employ data lakes, APIs, and middleware solutions for interoperability. \\n \\n6. High Computational Costs \\n• Challenge: Running machine learning models and large-scale analytics requires \\nsignificant computational power. \\n• Impact: Expensive cloud resources and infrastructure investments can limit \\nscalability. \\n• Solution: Optimize data pipelines, use edge computing, and implement cost-\\nefficient AI models. \\n \\n7. Talent Shortage & Skill Gaps \\n• Challenge: Organizations struggle to find skilled professionals in Big Data, AI, and \\ncloud technologies. \\n• Impact: Delays in project implementation and reduced efficiency in analytics. \\n• Solution: Invest in employee training, AI-driven automation, and low-code/no-code \\nanalytics platforms.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='8. Ethical & Bias Issues \\n• Challenge: AI-driven analytics can inherit biases from historical data, leading to \\nunfair outcomes. \\n• Impact: Discrimination in hiring, finance, and healthcare decisions. \\n• Solution: Ensure diverse datasets, apply bias detection algorithms, and enforce \\nethical AI guidelines. \\n \\n9. Real-time Analytics Complexity \\n• Challenge: Processing high-velocity data streams from IoT, stock markets, and social \\nmedia requires advanced real-time analytics. \\n• Impact: Delays in detecting fraud, monitoring system failures, and responding to \\ncustomer needs. \\n• Solution: Leverage real-time processing frameworks like Apache Kafka, Apache \\nFlink, and in-memory databases. \\n \\n10. Compliance & Regulatory Challenges \\n• Challenge: Governments enforce strict regulations on data usage, retention, and \\ncross-border transfers. \\n• Impact: Non-compliance results in heavy fines and operational disruptions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 19, 'page_label': '20', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Challenge: Governments enforce strict regulations on data usage, retention, and \\ncross-border transfers. \\n• Impact: Non-compliance results in heavy fines and operational disruptions. \\n• Solution: Adopt transparent data governance policies, comply with GDPR, CCPA, \\nand industry standards. \\n \\nConclusion \\nAddressing these challenges is crucial for businesses to fully leverage Big Data analytics. By \\nadopting advanced AI, cloud computing, security protocols, and ethical AI practices, \\norganizations can unlock the true potential of Big Data for competitive advantage.         \\n9. Explain the big data architecture for \\nhandling large-scale social media analytics.  \\nChatGPT said: \\nBig Data Architecture for Handling Large-Scale Social Media Analytics'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Social media generates vast amounts of high-velocity, high-variety data. A robust Big Data \\nArchitecture is essential to process, analyze, and extract meaningful insights in real-time. \\n \\n1. Data Sources \\nSocial media data originates from: \\n• Platforms: Facebook, Twitter, Instagram, LinkedIn, TikTok, etc. \\n• Media Types: Text posts, images, videos, comments, reactions, hashtags. \\n• APIs & Streams: Twitter API, Facebook Graph API, YouTube Data API. \\n \\n2. Data Ingestion Layer \\nResponsible for collecting real-time and batch data. \\n• Tools: Apache Kafka, Apache Flume, AWS Kinesis, Google Pub/Sub. \\n• Process: Streams large volumes of unstructured and semi-structured data for \\nfurther processing. \\n \\n3. Storage Layer \\nStores raw, processed, and structured data efficiently. \\n• Distributed Storage: Hadoop HDFS, Amazon S3, Google Cloud Storage. \\n• NoSQL Databases: MongoDB, Cassandra for semi-structured and unstructured data. \\n• Columnar Databases: Apache HBase for fast read/write access.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 20, 'page_label': '21', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• NoSQL Databases: MongoDB, Cassandra for semi-structured and unstructured data. \\n• Columnar Databases: Apache HBase for fast read/write access. \\n \\n4. Processing & Analytics Layer \\nHandles real-time and batch processing for insights extraction. \\n• Batch Processing: Apache Spark, Hadoop MapReduce for deep historical analysis. \\n• Real-time Processing: Apache Flink, Storm, or Spark Streaming for immediate trend \\ndetection. \\n• Machine Learning & AI: TensorFlow, PyTorch, Scikit-Learn for sentiment analysis, \\nuser behavior prediction. \\n \\n5. Querying & Data Access Layer \\nProvides interfaces to access, query, and manipulate data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• SQL Query Engines: Apache Hive, Presto, Google BigQuery for structured queries. \\n• Graph Databases: Neo4j for relationship analysis (e.g., influencer networks). \\n• Data Indexing: Elasticsearch for fast text searches and log analysis. \\n \\n6. Visualization & Business Intelligence Layer \\nConverts data into meaningful insights for decision-making. \\n• Tools: Tableau, Power BI, Kibana, Google Data Studio. \\n• Use Cases: Engagement metrics, hashtag trends, audience demographics, and real-\\ntime dashboards. \\n \\n7. Security & Governance Layer \\nEnsures compliance, privacy, and data integrity. \\n• Access Control: AWS IAM, Role-Based Access Control (RBAC). \\n• Data Privacy: GDPR, CCPA compliance for personal data protection. \\n• Encryption & Logging: TLS, SSL, and logging tools for secure access. \\n \\nConclusion \\nA well-structured Big Data architecture enables organizations to analyze large-scale social \\nmedia data, detect trends, enhance customer engagement, and improve business'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 21, 'page_label': '22', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Conclusion \\nA well-structured Big Data architecture enables organizations to analyze large-scale social \\nmedia data, detect trends, enhance customer engagement, and improve business \\nstrategies in real time.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='CH : 2 15 Marks \\n1.If a file of 5 GB is uploaded to HDFS and the block size is 1 \\nGB, explain how HDFS will divide and store this file across \\nmultiple DataNodes. Assume replication factor of 2. \\nHDFS File Storage and Replication in a Distributed System (15 Marks) \\nWhen a 5 GB file is uploaded to HDFS with a block size of 1 GB and a replication factor of 2, \\nthe system handles the storage and replication as follows: \\n \\n1. File Splitting in HDFS \\nHDFS follows a distributed file storage system, where large files are divided into fixed-size \\nblocks. Since the block size is 1 GB, a 5 GB file will be split into 5 blocks, each of size 1 GB. \\nBlock Number Size \\nBlock 1 1 GB \\nBlock 2 1 GB \\nBlock 3 1 GB \\nBlock 4 1 GB \\nBlock 5 1 GB \\nThese blocks are then distributed across multiple DataNodes for parallel processing and \\nfault tolerance. \\n \\n2. Data Distribution Across DataNodes \\nHDFS follows a master-slave architecture, where the NameNode manages metadata, and \\nDataNodes store the actual blocks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 22, 'page_label': '23', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='fault tolerance. \\n \\n2. Data Distribution Across DataNodes \\nHDFS follows a master-slave architecture, where the NameNode manages metadata, and \\nDataNodes store the actual blocks. \\nEach of the 5 blocks is stored on a different DataNode, and since the replication factor is 2, \\neach block has two copies stored on different nodes. \\nBlock Primary DataNode Replica DataNode \\nBlock 1 DataNode A DataNode D \\nBlock 2 DataNode B DataNode E \\nBlock 3 DataNode C DataNode F'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Block Primary DataNode Replica DataNode \\nBlock 4 DataNode D DataNode A \\nBlock 5 DataNode E DataNode B \\nThis replication ensures fault tolerance, meaning if one DataNode fails, another node has a \\ncopy. \\n \\n3. Advantages of HDFS Storage Strategy \\nHDFS ensures reliability, fault tolerance, and parallel processing by implementing the \\nfollowing strategies: \\n   Fault Tolerance: \\n• Since each block has 2 copies, data is not lost even if a node crashes. \\n• The NameNode monitors block health and re-replicates blocks in case of failures. \\n   Load Balancing & Scalability: \\n• Blocks are evenly distributed across multiple DataNodes, preventing overloading \\non a single machine. \\n• The system scales horizontally by adding more DataNodes when storage demand \\nincreases. \\n   Parallel Processing (MapReduce Support): \\n• Since different blocks are stored on multiple nodes, parallel processing using \\nMapReduce speeds up large-scale computations. \\n   High Availability:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 23, 'page_label': '24', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Parallel Processing (MapReduce Support): \\n• Since different blocks are stored on multiple nodes, parallel processing using \\nMapReduce speeds up large-scale computations. \\n   High Availability: \\n• The NameNode tracks all block locations and helps in quick recovery if a node fails. \\n• HDFS ensures continuous access to data even during failures. \\n \\n4. Step-by-Step Process of Uploading a File to HDFS \\n1⃣ File Upload: The 5 GB file is uploaded to HDFS. \\n2⃣ Block Splitting: HDFS divides the file into 5 blocks (each 1 GB). \\n3⃣ DataNode Assignment: The NameNode assigns DataNodes to store each block. \\n4⃣ Replication: Each block is replicated twice across different DataNodes. \\n5⃣ Data Retrieval: When a client requests the file, blocks are fetched and reassembled.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Conclusion \\nHDFS follows an efficient, scalable, and fault-tolerant architecture to manage large files in \\ndistributed environments. By splitting the file into blocks, distributing them across nodes, \\nand ensuring replication, HDFS ensures high availability, reliability, and efficient processing \\nof big data workloads. \\n2. Demonstrate the working of Map Reduce algorithm \\nto find the frequency of words in a given input text. \\nprovide answer for 15 marks \\nChatGPT said: \\nDemonstration of MapReduce Algorithm for Word Frequency Calculation (15 Marks) \\n1. Introduction to MapReduce \\nMapReduce is a distributed data processing model that divides tasks into two main \\nphases: \\n  Map Phase: Processes input data and generates key-value pairs. \\n  Reduce Phase: Aggregates and summarizes results. \\nIn this example, we demonstrate how MapReduce calculates word frequency from a given \\ninput text. \\n \\n2. Input Example \\nConsider the following input text: \\n\"Big data is powerful. Big data is everywhere.\"'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='input text. \\n \\n2. Input Example \\nConsider the following input text: \\n\"Big data is powerful. Big data is everywhere.\" \\n \\n3. Step-by-Step Execution of MapReduce Algorithm \\nStep 1: Map Phase \\n• The input text is split into words. \\n• Each word is emitted as a key-value pair (word, 1). \\nInput (Text Split into Words) Mapper Output (Key-Value Pairs) \\nBig (Big, 1) \\ndata (data, 1) \\nis (is, 1)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Input (Text Split into Words) Mapper Output (Key-Value Pairs) \\npowerful. (powerful, 1) \\nBig (Big, 1) \\ndata (data, 1) \\nis (is, 1) \\neverywhere. (everywhere, 1) \\n  Intermediate Output from Mapper: \\nscss \\nCopyEdit \\n(Big, 1)   \\n(data, 1)   \\n(is, 1)   \\n(powerful, 1)   \\n(Big, 1)   \\n(data, 1)   \\n(is, 1)   \\n(everywhere, 1)   \\n \\nStep 2: Shuffle & Sort Phase \\n• The MapReduce framework groups values by keys before passing them to the \\nreducer. \\nShuffled Key-Value Pairs \\n(Big, [1, 1]) \\n(data, [1, 1]) \\n(is, [1, 1]) \\n(powerful, [1]) \\n(everywhere, [1])'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 26, 'page_label': '27', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Step 3: Reduce Phase \\n• The Reducer aggregates the counts for each word. \\nReducer Input Reducer Output \\n(Big, [1, 1]) (Big, 2) \\n(data, [1, 1]) (data, 2) \\n(is, [1, 1]) (is, 2) \\n(powerful, [1]) (powerful, 1) \\n(everywhere, [1]) (everywhere, 1) \\n  Final Output from Reducer: \\nscss \\nCopyEdit \\n(Big, 2)   \\n(data, 2)   \\n(is, 2)   \\n(powerful, 1)   \\n(everywhere, 1)   \\n \\n4. Python Implementation of MapReduce Word Count \\nHere’s a simple Python program to demonstrate word frequency using MapReduce: \\npython \\nCopyEdit \\nfrom collections import defaultdict \\n# Sample input text \\ntext = \"Big data is powerful. Big data is everywhere.\" \\n# Step 1: Mapper Function \\ndef mapper(text): \\n    words = text.lower().replace(\".\", \"\").split()  # Convert to lowercase & split \\n    return [(word, 1) for word in words]  # Emit (word, 1) pairs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='# Step 2: Shuffle & Sort (Grouping by Key) \\ndef shuffle_sort(mapped_data): \\n    grouped_data = defaultdict(list) \\n    for word, count in mapped_data: \\n        grouped_data[word].append(count) \\n    return grouped_data \\n# Step 3: Reducer Function \\ndef reducer(grouped_data): \\n    return {word: sum(counts) for word, counts in grouped_data.items()}  # Summing counts \\n# Running MapReduce \\nmapped_data = mapper(text) \\ngrouped_data = shuffle_sort(mapped_data) \\nword_frequencies = reducer(grouped_data) \\n# Display Output \\nprint(\"Word Frequencies:\", word_frequencies) \\n  Expected Output: \\nyaml \\nCopyEdit \\nWord Frequencies: {\\'big\\': 2, \\'data\\': 2, \\'is\\': 2, \\'powerful\\': 1, \\'everywhere\\': 1} \\n \\n5. Key Advantages of MapReduce for Word Count \\n   Scalability: Handles massive datasets across multiple machines. \\n   Fault Tolerance: Ensures high availability in distributed systems. \\n   Parallel Processing: Efficiently splits work across nodes. \\n   Load Balancing: Distributes tasks evenly to optimize performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 27, 'page_label': '28', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Parallel Processing: Efficiently splits work across nodes. \\n   Load Balancing: Distributes tasks evenly to optimize performance. \\n \\n6. Conclusion \\nThe MapReduce algorithm efficiently counts word occurrences in large-scale datasets. It \\nsplits text into words, maps them to key-value pairs, shuffles & sorts the data, and reduces \\nit to compute the final word frequencies. This approach is widely used in big data \\nanalytics, log processing, and text mining applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='3. Explain in detail about Hadoop YARN. \\nhttps://www.geeksforgeeks.org/hadoop-yarn-architecture/ \\nHadoop YARN (Yet Another Resource Negotiator) - Detailed Explanation (15 Marks) \\n1. Introduction to Hadoop YARN \\nHadoop YARN (Yet Another Resource Negotiator) is the resource management layer in the \\nHadoop ecosystem. It allows multiple applications to share cluster resources efficiently \\nwhile ensuring scalability, fault tolerance, and high availability. \\nKey Features: \\n  Decouples resource management from job scheduling. \\n  Supports multi-tenancy and dynamic resource allocation. \\n  Enables parallel execution of applications. \\n \\n2. YARN Architecture & Components \\nHadoop YARN consists of the following key components: \\n1. ResourceManager (RM) \\n• Master daemon responsible for managing resources across the cluster. \\n• Has two main components:  \\no Scheduler: Allocates resources based on policies (FIFO, Fair, Capacity). \\no ApplicationManager: Manages application lifecycle. \\n2. NodeManager (NM)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 28, 'page_label': '29', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Has two main components:  \\no Scheduler: Allocates resources based on policies (FIFO, Fair, Capacity). \\no ApplicationManager: Manages application lifecycle. \\n2. NodeManager (NM) \\n• Runs on each worker node and monitors resource usage. \\n• Communicates with ResourceManager and executes tasks in containers. \\n3. ApplicationMaster (AM) \\n• Created for each application and negotiates resources from RM. \\n• Monitors execution of tasks and handles failures. \\n4. Containers \\n• Isolated execution environments where actual tasks run. \\n• Dynamically allocated by RM based on CPU & memory requirements. \\n \\n3. Working of YARN'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Step 1: Application Submission \\n• A user submits an application (MapReduce, Spark, Hive) to YARN. \\nStep 2: Resource Allocation \\n• RM assigns containers to run the ApplicationMaster (AM) for that job. \\nStep 3: Task Execution \\n• AM requests resources from RM. \\n• NM launches containers and executes tasks. \\nStep 4: Monitoring & Completion \\n• AM monitors task execution and reports status to RM. \\n• Once completed, AM releases resources back to the cluster. \\n \\n4. Advantages of YARN \\n   Efficient Resource Utilization – Dynamic allocation of CPU & memory. \\n   Multi-Tenancy Support – Runs multiple frameworks like Spark, Tez, Flink, HBase. \\n   Scalability – Handles thousands of nodes efficiently. \\n   Fault Tolerance – Automatically reschedules failed tasks. \\n \\n5. Real-World Applications of YARN \\n  Big Data Processing – Supports MapReduce, Apache Spark, Apache Flink. \\n  Data Warehousing – Runs Apache Hive for SQL-based queries.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 29, 'page_label': '30', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Real-World Applications of YARN \\n  Big Data Processing – Supports MapReduce, Apache Spark, Apache Flink. \\n  Data Warehousing – Runs Apache Hive for SQL-based queries. \\n  Machine Learning & AI – Supports ML frameworks like TensorFlow on YARN. \\n  Streaming Applications – Works with Apache Storm & Kafka for real-time analytics. \\n \\n6. Conclusion \\nHadoop YARN efficiently manages cluster resources, enabling large-scale data processing. \\nIts scalability, fault tolerance, and flexibility make it a backbone for big data applications. \\n        \\n5. Explain about Hadoop distributed file system \\narchitecture with neat diagram. \\nHadoop Distributed File System (HDFS) Architecture \\n1. Introduction to HDFS'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Hadoop Distributed File System (HDFS) is a highly scalable, fault-tolerant, and \\ndistributed storage system designed for big data applications. It enables efficient \\nstorage and processing of massive datasets across a cluster of commodity hardware. \\nHDFS is inspired by the Google File System (GFS) and follows a master-slave \\narchitecture to manage large-scale data efficiently. \\n \\n2. Key Components of HDFS \\nHDFS consists of three main components: \\n1. NameNode (Master Node) \\n• Acts as the central metadata manager of the HDFS cluster. \\n• Tracks file structure, block locations, and namespace metadata. \\n• Does not store actual data but manages file system operations. \\n• A single point of failure unless High Availability (HA) is enabled. \\n2. DataNode (Worker Nodes) \\n• Stores actual data blocks on local disks. \\n• Periodically sends heartbeat signals to the NameNode to confirm availability. \\n• Performs read, write, replication, and deletion operations based on NameNode \\ninstructions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 30, 'page_label': '31', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Periodically sends heartbeat signals to the NameNode to confirm availability. \\n• Performs read, write, replication, and deletion operations based on NameNode \\ninstructions. \\n3. Secondary NameNode \\n• Not a backup NameNode but assists in maintaining system consistency. \\n• Periodically saves checkpoints of the file system metadata. \\n \\n3. HDFS File Storage and Block Replication \\n• When a file is uploaded, HDFS splits it into fixed-size blocks (default 128MB or \\n256MB). \\n• Each block is stored across multiple DataNodes for redundancy and fault tolerance. \\n• Default replication factor is 3, meaning each block is replicated on three different \\nnodes. \\n  Example: \\nA 600MB file stored in HDFS with a block size of 128MB will be divided into 5 blocks: \\nB1 (128MB), B2 (128MB), B3 (128MB), B4 (128MB), B5 (88MB) \\nEach block will be replicated three times across different DataNodes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Data Flow in HDFS \\nA. Writing Data to HDFS \\n     Client → NameNode → Assigns DataNodes → Data is split & stored with \\nreplication. \\n• The client requests the NameNode to store a file. \\n• The NameNode allocates DataNodes to store the file in blocks. \\n• Data is written in parallel across multiple DataNodes with replication. \\nB. Reading Data from HDFS \\n     Client → NameNode → Fetches block locations → Reads directly from DataNodes. \\n• The client requests the NameNode to read a file. \\n• The NameNode provides the list of DataNodes containing the requested blocks. \\n• The client retrieves data directly from the nearest DataNodes to optimize \\nperformance. \\n \\n5. HDFS Architecture Diagram \\n     Below is a neat diagram illustrating HDFS architecture: \\n(Insert a well-labeled diagram with NameNode, DataNodes, Replication, and Client \\ninteractions.) \\n \\n6. Features & Benefits of HDFS \\n   Scalability – Handles petabytes of data efficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 31, 'page_label': '32', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='(Insert a well-labeled diagram with NameNode, DataNodes, Replication, and Client \\ninteractions.) \\n \\n6. Features & Benefits of HDFS \\n   Scalability – Handles petabytes of data efficiently. \\n   Fault Tolerance – Data replication ensures no data loss. \\n   High Throughput – Optimized for large-scale batch processing. \\n   Cost-Effective – Works on commodity hardware, reducing infrastructure costs. \\n   Reliability – Automatic replication and fault recovery. \\n \\n7. Real-World Applications of HDFS \\n  Social Media Platforms – Facebook, Twitter, and Instagram store vast amounts of \\nuser data. \\n  Search Engines – Google and Bing use distributed storage for indexing and ranking \\nweb pages.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='E-Commerce Platforms – Amazon and Flipkart process customer data and \\ntransaction logs. \\n  Healthcare & Finance – Fraud detection, medical imaging, and data-driven \\ndecision-making. \\n \\n8. Conclusion \\nHDFS is the backbone of the Hadoop ecosystem, providing a reliable, scalable, and \\nefficient storage solution for big data applications. Its ability to handle large datasets \\nacross distributed environments makes it a crucial technology for modern data-driven \\nenterprises.         \\nhttps://www.geeksforgeeks.org/explain-the-hadoop-distributed-file-system-hdfs-\\narchitecture-and-advantages/ \\n \\n6.How does the MapReduce programming model \\nwork? Explain with an example.  \\nChatGPT said: \\nMapReduce Programming Model \\n1. Introduction to MapReduce \\nMapReduce is a distributed computing framework in Hadoop used for processing \\nlarge-scale datasets in parallel across multiple nodes. It follows a divide-and-conquer \\napproach, breaking tasks into smaller chunks that can be executed independently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 32, 'page_label': '33', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='large-scale datasets in parallel across multiple nodes. It follows a divide-and-conquer \\napproach, breaking tasks into smaller chunks that can be executed independently. \\n• Developed by Google, later implemented in Hadoop. \\n• Works on the principle of parallelism to process big data efficiently.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Consists of two main phases: Map and Reduce. \\n \\n2. Working of MapReduce \\nA. Map Phase (Splitting & Processing) \\n• Input data is split into chunks (blocks) and processed in parallel. \\n• Each chunk is passed to a Mapper function, which processes the data and \\ngenerates intermediate key-value pairs. \\nB. Shuffle & Sort Phase (Intermediate Processing) \\n• The framework sorts and groups the key-value pairs from the Mappers. \\n• Ensures that all values associated with a specific key are grouped together before \\npassing them to the Reducer. \\nC. Reduce Phase (Aggregation & Output Generation) \\n• The grouped data is passed to the Reducer function, which processes and \\naggregates the values for each key. \\n• The final result is stored in HDFS. \\n \\n3. Example: Word Count Program Using MapReduce \\nProblem Statement: \\nFind the frequency of each word in a given input text file. \\nStep-by-Step Execution: \\n1⃣ Input Data (Stored in HDFS): \\ncsharp \\nCopyEdit \\nHello Hadoop   \\nHadoop is powerful'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 33, 'page_label': '34', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Problem Statement: \\nFind the frequency of each word in a given input text file. \\nStep-by-Step Execution: \\n1⃣ Input Data (Stored in HDFS): \\ncsharp \\nCopyEdit \\nHello Hadoop   \\nHadoop is powerful   \\nHello Big Data   \\n2⃣ Map Phase (Key-Value Generation by Mapper): \\nEach line is processed by a Mapper function that splits text into words and assigns a \\ncount of 1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 34, 'page_label': '35', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Input Line Mapper Output (Key, Value) \\nHello Hadoop (Hello, 1), (Hadoop, 1) \\nHadoop is powerful (Hadoop, 1), (is, 1), (powerful, 1) \\nHello Big Data (Hello, 1), (Big, 1), (Data, 1) \\n3⃣ Shuffle & Sort Phase: \\nThe framework groups and sorts values by keys. \\nKey Grouped Values \\nBig (1) \\nData (1) \\nHadoop (1, 1) \\nHello (1, 1) \\nis (1) \\npowerful (1) \\n4⃣ Reduce Phase (Aggregation by Reducer): \\nThe Reducer function sums the values for each key. \\nKey Final Count (Reducer Output) \\nBig 1 \\nData 1 \\nHadoop 2 \\nHello 2 \\nis 1 \\npowerful 1 \\n5⃣ Final Output Stored in HDFS: \\ncsharp \\nCopyEdit \\nBig 1   \\nData 1'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Hadoop 2   \\nHello 2   \\nis 1   \\npowerful 1   \\n \\n4. Advantages of MapReduce \\n   Scalability – Can process petabytes of data efficiently. \\n   Fault Tolerance – Automatic recovery from node failures. \\n   Parallelism – Runs tasks across multiple nodes simultaneously. \\n   Ease of Use – Handles low-level distributed computing complexities. \\n \\n5. Conclusion \\nMapReduce is a highly efficient and scalable data processing model widely used in big \\ndata analytics. Its parallel processing capabilities make it suitable for handling massive \\ndatasets in real-world applications like log analysis, recommendation systems, and \\nmachine learning preprocessing.         \\n4o \\n7. Summarize the features of Hadoop and explain \\nthe functionalities of Hadoop cluster? provide \\nanswer for 15 marks  \\nChatGPT said: \\nHadoop: Features & Cluster Functionalities \\n1. Features of Hadoop \\nHadoop is an open-source framework for storing and processing large-scale data across'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 35, 'page_label': '36', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='answer for 15 marks  \\nChatGPT said: \\nHadoop: Features & Cluster Functionalities \\n1. Features of Hadoop \\nHadoop is an open-source framework for storing and processing large-scale data across \\na distributed computing environment. Its key features include: \\nA. Scalability \\n• Can handle petabytes of data by adding new nodes to the cluster without affecting \\nperformance. \\nB. Fault Tolerance \\n• Uses HDFS (Hadoop Distributed File System) to replicate data across multiple \\nnodes, ensuring reliability even if nodes fail.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Distributed Processing \\n• Utilizes the MapReduce programming model for parallel execution, reducing \\nprocessing time. \\nD. Cost-Effective \\n• Runs on commodity hardware, reducing infrastructure costs. \\nE. High Availability \\n• If one node fails, data remains accessible due to multiple copies stored across \\ndifferent nodes. \\nF. Flexibility \\n• Supports structured, semi-structured, and unstructured data from logs, social \\nmedia, emails, and sensors. \\nG. Security & Authentication \\n• Kerberos-based authentication and data encryption provide secure data access. \\n \\n2. Functionalities of a Hadoop Cluster \\nA Hadoop cluster is a group of interconnected computers that work together to store \\nand process massive datasets. It consists of the following key components: \\nA. Master Node (NameNode & ResourceManager) \\n1. NameNode (HDFS Master) \\no Manages metadata (file locations, permissions, hierarchy) in HDFS. \\no Directs DataNodes on where to store and retrieve data. \\n2. ResourceManager (YARN Master)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 36, 'page_label': '37', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. NameNode (HDFS Master) \\no Manages metadata (file locations, permissions, hierarchy) in HDFS. \\no Directs DataNodes on where to store and retrieve data. \\n2. ResourceManager (YARN Master) \\no Allocates resources (CPU, memory) to applications running in the cluster. \\no Manages job scheduling and execution. \\nB. Worker Nodes (DataNodes & NodeManagers) \\n1. DataNodes (HDFS Workers) \\no Store actual data in block format (default 128MB per block). \\no Send heartbeat signals to the NameNode for health monitoring. \\n2. NodeManagers (YARN Workers) \\no Execute tasks assigned by the ResourceManager. \\no Monitor resource usage and report back to the ResourceManager.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Job Execution Process in a Hadoop Cluster \\n1. Data is split into blocks and distributed across DataNodes. \\n2. The NameNode keeps track of data locations. \\n3. The ResourceManager assigns computing resources for MapReduce tasks. \\n4. Mappers process data in parallel and send intermediate results to Reducers. \\n5. Reducers aggregate data and store the final output in HDFS. \\n \\n3. Conclusion \\nHadoop’s scalability, fault tolerance, and distributed processing make it ideal for big \\ndata applications like log analysis, recommendation systems, and real-time analytics. A \\nHadoop cluster efficiently manages data storage and processing, ensuring high \\navailability and performance across large datasets.  \\n \\n \\n8. Apply the limitations of the traditional \\nMapReduce model and how YARN helps overcome \\nthem. provide answer for 15 marks \\nChatGPT said: \\nLimitations of Traditional MapReduce and How YARN Overcomes Them \\n1. Introduction to MapReduce'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 37, 'page_label': '38', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='MapReduce model and how YARN helps overcome \\nthem. provide answer for 15 marks \\nChatGPT said: \\nLimitations of Traditional MapReduce and How YARN Overcomes Them \\n1. Introduction to MapReduce \\nMapReduce is a distributed processing model used in Hadoop for large-scale data \\nprocessing. It follows a two-stage process: \\n1. Map Phase: Processes input data and generates key-value pairs. \\n2. Reduce Phase: Aggregates the intermediate output to produce final results. \\nAlthough MapReduce was revolutionary in handling big data, it had several limitations \\nthat restricted its efficiency and scalability. YARN (Yet Another Resource Negotiator) \\nwas introduced in Hadoop 2.0 to overcome these challenges. \\n \\n2. Limitations of Traditional MapReduce \\nA. Scalability Issues'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Limitation: The JobTracker manages both resource allocation and job scheduling, \\nleading to bottlenecks. \\n• Impact: As clusters grow, JobTracker fails to efficiently manage resources, limiting \\nscalability. \\nB. Inefficient Resource Utilization \\n• Limitation: Each MapReduce job requires separate Map and Reduce slots, leading \\nto resource wastage. \\n• Impact: If only reduce tasks are running, map slots remain idle, reducing cluster \\nefficiency. \\nC. High Latency for Iterative Jobs \\n• Limitation: Each MapReduce job must reload data from HDFS, even if the same \\ndata is required multiple times. \\n• Impact: Machine Learning, Graph Processing, and Iterative Analytics suffer due to \\nrepeated I/O operations. \\nD. Limited Support for Real-Time and Interactive Processing \\n• Limitation: MapReduce is batch-oriented and not optimized for real-time or \\ninteractive queries. \\n• Impact: Not suitable for use cases like stream processing, ad-hoc querying, and \\nreal-time analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 38, 'page_label': '39', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='interactive queries. \\n• Impact: Not suitable for use cases like stream processing, ad-hoc querying, and \\nreal-time analytics. \\nE. Single Point of Failure \\n• Limitation: The JobTracker is a single component responsible for all jobs and \\nresources. \\n• Impact: If JobTracker fails, entire Hadoop processing stops. \\nF. Lack of Multi-Tenancy \\n• Limitation: Hadoop’s MapReduce jobs run in a single execution model, restricting \\nmultiple applications. \\n• Impact: Difficult to run multiple workloads (e.g., Spark, Tez, or Storm) on the same \\ncluster. \\n \\n3. How YARN Overcomes These Limitations \\nYARN decouples job management and resource management, making Hadoop more \\nefficient and flexible. \\nA. Enhanced Scalability with ResourceManager'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Solution: YARN introduces a ResourceManager (RM) and NodeManagers (NM) \\ninstead of JobTracker. \\n• Benefit: Distributes resource management, allowing thousands of nodes to be \\nhandled efficiently. \\nB. Improved Resource Utilization via Dynamic Allocation \\n• Solution: Uses containers instead of fixed slots for Map and Reduce tasks. \\n• Benefit: Resources are dynamically assigned based on actual needs, improving \\nutilization. \\nC. Reduced Latency for Iterative and Interactive Processing \\n• Solution: YARN allows applications like Apache Spark, Tez, and HBase to run \\nalongside MapReduce. \\n• Benefit: In-memory computing reduces latency for iterative jobs like machine \\nlearning and graph analytics. \\nD. Support for Real-Time Processing and Multi-Tenancy \\n• Solution: YARN enables multiple data processing frameworks (e.g., Spark, Flink, \\nStorm) to run concurrently. \\n• Benefit: Hadoop supports real-time, batch, and interactive applications \\nsimultaneously.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 39, 'page_label': '40', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Storm) to run concurrently. \\n• Benefit: Hadoop supports real-time, batch, and interactive applications \\nsimultaneously. \\nE. Fault Tolerance and High Availability \\n• Solution: JobTracker responsibilities are split into ResourceManager and \\nApplicationMaster. \\n• Benefit: Eliminates a single point of failure, making Hadoop more resilient. \\nF. Better Cluster Utilization \\n• Solution: Dynamic resource allocation prevents idle resources. \\n• Benefit: Maximizes cluster efficiency and reduces execution time for jobs. \\n \\n4. Conclusion \\nWhile traditional MapReduce had limitations like poor resource utilization, high \\nlatency, and lack of multi-tenancy, YARN provided a more efficient, scalable, and \\nflexible framework for Hadoop. By separating job scheduling and resource \\nmanagement, YARN supports multiple processing engines, better fault tolerance, and \\nimproved performance, making Hadoop suitable for modern big data workloads.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='UNIT 3 15 marks \\n1.Explain in detail about NoSQL databases and \\nschema less models . Justify the need for the same. \\nprovide answer for 15 marks \\nNoSQL Databases and Schema-less Models \\n1. Introduction to NoSQL Databases \\nNoSQL databases (Not Only SQL) are modern database management systems designed to \\nhandle large volumes of data that are often unstructured or semi-structured. Unlike \\ntraditional relational databases (RDBMS) that follow a strict schema with predefined \\ntables and relationships, NoSQL databases provide flexibility, scalability, and high \\nperformance by allowing dynamic schema and distributed storage. \\nNoSQL databases were developed to address the limitations of relational databases, \\nespecially in handling Big Data, real-time applications, and cloud computing environments. \\n \\n2. Characteristics of NoSQL Databases \\nNoSQL databases have the following key characteristics: \\n1. Schema-Less Data Model – No predefined schema is required, allowing easy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 40, 'page_label': '41', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. Characteristics of NoSQL Databases \\nNoSQL databases have the following key characteristics: \\n1. Schema-Less Data Model – No predefined schema is required, allowing easy \\nmodifications to data structures. \\n2. Scalability – NoSQL databases support horizontal scaling, meaning they can expand \\nacross multiple servers. \\n3. High Availability – Designed for distributed architecture, ensuring redundancy and \\nfault tolerance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. High-Speed Read/Write Operations – Optimized for handling real-time data \\nprocessing. \\n5. Flexible Data Storage – Supports structured, semi-structured, and unstructured \\ndata formats like JSON, XML, BSON, etc. \\n \\n3. Types of NoSQL Databases and Their Applications \\nNoSQL databases are classified based on their data model: \\nA. Key-Value Stores \\n• Data is stored as key-value pairs (like a dictionary). \\n• Provides fast lookups and is ideal for caching and session management. \\n• Examples: Redis, DynamoDB, Riak. \\n• Use Case: Storing user session data in web applications. \\nB. Document Stores \\n• Stores data in document format (JSON, BSON, or XML). \\n• Supports nested structures and dynamic fields. \\n• Examples: MongoDB, CouchDB, ArangoDB. \\n• Use Case: Storing customer profiles in e-commerce platforms. \\nC. Column-Family Stores \\n• Data is stored in columns rather than rows, making it efficient for large-scale data \\nprocessing. \\n• Suitable for handling big data workloads.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 41, 'page_label': '42', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='C. Column-Family Stores \\n• Data is stored in columns rather than rows, making it efficient for large-scale data \\nprocessing. \\n• Suitable for handling big data workloads. \\n• Examples: Apache Cassandra, HBase. \\n• Use Case: Analyzing real-time user behavior in social media platforms. \\nD. Graph Databases \\n• Designed to store and analyze complex relationships between data. \\n• Used in applications requiring network analysis and recommendation systems. \\n• Examples: Neo4j, Amazon Neptune. \\n• Use Case: Social media relationship mapping and fraud detection. \\n \\n4. Understanding Schema-Less Models'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A schema-less model means that data can be stored without a predefined structure. \\nUnlike RDBMS, where each table has a fixed schema, NoSQL databases allow documents \\nor records to have different attributes, making them flexible and adaptable. \\nAdvantages of Schema-Less Models \\n✔ Flexibility – Developers can modify data structures without downtime. \\n✔ Scalability – Easier to distribute across multiple servers. \\n✔ Performance – Faster read/write operations due to denormalized storage. \\n✔ Ease of Integration – Works well with cloud-native and microservices architectures. \\n \\n5. Justification for Using NoSQL & Schema-Less Models \\nA. Big Data & Real-Time Processing \\n• Traditional relational databases struggle with high-velocity and high-volume data. \\n• NoSQL databases efficiently handle large-scale distributed data for real-time \\nanalytics. \\n• Example: Social media platforms processing millions of posts per second. \\nB. Horizontal Scalability'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• NoSQL databases efficiently handle large-scale distributed data for real-time \\nanalytics. \\n• Example: Social media platforms processing millions of posts per second. \\nB. Horizontal Scalability \\n• RDBMS scales vertically (adding resources to a single machine), which is costly. \\n• NoSQL databases scale horizontally by distributing data across multiple servers, \\nreducing costs. \\n• Example: Cloud applications like Netflix store terabytes of user data using a \\nhorizontally scalable NoSQL architecture. \\nC. Handling Unstructured Data \\n• RDBMS enforces strict data formats, making it hard to store JSON, multimedia, \\nlogs, or IoT data. \\n• NoSQL databases can handle varied data formats without restructuring. \\n• Example: Storing user-generated content in a blogging platform. \\nD. Cloud and Microservices Adoption \\n• NoSQL databases support distributed computing and microservices-based \\narchitectures. \\n• Enable high availability, auto-scaling, and fast processing in cloud environments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 42, 'page_label': '43', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• NoSQL databases support distributed computing and microservices-based \\narchitectures. \\n• Enable high availability, auto-scaling, and fast processing in cloud environments. \\n• Example: Online banking systems managing millions of transactions in real time.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Comparison: NoSQL vs. RDBMS \\nFeature NoSQL RDBMS \\nSchema Schema-less, flexible Fixed schema, structured \\nScalability Horizontally scalable Vertically scalable \\nPerformance High-speed read/write Slower for large data \\nData Format JSON, XML, Key-Value, Graph Tables with rows & columns \\nUse Case Big Data, IoT, real-time apps Banking, ERP , transactional apps \\n \\n7. Real-World Applications of NoSQL Databases \\n1. Social Media Platforms – Facebook uses Apache Cassandra to manage billions of \\nposts. \\n2. E-commerce Websites – Amazon uses DynamoDB for product recommendations. \\n3. Streaming Services – Netflix stores metadata in Apache Cassandra. \\n4. Banking and Finance – Fraud detection using graph databases like Neo4j. \\n5. IoT Applications – Smart devices store sensor data in MongoDB and CouchDB. \\n \\n8. Challenges of NoSQL Databases \\n1. Lack of Standardization – No universal query language like SQL. \\n2. Complex Transactions – Not ideal for applications requiring ACID compliance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 43, 'page_label': '44', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='8. Challenges of NoSQL Databases \\n1. Lack of Standardization – No universal query language like SQL. \\n2. Complex Transactions – Not ideal for applications requiring ACID compliance. \\n3. Security Risks – Schema-less design can introduce data inconsistency. \\n4. Backup & Recovery – Requires specialized tools for large-scale backups. \\n \\n9. Conclusion \\nNoSQL databases and schema-less models provide a powerful alternative to traditional \\nRDBMS, offering scalability, flexibility, and performance for modern applications. They are \\nessential for big data analytics, cloud computing, IoT, and AI-driven applications. As \\nbusinesses continue to generate vast amounts of unstructured data, NoSQL databases will \\nplay a crucial role in future data management.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. Explain the concept of High-Performance \\nArchitecture in big data processing. \\nHigh-Performance Architecture in Big Data Processing \\n1. Introduction to High-Performance Architecture \\nHigh-Performance Architecture (HPA) in big data processing refers to optimized computing \\nframeworks, storage systems, and network architectures that enable rapid, efficient, and \\nscalable data handling. With the massive growth of structured and unstructured data from \\nsources like IoT, social media, and enterprise applications, traditional architectures \\nstruggle to process data efficiently. \\nHPA leverages parallel processing, distributed computing, and advanced memory \\nmanagement to handle high-volume, high-velocity, and high-variety data. \\n \\n2. Key Components of High-Performance Big Data Architecture \\nA. Distributed Computing Frameworks \\n• Apache Hadoop: Uses HDFS (Hadoop Distributed File System) for storing and \\nMapReduce for parallel processing.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 44, 'page_label': '45', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='A. Distributed Computing Frameworks \\n• Apache Hadoop: Uses HDFS (Hadoop Distributed File System) for storing and \\nMapReduce for parallel processing. \\n• Apache Spark: An in-memory, distributed processing engine that performs real-\\ntime analytics much faster than Hadoop. \\nB. Scalable Storage Systems \\n• HDFS (Hadoop Distributed File System): Stores large datasets across multiple \\nnodes. \\n• NoSQL Databases (MongoDB, Cassandra): Schema-less models that offer scalability. \\n• Cloud Storage (AWS S3, Google Cloud Storage): Provides elastic storage for \\nhandling dynamic workloads. \\nC. Parallel Processing Techniques \\n• Multi-threading and Multi-core Processing: Enhances CPU utilization for faster \\ncomputations. \\n• Batch and Stream Processing:  \\no Batch Processing (Hadoop): Suitable for large-scale, historical data analysis. \\no Stream Processing (Apache Kafka, Apache Flink): Handles real-time data \\nstreams. \\nD. High-Speed Networking and Interconnects'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Software-Defined Networking (SDN): Optimizes data flow and bandwidth \\nallocation. \\n• High-Performance Clusters: Uses InfiniBand, Fiber Optics, and 5G Networks to \\nreduce latency and improve throughput. \\n \\n3. Characteristics of High-Performance Architecture \\nFeature Description \\nScalability Supports horizontal scaling to handle massive data volumes. \\nLow Latency Reduces response time using in-memory processing (Spark, Redis). \\nFault Tolerance Ensures data redundancy and system recovery (HDFS, Replication). \\nReal-time Processing Uses streaming technologies for live data analysis. \\nOptimized Resource \\nUtilization \\nEfficiently manages CPU, memory, and storage using virtualization \\nand containerization (Docker, Kubernetes). \\n \\n4. Workflow of High-Performance Big Data Processing \\n1. Data Ingestion – Capturing data from IoT, logs, transactions, and social media. \\n2. Data Storage – Using HDFS, NoSQL, or cloud-based systems for scalable storage. \\n3. Data Processing –'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 45, 'page_label': '46', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Data Ingestion – Capturing data from IoT, logs, transactions, and social media. \\n2. Data Storage – Using HDFS, NoSQL, or cloud-based systems for scalable storage. \\n3. Data Processing –  \\no Batch Processing (Hadoop MapReduce) for historical data. \\no Real-time Processing (Apache Spark, Kafka) for event-driven analytics. \\n4. Data Analysis & Visualization – Using ML models, BI tools (Tableau, Power BI). \\n5. Data Output & Decision Making – Processed insights are used in AI, business \\nanalytics, and predictive modeling. \\n \\n5. Applications of High-Performance Architecture in Big Data \\n1. E-Commerce & Recommendation Systems – Amazon, Netflix use Spark for real-time \\nrecommendations. \\n2. Healthcare & Genomics – AI-driven diagnosis and genomic analysis with distributed \\ncomputing. \\n3. Financial Fraud Detection – Banks use Spark with Graph Processing for anomaly \\ndetection.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Smart Cities & IoT – Real-time traffic monitoring, energy grid management. \\n5. Cybersecurity – AI-based threat detection & log analysis using high-speed \\ncomputing. \\n \\n6. Advantages of High-Performance Big Data Architecture \\n✔ Faster Processing – In-memory computing reduces latency. \\n✔ Scalable & Cost-Effective – Cloud-based architecture adapts to workload demands. \\n✔ Improved Fault Tolerance – Distributed storage prevents single points of failure. \\n✔ Real-Time Insights – Businesses can act on live data streams. \\n✔ Optimized Hardware Utilization – Uses GPUs, TPUs, and FPGAs for AI workloads. \\n \\n7. Challenges & Future Trends \\n  Challenges: \\n• High infrastructure costs for real-time processing. \\n• Complexity in integrating multi-cloud and hybrid environments. \\n• Security & privacy concerns in distributed computing. \\n        Future Trends: \\n• AI-Driven Data Processing – Automated data pipeline optimization. \\n• Quantum Computing – Future applications for massive data analysis.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 46, 'page_label': '47', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Future Trends: \\n• AI-Driven Data Processing – Automated data pipeline optimization. \\n• Quantum Computing – Future applications for massive data analysis. \\n• Edge Computing – 5G-powered low-latency processing for IoT & autonomous \\nsystems. \\n \\n8. Conclusion \\nHigh-Performance Architecture in big data processing enables organizations to process \\nmassive datasets efficiently while ensuring scalability, low latency, and real-time insights. \\nTechnologies like Apache Spark, Hadoop, NoSQL, cloud computing, and AI-powered \\nanalytics will continue shaping the next generation of big data solutions.         \\n3.Describe schema-less models in NoSQL. How do \\nthey enhance data flexibility? providde answetr for 15 \\nmarks question  \\nChatGPT said:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Schema-less Models in NoSQL and Their Role in Enhancing Data Flexibility \\n1. Introduction to Schema-less Models in NoSQL \\nA schema-less model in NoSQL databases means that the database does not enforce a \\nrigid, predefined schema like traditional relational databases (RDBMS). Instead, data can \\nbe stored in a flexible format, allowing dynamic schema evolution based on application \\nneeds. \\nThis flexibility is particularly useful in big data, real-time applications, and unstructured \\ndata storage, where data structures frequently change. \\n \\n2. Types of Schema-less NoSQL Databases \\nNoSQL databases are categorized based on how they store data: \\nA. Document Stores (e.g., MongoDB, CouchDB) \\n• Stores data in JSON, BSON, or XML formats. \\n• Each document can have different fields, structures, or types. \\n• Example:  \\njson \\nCopyEdit \\n{ \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\" } \\n{ \"name\": \"Bob\", \"email\": \"bob@example.com\", \"purchases\": [\"Laptop\", \"Phone\"] }'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 47, 'page_label': '48', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Example:  \\njson \\nCopyEdit \\n{ \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\" } \\n{ \"name\": \"Bob\", \"email\": \"bob@example.com\", \"purchases\": [\"Laptop\", \"Phone\"] } \\no Alice’s record has age & city, while Bob’s has email & purchases—no fixed \\nschema! \\nB. Key-Value Stores (e.g., Redis, DynamoDB) \\n• Data stored as key-value pairs. \\n• No constraints on the type of values (text, JSON, binary, etc.). \\n• Example:  \\njson \\nCopyEdit \\n\"user_101\" : { \"name\": \"Alice\", \"age\": 25 } \\n\"session_567\" : { \"lastLogin\": \"2024-03-20\", \"active\": true } \\no Different keys store different types of values without predefined schema. \\nC. Column-Family Stores (e.g., Cassandra, HBase)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Data stored in flexible columns, allowing variable attributes per row. \\n• Example:  \\nsql \\nCopyEdit \\nRow ID | Name  | Age | Email   \\n1      | Alice | 25  | -   \\n2      | Bob   | -   | bob@example.com   \\no Some rows may not have certain columns, unlike traditional RDBMS. \\nD. Graph Databases (e.g., Neo4j, ArangoDB) \\n• Stores nodes & relationships without fixed schema. \\n• Example:  \\nscss \\nCopyEdit \\n(Alice) -[:FRIENDS_WITH]-> (Bob) \\n(Bob) -[:PURCHASED]-> (\"Laptop\") \\no Flexible structures allow dynamic relationships. \\n \\n3. How Schema-less Models Enhance Data Flexibility \\n   A. Dynamic Schema Evolution \\n• NoSQL databases do not require ALTER TABLE commands. \\n• Applications can add new fields without downtime. \\n   B. Handling Unstructured & Semi-Structured Data \\n• Supports JSON, XML, key-value pairs, which can store logs, social media posts, and \\nIoT data. \\n• Example: A social media platform can store varying user details dynamically. \\n   C. Scalability & Performance Boost'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 48, 'page_label': '49', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='IoT data. \\n• Example: A social media platform can store varying user details dynamically. \\n   C. Scalability & Performance Boost \\n• Schema-less NoSQL databases support horizontal scaling, ideal for big data \\napplications. \\n• Example: Cassandra scales across multiple nodes efficiently. \\n   D. Multi-format Data Storage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Unlike relational databases, which enforce uniform structure, schema-less models \\nallow storing heterogeneous data. \\n• Example: An e-commerce database can store both customer reviews (text) and \\nproduct specs (JSON) in the same collection. \\n   E. Faster Development & Agile Adaptability \\n• Developers can iterate faster without modifying schemas. \\n• Startups & AI-driven applications benefit from schema-less flexibility. \\n \\n4. Real-World Applications of Schema-less NoSQL Models \\nIndustry Use Case NoSQL Type \\nSocial Media Storing user posts, comments, likes Document Store \\n(MongoDB) \\nE-Commerce Product catalogs with varied \\nattributes Column Store (Cassandra) \\nIoT & Sensors Streaming real-time sensor data Key-Value Store (Redis) \\nFraud Detection Graph-based fraud detection Graph DB (Neo4j) \\nAI & Machine \\nLearning Storing unstructured training datasets Document Store (CouchDB) \\n \\n5. Advantages of Schema-less NoSQL Models \\nAdvantage Description'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 49, 'page_label': '50', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='AI & Machine \\nLearning Storing unstructured training datasets Document Store (CouchDB) \\n \\n5. Advantages of Schema-less NoSQL Models \\nAdvantage Description \\nFlexibility Data can change dynamically without altering database \\nschema. \\nScalability NoSQL databases scale horizontally, handling massive data \\nloads. \\nPerformance Faster queries with distributed architecture (Cassandra, \\nDynamoDB). \\nMulti-format Data \\nSupport Stores structured, semi-structured, and unstructured data. \\nHigh Availability Supports replication & partitioning for data redundancy.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='6. Challenges of Schema-less NoSQL Models \\nChallenge Solution \\nLack of ACID Compliance Use eventual consistency models for distributed transactions. \\nComplex Queries Use indexing & caching (Redis, Elasticsearch) for optimization. \\nData Integrity Issues Implement application-layer validation to ensure data quality. \\n \\n7. Conclusion \\nSchema-less models in NoSQL databases provide unmatched flexibility, scalability, and \\nadaptability for modern applications. Whether handling real-time data, unstructured \\ninformation, or high-velocity transactions, schema-less NoSQL solutions like MongoDB, \\nCassandra, Redis, and Neo4j are transforming big data storage, AI systems, and cloud \\napplications.         \\n4.Describe about document store and key value store \\nprovide answer in 15 marks detail approach \\nChatGPT said: \\nDocument Store and Key-Value Store in NoSQL \\n1. Introduction to NoSQL Databases \\nNoSQL databases are designed to handle large-scale, unstructured, or semi-structured'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 50, 'page_label': '51', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='ChatGPT said: \\nDocument Store and Key-Value Store in NoSQL \\n1. Introduction to NoSQL Databases \\nNoSQL databases are designed to handle large-scale, unstructured, or semi-structured \\ndata efficiently. Among the various types, Document Stores and Key-Value Stores play a \\ncrucial role in modern applications due to their scalability, flexibility, and high \\nperformance. \\n \\n2. Document Store \\nA. Definition \\nA document store is a NoSQL database that stores data in semi-structured formats such as \\nJSON, BSON, or XML. Each document contains a self-contained dataset that can have a \\nunique structure, making document stores highly flexible and schema-less. \\nB. Features of Document Stores \\n• Schema-flexibility: No predefined schema is required. \\n• Hierarchical Storage: Stores data as nested key-value pairs inside a document. \\n• Indexing & Querying: Allows complex queries using indexes on JSON fields.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Scalability: Supports horizontal scaling via sharding and replication. \\n• Embedded Relationships: Stores arrays and nested objects within documents. \\nC. Structure of a Document Store (Example in MongoDB) \\nA JSON-like document stored in MongoDB: \\njson \\nCopyEdit \\n{ \\n  \"_id\": \"101\", \\n  \"name\": \"Alice\", \\n  \"age\": 25, \\n  \"email\": \"alice@example.com\", \\n  \"orders\": [ \\n    { \"item\": \"Laptop\", \"price\": 1000 }, \\n    { \"item\": \"Mouse\", \"price\": 50 } \\n  ] \\n} \\n• Each document contains key-value pairs that store user information dynamically. \\n• Arrays (orders) allow storing multiple related objects inside a single document. \\nD. Examples of Document Store Databases \\nDatabase Description \\nMongoDB The most popular document store with JSON-like documents. \\nCouchDB Stores data in JSON format with RESTful APIs for querying. \\nRethinkDB Real-time document storage with continuous data updates. \\nFirebase Firestore A cloud-based NoSQL document database for mobile apps. \\nE. Use Cases of Document Stores'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 51, 'page_label': '52', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='RethinkDB Real-time document storage with continuous data updates. \\nFirebase Firestore A cloud-based NoSQL document database for mobile apps. \\nE. Use Cases of Document Stores \\nIndustry Application \\nE-commerce Storing customer profiles, product catalogs dynamically. \\nSocial Media Storing user posts, comments, likes in a flexible format.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Industry Application \\nHealthcare Managing patient records, prescriptions, and reports. \\nIoT & AI Handling sensor data & AI-generated unstructured datasets. \\n \\n3. Key-Value Store \\nA. Definition \\nA key-value store is the simplest form of a NoSQL database where data is stored as a \\nunique key and an associated value. These databases are highly optimized for read/write \\nspeed, making them ideal for caching and real-time applications. \\nB. Features of Key-Value Stores \\n• Simple Data Storage: Stores data as key-value pairs (similar to a dictionary). \\n• High Performance: Fast retrieval using hash indexing. \\n• Scalability: Easily scalable by distributing keys across multiple nodes. \\n• Eventual Consistency: Supports distributed caching with minimal latency. \\n• Schema-less: The value can be strings, JSON, binary, or even entire objects. \\nC. Structure of a Key-Value Store (Example in Redis) \\nA key-value pair stored in Redis: \\nsql \\nCopyEdit'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 52, 'page_label': '53', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Schema-less: The value can be strings, JSON, binary, or even entire objects. \\nC. Structure of a Key-Value Store (Example in Redis) \\nA key-value pair stored in Redis: \\nsql \\nCopyEdit \\nSET user_101 \"{ \\'name\\': \\'Alice\\', \\'age\\': 25, \\'email\\': \\'alice@example.com\\' }\" \\nGET user_101 \\n• The key (\"user_101\") is mapped to a JSON-like object containing user details. \\n• Unlike document stores, key-value databases lack built-in query mechanisms and \\nrely on key lookups. \\nD. Examples of Key-Value Store Databases \\nDatabase Description \\nRedis An in-memory key-value store used for caching and real-time processing. \\nDynamoDB AWS managed NoSQL database for highly available applications. \\nRiak KV A distributed key-value store optimized for availability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Database Description \\nBerkeley DB A lightweight embedded key-value database. \\nE. Use Cases of Key-Value Stores \\nIndustry Application \\nWeb Applications Session storage, authentication tokens (e.g., JWT, OAuth). \\nGaming Storing leaderboards, player stats, real-time game states. \\nE-commerce Caching product details, frequently accessed data. \\nIoT Storing real-time sensor readings, device configurations. \\n \\n4. Comparison: Document Store vs. Key-Value Store \\nFeature Document Store Key-Value Store \\nData Format JSON, BSON, XML Key-value pairs (String, Binary, \\nJSON) \\nSchema Schema-less (Flexible structure) Schema-less (No structure) \\nQuerying Supports complex queries on nested \\nfields Only key-based retrieval \\nPerformance Fast but slower than key-value stores Ultra-fast lookup operations \\nUse Case Semi-structured data (profiles, logs, \\nproducts) Caching, real-time applications \\n \\n5. Conclusion \\nBoth document stores and key-value stores serve critical roles in modern NoSQL \\ndatabases:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 53, 'page_label': '54', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Use Case Semi-structured data (profiles, logs, \\nproducts) Caching, real-time applications \\n \\n5. Conclusion \\nBoth document stores and key-value stores serve critical roles in modern NoSQL \\ndatabases: \\n• Document Stores (MongoDB, CouchDB) offer schema-flexibility and powerful \\nqueries for dynamic applications like e-commerce and social media. \\n• Key-Value Stores (Redis, DynamoDB) provide blazing-fast read/write speeds for use \\ncases like caching, gaming, and real-time analytics. \\nUnderstanding their strengths allows businesses to choose the right database based on \\napplication needs, ensuring scalability, flexibility, and high performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Explain in detail about system architecture of Hbase \\nSystem Architecture of HBase \\n1. Introduction to HBase \\nHBase is a distributed, column-oriented NoSQL database built on top of Hadoop’s HDFS \\n(Hadoop Distributed File System). It is designed to handle large-scale, real-time data \\nprocessing with high availability and scalability. \\nKey Characteristics of HBase: \\n• Modeled after Google’s Bigtable \\n• Stores structured data in tables \\n• Supports random reads and writes \\n• Provides high throughput for real-time applications \\n• Runs on top of HDFS for fault tolerance \\n \\n2. HBase System Architecture \\nHBase follows a master-slave architecture with three main components: \\nA. HBase Master Server \\nThe HBase Master Server manages and coordinates the RegionServers in the cluster. \\nFunctions of HBase Master: \\n1. Assigns regions (data partitions) to RegionServers. \\n2. Monitors RegionServers for health checks and load balancing. \\n3. Handles schema changes (table creation, deletion, and updates).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 54, 'page_label': '55', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='1. Assigns regions (data partitions) to RegionServers. \\n2. Monitors RegionServers for health checks and load balancing. \\n3. Handles schema changes (table creation, deletion, and updates). \\n4. Manages splits and merges of regions dynamically. \\n5. Coordinates with ZooKeeper for failover and recovery. \\nB. RegionServers \\nRegionServers are responsible for handling read/write requests and managing data \\nstorage. \\nFunctions of RegionServers: \\n1. Stores actual data in HBase tables. \\n2. Handles client read/write requests. \\n3. Manages regions (subsets of tables) assigned by the Master.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='4. Caches frequently accessed data for fast retrieval. \\n5. Flushes in-memory data to HDFS for durability. \\nEach RegionServer is responsible for multiple regions, and regions grow over time until \\nthey split into smaller ones. \\nC. ZooKeeper (Coordination Service) \\nHBase relies on ZooKeeper, a distributed coordination service, to manage cluster state. \\nFunctions of ZooKeeper in HBase: \\n1. Tracks active HBase Masters and RegionServers. \\n2. Manages leader election and failover. \\n3. Maintains metadata and configuration details. \\n4. Ensures synchronization between distributed components. \\n \\n3. Storage Architecture of HBase \\nHBase organizes data in a highly optimized format for high-speed reads and writes. The \\ncore storage architecture consists of: \\nA. Tables, Rows, and Column Families \\n• Data is stored in tables, consisting of rows and column families. \\n• Each row is identified by a unique row key. \\n• Column families group related columns together for better storage efficiency.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 55, 'page_label': '56', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='• Data is stored in tables, consisting of rows and column families. \\n• Each row is identified by a unique row key. \\n• Column families group related columns together for better storage efficiency. \\nB. HFile (Storage on HDFS) \\nHBase stores data in HFiles on HDFS for durability and scalability. \\n• Data is first written to the Write-Ahead Log (WAL) to prevent data loss. \\n• Then, it is stored in MemStore (in-memory) before being flushed to HFiles on disk. \\nC. MemStore and HFile (Data Write Path) \\n1. Client writes data to MemStore (in-memory buffer). \\n2. Data is logged in Write-Ahead Log (WAL) to ensure durability. \\n3. When MemStore reaches a threshold, it flushes data to HFiles on HDFS. \\n4. Compaction merges smaller HFiles into larger ones to optimize read performance. \\nD. Data Read Path \\n1. Client sends a read request to the RegionServer.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='2. RegionServer checks MemStore (if data is in-memory). \\n3. If not found, it searches in HFiles (on disk in HDFS). \\n4. Uses Bloom Filters and block cache to speed up reads. \\n \\n4. HBase Write and Read Operations \\nA. Write Operation (Insert/Update Data) \\n1. Client sends a write request to the appropriate RegionServer. \\n2. Data is first written to the Write-Ahead Log (WAL). \\n3. Data is then stored in MemStore (in-memory cache). \\n4. Once MemStore is full, data is flushed to disk (HFile on HDFS). \\n5. Periodic compaction merges HFiles to optimize storage. \\nB. Read Operation (Retrieve Data) \\n1. Client sends a read request to the RegionServer. \\n2. RegionServer first checks MemStore for recent data. \\n3. If data is not found, it checks BlockCache (cached data). \\n4. If still not found, it reads from HFiles on HDFS. \\n5. Data is returned to the client. \\n \\n5. HBase vs. Traditional RDBMS \\nFeature HBase (NoSQL) Traditional RDBMS \\nData Model Column-Oriented Row-Oriented'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 56, 'page_label': '57', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='5. Data is returned to the client. \\n \\n5. HBase vs. Traditional RDBMS \\nFeature HBase (NoSQL) Traditional RDBMS \\nData Model Column-Oriented Row-Oriented \\nSchema Schema-less Fixed Schema \\nScalability Horizontally Scalable Limited Scalability \\nData Storage Distributed (HDFS) Centralized \\nRead/Write Speed High for big data Slower for large datasets \\nUse Case Real-time Big Data Processing OLTP Transactions \\n \\n6. Use Cases of HBase'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T07:38:50+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-20T07:38:50+05:30', 'source': '..\\\\data\\\\pdf\\\\bda ccet1.pdf', 'total_pages': 58, 'page': 57, 'page_label': '58', 'source_file': 'bda ccet1.pdf', 'file_type': 'pdf'}, page_content='Industry Application \\nSocial Media Storing user activity logs, messages, and posts \\nFinance Managing real-time stock market transactions \\nE-commerce Handling product catalogs and customer interactions \\nHealthcare Storing patient records and medical history \\nIoT & AI Processing sensor data from connected devices \\n \\n7. Conclusion \\nHBase is a highly scalable, real-time, column-oriented NoSQL database built for big data \\napplications. Its master-slave architecture, distributed storage on HDFS, and integration \\nwith Hadoop make it a powerful choice for applications requiring fast reads, writes, and \\nhigh availability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='(SET 1 15 marks) \\n1. Customer wants you to develop the software for the remote controlled electronic toy. \\nCustomer is not sure of all requirements for the product. She is provided with initial set of \\nrequirement with which she wants to have feel of the product. She has informed that few \\nmore requirements will be provided later. Explain observations made in this scenario and \\nwhich model will be suitable for this requirement. \\nChatGPT said: \\nObservations in the Given Scenario: \\n1. Unclear and Evolving Requirements – The customer has only provided an initial set \\nof requirements and plans to introduce more in the future. This indicates that the \\nproject scope is not fully defined at the start. \\n2. Incremental Development Expected – Since the customer wants to have a \"feel\" of \\nthe product with the available requirements, an iterative approach would be \\nbeneficial to incorporate feedback and evolving needs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='the product with the available requirements, an iterative approach would be \\nbeneficial to incorporate feedback and evolving needs. \\n3. Customer Involvement is Key – The customer will be actively engaged, providing \\nnew requirements after evaluating initial versions of the software. \\n4. Need for Flexibility – A rigid development model (like the Waterfall model) may not \\nbe suitable since new requirements will emerge, necessitating adaptability. \\n5. Prototyping is Essential – The need for an early demonstration of the product \\nsuggests that an initial prototype or working version should be developed before \\nfinalizing the full requirements. \\nSuitable Software Development Model: Incremental Model \\nThe Incremental Model is the most suitable approach for this scenario because: \\n1. Iterative Delivery – The project can be broken into multiple increments. The first \\nincrement will be developed based on the initial requirements, and subsequent'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Iterative Delivery – The project can be broken into multiple increments. The first \\nincrement will be developed based on the initial requirements, and subsequent \\nincrements will incorporate new requirements as they are provided. \\n2. Customer Feedback Integration – Each increment allows the customer to test the \\nproduct, suggest modifications, and refine requirements, ensuring better alignment \\nwith their expectations. \\n3. Risk Reduction – Since the development happens in stages, issues can be identified \\nand fixed in earlier increments, reducing the risk of major failures later. \\n4. Faster Time-to-Market – A basic version of the software can be delivered early, \\nallowing the customer to interact with it before all features are implemented. \\n5. Flexibility to Accommodate Changes – New requirements can be easily incorporated \\nin upcoming increments without disrupting the entire project.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Alternative Model: Prototype Model \\nAnother possible approach is the Prototype Model, where an initial prototype is developed \\nbased on the given requirements. The customer can review it, suggest modifications, and \\nrefine the requirements before proceeding with the final development. However, if \\ncontinuous additions to the product are expected, the Incremental Model remains the \\nbetter choice. \\nConclusion \\nSince the customer is uncertain about all requirements and expects to refine them after \\ntesting an initial version, the Incremental Model is the most suitable approach. It ensures \\nearly delivery, flexibility, and continuous customer involvement, leading to a well-refined and \\ncustomer-centric final product. \\nObservations in the Given Scenario \\n1. Unclear and Evolving Requirements \\no The customer has only provided an initial set of requirements and has \\nexplicitly mentioned that more requirements will be given later.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Unclear and Evolving Requirements \\no The customer has only provided an initial set of requirements and has \\nexplicitly mentioned that more requirements will be given later. \\no This suggests that the customer does not have a complete vision of the final \\nproduct at the beginning of development. \\no The software development process must accommodate changes and new \\nrequirements as they emerge. \\n2. Customer Wants to Experience the Product Early \\no The customer wants to \"have a feel\" of the product before finalizing \\nadditional requirements. \\no This indicates the need for early software delivery so the customer can \\ninteract with it and provide feedback. \\no A development model that allows for early prototyping or incremental \\ndelivery is required. \\n3. Continuous Customer Involvement is Needed \\no Since the customer will be providing new requirements after testing the initial \\nversion, frequent interaction between the development team and the \\ncustomer is necessary.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o Since the customer will be providing new requirements after testing the initial \\nversion, frequent interaction between the development team and the \\ncustomer is necessary. \\no The development approach should support continuous collaboration, \\nfeedback, and modifications. \\n4. Risk of Major Changes in Requirements \\no Given that the requirements are incomplete, there is a possibility of \\nsignificant changes in design, functionality, or scope during development.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o A rigid model like the Waterfall Model would be unsuitable because it does \\nnot accommodate requirement changes once development has started. \\n5. Prototype or Working Version Needed \\no Since the customer wants to experience the product before finalizing all \\nrequirements, a development approach that supports prototyping or early-\\nstage working versions of the product is essential. \\n \\nMost Suitable Software Development Model: Incremental Model \\nThe Incremental Model is the best choice for this scenario due to its flexibility, iterative \\nnature, and ability to incorporate evolving requirements. \\nExplanation of the Incremental Model \\nThe Incremental Model is a software development approach where the system is built in \\nmultiple small parts (increments). Each increment adds additional features to the existing \\nsystem based on user feedback and newly defined requirements. \\nHow the Incremental Model Works in This Scenario \\n1. First Increment'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='system based on user feedback and newly defined requirements. \\nHow the Incremental Model Works in This Scenario \\n1. First Increment \\no The development team builds the first version of the software based on the \\ninitial set of requirements. \\no This provides the customer with an early version to test and give feedback. \\n2. Customer Feedback & New Requirements \\no After testing the first increment, the customer provides additional \\nrequirements and modifications. \\no The development team incorporates these in the next increment. \\n3. Subsequent Increments \\no Each new increment builds upon the previous version, integrating newly \\nidentified requirements. \\no The process continues until the final product is complete. \\nAdvantages of Using the Incremental Model in This Case \\n   Accommodates Changing Requirements \\n• Since the customer has evolving needs, the incremental model allows new \\nrequirements to be added in later stages. \\n   Early Product Delivery'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"• A functional part of the software is delivered early, enabling the customer to interact \\nwith it and refine their expectations. \\n   Better Risk Management \\n• The project is developed in smaller parts, reducing risks compared to building the \\nentire system at once. \\n   Customer Involvement Throughout Development \\n• The customer actively participates in each increment, ensuring that the final product \\naligns with their expectations. \\n   Faster Identification and Resolution of Issues \\n• Bugs and design flaws can be detected early in the development process, reducing \\noverall project risks. \\n \\nAlternative Approach: Prototype Model \\nIf the customer is unsure about the product's functionality and wants an early \\ndemonstration before development starts, the Prototype Model could also be considered. \\nHow the Prototype Model Works \\n1. A quick prototype (basic working model) is created using initial requirements. \\n2. The customer reviews the prototype and suggests modifications.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='How the Prototype Model Works \\n1. A quick prototype (basic working model) is created using initial requirements. \\n2. The customer reviews the prototype and suggests modifications. \\n3. The prototype is refined until the customer is satisfied. \\n4. Once finalized, the actual software development begins based on the refined \\nprototype. \\n   Suitable if the customer is unclear about core functionalities. \\n   Helps in requirement gathering and validation before full-scale development. \\n   Not ideal for continuous requirement changes. \\nSince the customer in this case will be providing additional requirements throughout the \\ndevelopment, the Incremental Model remains the best approach over the Prototype Model. \\n \\nConclusion \\nFor a remote-controlled electronic toy with evolving requirements, the Incremental Model is \\nthe best choice. It enables: \\n• Early delivery of working software, \\n• Continuous customer involvement,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"• Integration of new requirements at later stages, \\n• Better risk management, \\n• Higher flexibility and adaptability to changes. \\nThus, the Incremental Model ensures a structured yet flexible development approach, \\ndelivering a product that meets the customer's evolving expectations efficiently.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='SET2(15 marks) \\n1. Customer has provided you with all the requirements and also assured there will not be \\nany change in the requirement. He expects deliverables at every stage of development. You \\nhave carried out the similar project earlier and you are sure that the project could be \\nexecuted systemically. Explain observations made in this scenario and which model will be \\nsuitable for this requirement.  \\nChatGPT said: \\nObservations in the Given Scenario \\n1. Clearly Defined and Fixed Requirements \\no The customer has provided a complete and well-defined set of requirements. \\no There will be no changes in requirements throughout the project lifecycle. \\no This eliminates the need for an iterative or adaptive model that \\naccommodates changes. \\n2. Expectation of Deliverables at Every Stage \\no The customer expects progress to be demonstrated at each stage of \\ndevelopment. \\no This suggests a need for a structured development approach with clearly'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o The customer expects progress to be demonstrated at each stage of \\ndevelopment. \\no This suggests a need for a structured development approach with clearly \\ndefined phases and deliverables at each stage. \\n3. Previous Experience with Similar Projects \\no The development team has already worked on a similar project before. \\no This indicates that the team has prior knowledge, making it easier to estimate \\ntimelines and execute the project efficiently. \\no There is low uncertainty, reducing the need for prototyping or iterative \\nrefinement. \\n4. Systematic Execution is Feasible \\no Since the project requirements are well-defined and the team is experienced, \\na well-structured development process can be followed without disruptions. \\no There is no need for frequent customer interaction beyond the predefined \\nmilestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario due to its structured, phase-wise'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='milestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario due to its structured, phase-wise \\napproach that aligns well with the given conditions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Explanation of the Waterfall Model \\nThe Waterfall Model is a linear, sequential approach where each phase is completed before \\nmoving on to the next. It follows a strict order, ensuring that deliverables are provided at the \\nend of each stage. \\nPhases of the Waterfall Model and How They Apply to This Project \\n1. Requirement Gathering and Analysis \\no Since the requirements are already fully defined and fixed, this phase will be \\nstraightforward. \\no The development team will document all requirements to ensure clarity. \\n2. System Design \\no Based on the requirements, the system architecture and design will be \\ncreated. \\no This includes database design, system interfaces, and technical specifications. \\no A deliverable (design document) can be provided to the customer at this \\nstage. \\n3. Implementation (Coding) \\no The actual development (coding) takes place based on the finalized design. \\no Modules are developed and integrated systematically.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='stage. \\n3. Implementation (Coding) \\no The actual development (coding) takes place based on the finalized design. \\no Modules are developed and integrated systematically. \\no Deliverables can include partially completed modules or milestone updates. \\n4. Testing \\no The entire system undergoes rigorous testing after implementation. \\no Unit testing, integration testing, and system testing ensure that the software \\nmeets all requirements. \\no The customer can receive a test report as a deliverable at this stage. \\n5. Deployment \\no The software is deployed for customer use. \\no Since no changes are expected, this phase will be straightforward. \\n6. Maintenance \\no Although the requirements are fixed, maintenance may still be required for \\nbug fixes and optimizations. \\no The customer will receive regular updates regarding software stability and \\nperformance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Why the Waterfall Model is the Best Choice \\n   Well-Suited for Fixed Requirements \\n• Since the customer has clearly defined requirements and no changes are expected, \\nthe Waterfall Model ensures structured execution without disruptions. \\n   Systematic Execution \\n• Each phase follows a strict order, ensuring that deliverables are provided at every \\nstage as per the customer’s expectations. \\n   Predictability and Efficiency \\n• The development team has prior experience with similar projects, making \\nestimations more accurate and execution more efficient. \\n   Minimizes Risk \\n• Since all requirements are known from the beginning, risks associated with \\nrequirement changes or scope creep are eliminated. \\n   Clear Customer Expectations \\n• The customer knows what to expect at each stage, as the Waterfall Model provides \\nformal documentation and structured progress updates. \\n \\nAlternative Approach: V-Model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='• The customer knows what to expect at each stage, as the Waterfall Model provides \\nformal documentation and structured progress updates. \\n \\nAlternative Approach: V-Model \\nIf the project requires early validation and continuous testing at every stage, the V-Model \\n(Validation & Verification Model) could also be considered. \\nHow the V-Model Works \\n• It follows the same structured approach as the Waterfall Model but integrates testing \\nat every stage. \\n• Each development phase has a corresponding testing phase, ensuring issues are \\ndetected early. \\n   Ensures high-quality software through continuous testing. \\n   Minimizes post-development defects. \\n   Not necessary if testing at the final stage is sufficient. \\nSince the scenario does not specify a need for early-stage validation, the Waterfall Model \\nremains the best choice for this project.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Conclusion \\nFor a project with fixed requirements, systematic execution, and predefined stage-wise \\ndeliverables, the Waterfall Model is the most suitable choice. It ensures: \\n• A structured development process, \\n• Predictable timelines and outcomes, \\n• Formal deliverables at every stage, \\n• Minimal risk due to well-defined requirements. \\nThus, the Waterfall Model provides a clear and efficient approach, ensuring successful \\nproject completion with minimal complications. \\n \\n                                                                        [OR] \\nObservations in the Given Scenario \\n1. Well-Defined and Fixed Requirements \\no The customer has provided a complete and unchanging set of requirements. \\no This means there is no need for continuous requirement gathering or \\niterative refinements. \\no There is little to no risk of scope creep (where additional features are \\nrequested mid-development). \\n2. Expectation of Deliverables at Every Stage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='iterative refinements. \\no There is little to no risk of scope creep (where additional features are \\nrequested mid-development). \\n2. Expectation of Deliverables at Every Stage \\no The customer wants structured updates and deliverables at each stage. \\no This suggests the need for a formal, well-documented approach where each \\nphase produces a tangible output before moving to the next. \\n3. Previous Experience with Similar Projects \\no The development team has already successfully executed a similar project. \\no This indicates that: \\n▪ The project’s complexity is understood. \\n▪ Development timelines and risks can be estimated accurately. \\n▪ There is no need for a prototype or experimental approach. \\n4. Systematic Execution is Feasible \\no Since the requirements are fixed and the development team is experienced, a \\nstructured and disciplined execution approach will work best.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o There is no need for high customer involvement beyond predefined \\nmilestones. \\n \\nMost Suitable Software Development Model: Waterfall Model \\nThe Waterfall Model is the best choice for this scenario because it follows a structured, \\nlinear, and sequential approach to development. \\n \\nDetailed Explanation of the Waterfall Model \\nThe Waterfall Model follows a step-by-step execution process, where each phase is \\ncompleted before moving to the next. It is best suited when the requirements are fixed, \\ndeliverables are expected at each stage, and the project can be executed systematically. \\nPhases of the Waterfall Model and Their Relevance to This Project \\n1. Requirement Analysis Phase \\no Since the customer has already provided a complete and unchanging set of \\nrequirements, this phase will focus on: \\n▪ Documenting the requirements formally in a Software Requirement \\nSpecification (SRS) document. \\n▪ Reviewing the requirements with the customer to ensure there are no \\ngaps.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='▪ Documenting the requirements formally in a Software Requirement \\nSpecification (SRS) document. \\n▪ Reviewing the requirements with the customer to ensure there are no \\ngaps. \\n▪ Identifying constraints and dependencies early on. \\no    Deliverable: Finalized SRS document approved by the customer. \\n2. System Design Phase \\no In this phase, the system’s architecture is designed based on the SRS. \\no It involves: \\n▪ High-level design (HLD): Defines system architecture, modules, and \\ncomponents. \\n▪ Low-level design (LLD): Specifies detailed logic for each module. \\n▪ Database design: Designing data storage and access mechanisms. \\no    Deliverable: System Design Document (SDD) with diagrams, workflows, \\nand technical specifications. \\n3. Implementation (Coding) Phase'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o The actual development (coding) of the system begins based on the approved \\ndesign. \\no Since the team has prior experience with similar projects, coding can be \\ndone efficiently with minimal risk. \\no The development is modular, meaning individual components can be tested \\nbefore integration. \\no    Deliverable: Working software modules or intermediate builds at \\npredefined milestones. \\n4. Integration and Testing Phase \\no After coding, the system is assembled and tested to ensure it meets the \\nspecified requirements. \\no Testing involves: \\n▪ Unit testing: Testing individual modules for correctness. \\n▪ Integration testing: Ensuring different modules work together. \\n▪ System testing: Verifying overall functionality and performance. \\n▪ Acceptance testing: Ensuring the system meets customer \\nexpectations. \\no Since the requirements are fixed, testing is straightforward, focusing on \\nverification rather than exploratory changes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='▪ Acceptance testing: Ensuring the system meets customer \\nexpectations. \\no Since the requirements are fixed, testing is straightforward, focusing on \\nverification rather than exploratory changes. \\no    Deliverable: Test reports and a fully tested software system. \\n5. Deployment Phase \\no The system is delivered to the customer and deployed in the intended \\nenvironment. \\no This phase includes: \\n▪ Installation and configuration on customer infrastructure. \\n▪ User training (if required). \\n▪ Final acceptance testing by the customer. \\no    Deliverable: Fully operational software deployed at the customer site. \\n6. Maintenance Phase \\no While the requirements will not change, bug fixes, performance \\noptimization, and minor updates may be required. \\no The team provides support and maintenance as per the agreement.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='o    Deliverable: Software updates and maintenance reports. \\n \\nWhy the Waterfall Model is the Best Choice for This Project \\n   Well-Suited for Fixed Requirements \\n• Since the customer has already provided a complete and unchanging set of \\nrequirements, the Waterfall Model ensures that the project is executed \\nsystematically with no need for requirement adjustments. \\n   Structured Development Process \\n• The Waterfall Model follows a step-by-step approach where each phase is \\ncompleted before moving to the next. This matches the customer’s expectation of \\ndeliverables at every stage. \\n   Predictability and Efficiency \\n• The development team has prior experience with similar projects, making it easier \\nto estimate timelines, costs, and risks. \\n• Since the process is structured, project tracking is straightforward and transparent. \\n   Minimizes Risks and Errors \\n• Since testing is conducted only after implementation, defects are detected before \\ndeployment.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Minimizes Risks and Errors \\n• Since testing is conducted only after implementation, defects are detected before \\ndeployment. \\n• The project follows a systematic verification process, ensuring a high-quality final \\nproduct. \\n   Clear Customer Expectations and Milestones \\n• The customer knows exactly what to expect at each stage since the Waterfall Model \\nhas predefined phases and deliverables. \\n• There is no need for continuous feedback and iteration, reducing delays and scope \\ncreep. \\n \\nAlternative Approach: V-Model (Verification & Validation Model) \\nIf testing at every phase is a major priority, the V-Model could be considered. It is an \\nenhanced version of the Waterfall Model where each development phase has a \\ncorresponding testing phase. \\nKey Features of the V-Model: \\n• Follows the same structured approach as the Waterfall Model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='• Testing is performed after every phase rather than waiting until the end. \\n• Ensures early defect detection and improves software quality. \\n   Better for projects where high reliability is required. \\n   Reduces cost of fixing bugs by identifying them early. \\n   Not necessary if post-development testing is sufficient. \\nSince the scenario does not emphasize the need for early-stage validation, the Waterfall \\nModel remains the best choice. \\n \\nConclusion \\nFor a project where all requirements are fixed, deliverables are expected at each stage, and \\nsystematic execution is feasible, the Waterfall Model is the most appropriate choice. It \\nensures: \\n  Structured execution with clearly defined phases \\n  Predictable timelines and outcomes \\n  Minimal risk due to fixed requirements \\n  Efficient project tracking and milestone-based deliverables \\nThus, the Waterfall Model provides a well-organized and disciplined development'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Minimal risk due to fixed requirements \\n  Efficient project tracking and milestone-based deliverables \\nThus, the Waterfall Model provides a well-organized and disciplined development \\napproach, ensuring successful project completion while meeting customer expectations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"SET 1(7 & 8 marks): \\n(8 marks): \\nApply Bayesian Classification method with an example for class based modeling. provide \\ndetailed answer \\nChatGPT said: \\nApplying Bayesian Classification Method for Class-Based Modeling \\n1. Introduction to Bayesian Classification \\nBayesian Classification is based on Bayes' Theorem, which provides a probabilistic approach \\nto classification. It is widely used in machine learning, spam detection, medical diagnosis, \\nand text classification. \\nBayes' Theorem Formula \\n \\n2. Class-Based Modeling Using Bayesian Classification \\nClass-based modeling defines a set of classes representing different objects in a system, \\nwhere Bayesian classification helps predict the class of a new instance. \\nExample: Email Spam Detection (Class-Based Model with Bayesian Classification) \\nLet's consider a binary classification problem, where we classify an email as Spam (S) or Not \\nSpam (NS). \\nStep 1: Define Classes \\n• C1=C_1 =C1= Spam \\n• C2=C_2 =C2= Not Spam\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content=\"Let's consider a binary classification problem, where we classify an email as Spam (S) or Not \\nSpam (NS). \\nStep 1: Define Classes \\n• C1=C_1 =C1= Spam \\n• C2=C_2 =C2= Not Spam \\nStep 2: Collect Training Data (Feature Extraction)\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Assume we have a dataset containing words like \"free\", \"win\", \"offer\", \"urgent\", which are \\ncommon in spam emails. The dataset looks like: \\n \\n \\n \\n3. Applications of Bayesian Classification in Class-Based Modeling'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1. Medical Diagnosis → Classifying diseases based on symptoms. \\n2. Customer Segmentation → Predicting if a customer belongs to a premium or regular \\nclass. \\n3. Fraud Detection → Identifying fraudulent transactions in banking. \\n4. Sentiment Analysis → Classifying reviews as positive or negative. \\n \\n4. Conclusion \\nBayesian Classification provides a probabilistic and interpretable approach to class-based \\nmodeling. It is effective for spam filtering, medical diagnostics, and fraud detection, as \\ndemonstrated in the email classification example.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='(7 marks): \\n(i) Build the various architectural styles and design for software design process development.  \\nChatGPT said: \\nArchitectural Styles and Design for Software Development \\nSoftware architecture defines the structure, behavior, and interaction of software \\ncomponents. Different architectural styles are used based on system requirements like \\nscalability, maintainability, and performance. \\n \\n1. Common Architectural Styles \\n1.1 Layered Architecture \\n  Description: Organizes software into multiple layers where each layer performs a specific \\nfunction. \\n  Best for: Large enterprise applications, web applications, and software with clear \\nseparation of concerns. \\nExample: Web Application Architecture \\nLayers: \\n1. Presentation Layer (UI): Handles user interactions. \\n2. Business Logic Layer: Processes business rules. \\n3. Data Access Layer: Manages database operations. \\n4. Database Layer: Stores and retrieves data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='2. Business Logic Layer: Processes business rules. \\n3. Data Access Layer: Manages database operations. \\n4. Database Layer: Stores and retrieves data. \\n     Example Technologies: React (UI), Node.js (Business Logic), MySQL (Database). \\n \\n1.2 Client-Server Architecture \\n  Description: Divides system into client (user interface) and server (backend processing). \\n  Best for: Web applications, online banking, cloud services. \\nExample: Online Banking System \\n1. Client: Web/Mobile app sending requests. \\n2. Server: Processes transactions, verifies data. \\n3. Database: Stores user data securely. \\n     Example Technologies: JavaScript (Client), Spring Boot (Server), PostgreSQL (Database).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1.3 Microservices Architecture \\n  Description: Breaks software into independent services that communicate via APIs. \\n  Best for: Scalable cloud applications, e-commerce, streaming platforms. \\nExample: E-commerce Application \\n• User Service: Handles authentication. \\n• Product Service: Manages product catalog. \\n• Payment Service: Handles transactions. \\n     Example Technologies: Docker, Kubernetes, REST APIs, AWS. \\n \\n1.4 Event-Driven Architecture \\n  Description: Components communicate through events, ensuring asynchronous \\nprocessing. \\n  Best for: Real-time applications, IoT, stock market systems. \\nExample: IoT Smart Home System \\n1. Sensors detect motion. \\n2. Event is published to an event queue. \\n3. Security system triggers an alarm. \\n     Example Technologies: Kafka, RabbitMQ, MQTT. \\n \\n1.5 Model-View-Controller (MVC) Architecture \\n  Description: Separates application logic into Model, View, and Controller. \\n  Best for: Web applications, GUI-based applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='1.5 Model-View-Controller (MVC) Architecture \\n  Description: Separates application logic into Model, View, and Controller. \\n  Best for: Web applications, GUI-based applications. \\nExample: Web Framework (Django, Laravel, ASP .NET MVC) \\n1. Model: Manages data (Database). \\n2. View: Displays information (UI). \\n3. Controller: Handles requests (Logic). \\n     Example Technologies: Django (Python), Laravel (PHP), Angular. \\n \\n1.6 Pipe-and-Filter Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-28T08:05:53+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-28T08:05:53+05:30', 'source': '..\\\\data\\\\pdf\\\\Part B OE.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'Part B OE.pdf', 'file_type': 'pdf'}, page_content='Description: Data flows through filters (processing units) connected by pipes (data \\nstream). \\n  Best for: Data processing systems, AI/ML pipelines. \\nExample: Image Processing System \\n1. Filter 1: Read image file. \\n2. Filter 2: Apply color correction. \\n3. Filter 3: Save processed image. \\n     Example Technologies: Apache Spark, TensorFlow pipelines. \\n \\n1.7 Service-Oriented Architecture (SOA) \\n  Description: Uses reusable services that communicate via SOAP or REST. \\n  Best for: Enterprise applications, ERP systems. \\nExample: Airline Reservation System \\n• Flight Booking Service \\n• Payment Processing Service \\n• Notification Service \\n     Example Technologies: SOAP , XML, RESTful APIs. \\n \\nConclusion \\nChoosing the right architectural style ensures software is scalable, maintainable, and \\nefficient. Layered, Microservices, Event-Driven, and MVC are some of the most commonly \\nused architectures depending on business needs.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='1.Five Phases of Product Lifecycle Development – Detailed Explanation \\nProduct Lifecycle Development (PLM) is a strategic approach that ensures a product evolves \\nefficiently from its initial concept to its end-of-life phase. This structured process helps organizations \\nstreamline product development, optimize resources, reduce costs, and improve overall product \\nquality. \\n \\n1. Concept & Ideation Phase (Idea Generation & Feasibility) \\nObjective: \\nTo generate and evaluate innovative product ideas based on market needs, technological feasibility, \\nand business goals. \\nKey Activities: \\n1. Market Research & Consumer Insights: \\no Identifying industry trends and market gaps. \\no Conducting customer surveys, focus groups, and competitor analysis. \\no Understanding pain points and unmet needs. \\n2. Brainstorming & Innovation: \\no Encouraging idea generation within R&D teams. \\no Utilizing design thinking methodologies for creative solutions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Understanding pain points and unmet needs. \\n2. Brainstorming & Innovation: \\no Encouraging idea generation within R&D teams. \\no Utilizing design thinking methodologies for creative solutions. \\no Engaging stakeholders (marketing, sales, and engineering) to validate concepts. \\n3. Technical & Economic Feasibility Study: \\no Assessing the technological feasibility of the product. \\no Performing cost analysis, return on investment (ROI), and risk assessments. \\no Evaluating sustainability and regulatory compliance. \\n4. Product Requirement Definition: \\no Drafting high-level functional and technical requirements. \\no Setting initial specifications, including size, performance, and materials. \\no Identifying potential constraints (legal, environmental, and financial). \\nOutcome: \\n• Clear Product Concept Document (PCD) with specifications. \\n• Feasibility reports and go/no-go decision for product development. \\n \\n2. Design & Development Phase (Turning the Idea into a Prototype)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Objective: \\nTo transform the concept into a tangible product through detailed design, prototyping, and testing. \\nKey Activities: \\n1. Engineering & Product Design: \\no Creating Computer-Aided Design (CAD) models for visualization. \\no Selecting materials based on durability, cost, and sustainability. \\no Conducting stress tests, fluid dynamics simulations, and thermal analysis. \\n2. Prototype Development & Testing: \\no Building physical or digital prototypes for testing. \\no Running pilot tests to validate functionality, safety, and efficiency. \\no Gathering user feedback for improvements. \\n3. Software & Electronics Development (If Applicable): \\no Developing embedded software for IoT-enabled or digital products. \\no Ensuring cybersecurity measures and firmware updates. \\n4. Bill of Materials (BOM) & Supply Chain Planning: \\no Creating a multi-level BOM to define required parts and materials. \\no Identifying suppliers and manufacturers. \\n5. Regulatory & Compliance Checks:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Creating a multi-level BOM to define required parts and materials. \\no Identifying suppliers and manufacturers. \\n5. Regulatory & Compliance Checks: \\no Ensuring adherence to industry safety standards. \\no Getting necessary certifications (e.g., ISO, CE, FDA approvals). \\nOutcome: \\n• Fully functional prototype ready for validation. \\n• Finalized engineering documentation (BOM, design blueprints, regulatory reports). \\n \\n3. Manufacturing & Production Phase (Scaling Up for Market Readiness) \\nObjective: \\nTo transition from prototype to large-scale production while optimizing quality, efficiency, and cost. \\nKey Activities: \\n1. Production Process Planning: \\no Determining manufacturing methods (e.g., 3D printing, injection molding, CNC \\nmachining).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='o Optimizing production workflow for minimal waste. \\n2. Factory Setup & Automation: \\no Implementing smart manufacturing technologies such as IoT and AI-driven \\nautomation. \\no Establishing quality control stations and in-line testing. \\n3. Supply Chain & Logistics Optimization: \\no Coordinating with suppliers to ensure material availability. \\no Managing inventory using Enterprise Resource Planning (ERP) systems. \\n4. Quality Control & Testing: \\no Running stress, endurance, and safety tests on mass-produced units. \\no Conducting failure analysis and corrective actions. \\n5. Packaging & Distribution Readiness: \\no Designing sustainable packaging solutions to minimize waste. \\no Establishing distribution networks for global reach. \\nOutcome: \\n• Fully manufactured product meeting all safety, compliance, and market requirements. \\n• Scalability plan for mass production. \\n \\n4. Launch & Market Deployment Phase (Bringing the Product to Customers) \\nObjective:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• Scalability plan for mass production. \\n \\n4. Launch & Market Deployment Phase (Bringing the Product to Customers) \\nObjective: \\nTo introduce the product to the market through strategic sales, marketing, and distribution efforts. \\nKey Activities: \\n1. Go-To-Market (GTM) Strategy: \\no Defining pricing strategies (premium, penetration, skimming). \\no Selecting appropriate sales channels (e-commerce, retail, B2B partnerships). \\n2. Marketing & Branding: \\no Developing advertising campaigns (digital, social media, traditional). \\no Conducting product launch events and demos. \\n3. Customer Support & Service Preparation: \\no Setting up after-sales support teams for issue resolution. \\no Creating training materials for customers and retailers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='4. Feedback Collection & Market Adaptation: \\no Monitoring customer feedback via online reviews and surveys. \\no Adjusting marketing strategies based on sales performance. \\nOutcome: \\n• Successful market penetration and brand awareness. \\n• Sales data for performance evaluation. \\n \\n5. Service, Maintenance & End-of-Life (EOL) Phase (Sustaining & Retiring the Product) \\nObjective: \\nTo maintain product quality, enhance customer experience, and plan for its eventual phase-out. \\nKey Activities: \\n1. Customer Support & Warranty Management: \\no Providing help desks, chatbots, and service centers. \\no Handling repairs, replacements, and upgrades. \\n2. Software Updates & Security Patches (For Digital Products): \\no Rolling out firmware updates to fix vulnerabilities. \\no Ensuring compatibility with evolving technologies. \\n3. Product Enhancement & Redesign (If Applicable): \\no Gathering user feedback for next-gen product development. \\no Addressing common issues for an improved version.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='3. Product Enhancement & Redesign (If Applicable): \\no Gathering user feedback for next-gen product development. \\no Addressing common issues for an improved version. \\n4. End-of-Life (EOL) Planning & Sustainable Disposal: \\no Phasing out obsolete products responsibly. \\no Implementing circular economy strategies (recycling, repurposing). \\nOutcome: \\n• Extended product lifespan through continued support and updates. \\n• Responsible retirement or next-generation product planning. \\n \\nConclusion \\nThe five-phase PLM approach ensures products are innovative, efficiently developed, successfully \\nlaunched, and well-maintained. By leveraging advanced technologies like AI, IoT, and digital twins, \\ncompanies can optimize each phase, improve customer satisfaction, and drive long-term profitability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='7. Explain the steps for implementing a PLM strategy i want the answer for 15 marks so provide a \\ndetailed answer \\nChatGPT said: \\nSteps for Implementing a Product Lifecycle Management (PLM) Strategy \\nImplementing a PLM strategy is a complex yet critical process that enables businesses to manage a \\nproduct’s lifecycle from ideation to disposal. A well-structured PLM strategy enhances collaboration, \\nefficiency, innovation, and cost-effectiveness across the product’s journey. Below are the key steps \\nfor implementing a PLM strategy, explained in detail. \\n \\n1. Define Clear Business Objectives & Goals \\nBefore implementing PLM, organizations must clearly define their strategic objectives. These goals \\nshould align with the company’s overall business strategy. \\nKey Considerations: \\n• What problems will PLM solve? \\no Reduce product development time? \\no Improve collaboration? \\no Ensure regulatory compliance? \\n• Set Measurable Goals: \\no Reduce time-to-market by 20%.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• What problems will PLM solve? \\no Reduce product development time? \\no Improve collaboration? \\no Ensure regulatory compliance? \\n• Set Measurable Goals: \\no Reduce time-to-market by 20%. \\no Improve design efficiency by 30%. \\no Increase product reuse by 40%.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA car manufacturer wants to reduce design errors and improve collaboration between global teams \\nby implementing a PLM system. \\n \\n2. Conduct a Current State Assessment & Gap Analysis \\nA thorough assessment of the current product development process helps identify existing \\nchallenges, inefficiencies, and areas for improvement. \\nKey Considerations: \\n• Analyze Existing Processes & Tools: \\no How is product data managed today? \\no Are teams using spreadsheets, emails, or outdated software? \\n• Identify Gaps & Challenges: \\no Lack of real-time collaboration between teams. \\no Difficulty in tracking design changes and approvals. \\nExample: \\nAn electronics company finds that using manual document control for product data causes frequent \\nversion mismatches and production errors. \\n \\n3. Select the Right PLM System \\nChoosing the right PLM software is crucial for effective implementation. The selection should be \\nbased on the company’s size, industry, complexity, and long-term goals.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Choosing the right PLM software is crucial for effective implementation. The selection should be \\nbased on the company’s size, industry, complexity, and long-term goals. \\nKey Considerations: \\n• PLM Features to Look for: \\no Cloud-based or on-premise? \\no Integration with ERP , CAD, and other systems? \\no User-friendly interface and accessibility? \\n• Popular PLM Solutions: \\no Siemens Teamcenter \\no PTC Windchill \\no Dassault Systèmes ENOVIA \\no Autodesk Fusion Lifecycle \\nExample:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='A medical device company selects PTC Windchill because it ensures FDA compliance and \\ntraceability for medical regulations. \\n \\n4. Develop a PLM Implementation Roadmap \\nA step-by-step implementation plan ensures smooth adoption of PLM without disrupting existing \\noperations. \\nKey Considerations: \\n• Phased Implementation vs. Full Rollout: \\no Start with a pilot program before enterprise-wide rollout. \\no Prioritize critical departments (e.g., R&D, Engineering). \\n• Implementation Timeline: \\no Phase 1: System Setup & Data Migration (3 months) \\no Phase 2: Integration with CAD & ERP (6 months) \\no Phase 3: Enterprise-wide Adoption (12 months) \\nExample: \\nAn aerospace company first implements PLM in design and engineering, then gradually expands it \\nto manufacturing and supply chain management. \\n \\n5. Migrate & Standardize Product Data \\nPLM requires clean, structured, and standardized product data for efficiency. This step involves data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='to manufacturing and supply chain management. \\n \\n5. Migrate & Standardize Product Data \\nPLM requires clean, structured, and standardized product data for efficiency. This step involves data \\nmigration from legacy systems and ensuring a single source of truth for product information. \\nKey Considerations: \\n• Identify Critical Data to Migrate: \\no CAD files, BOM (Bill of Materials), compliance records. \\no Supplier information and past project documentation. \\n• Data Standardization: \\no Use common naming conventions and classification systems. \\no Ensure version control to avoid duplication errors. \\nExample: \\nA smartphone manufacturer standardizes its BOM structure in PLM to improve supplier \\ncoordination. \\n \\n6. Integrate PLM with Existing Enterprise Systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='PLM works best when integrated with other business-critical systems like ERP (Enterprise Resource \\nPlanning), CRM (Customer Relationship Management), and MES (Manufacturing Execution \\nSystems). \\nKey Considerations: \\n• Essential Integrations: \\no PLM + ERP: Ensures smooth manufacturing & procurement. \\no PLM + CAD: Seamless design data management. \\no PLM + CRM: Tracks customer feedback for product improvements. \\n• API & Data Flow Management: \\no Ensure real-time synchronization between PLM and other systems. \\nExample: \\nA fashion brand integrates PLM with ERP to manage raw materials, supplier orders, and production \\nschedules. \\n \\n7. Define & Automate Workflow Management \\nPLM improves efficiency by automating workflows such as design approvals, change requests, and \\ncompliance checks. \\nKey Considerations: \\n• Workflow Automation Examples: \\no Automated approval process for design changes. \\no Automatic alerts for compliance violations. \\n• Role-Based Access Control:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Key Considerations: \\n• Workflow Automation Examples: \\no Automated approval process for design changes. \\no Automatic alerts for compliance violations. \\n• Role-Based Access Control: \\no Ensure employees can access only relevant product data. \\nExample: \\nA pharmaceutical company automates FDA documentation approval workflows in its PLM system. \\n \\n8. Train Employees & Change Management \\nEmployees must understand and adapt to the new PLM system. Training and support are critical for \\nsuccessful adoption. \\nKey Considerations: \\n• Employee Training: \\no Conduct PLM workshops and hands-on practice sessions. \\no Provide user manuals and on-demand video tutorials.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='• Change Management Strategies: \\no Address employee resistance by demonstrating PLM benefits. \\no Use pilot programs before full-scale implementation. \\nExample: \\nA car manufacturer trains engineers, supply chain managers, and designers to effectively use PLM \\nfor collaboration. \\n \\n9. Monitor Performance & Continuous Improvement \\nPost-implementation, companies must monitor PLM system performance and make improvements \\nbased on real-time data. \\nKey Considerations: \\n• Key PLM Performance Metrics: \\no Time-to-market reduction. \\no Number of design change requests. \\no Cost savings from improved efficiency. \\n• Continuous Improvement Approach: \\no Regular feedback from employees. \\no System updates and feature enhancements. \\nExample: \\nA consumer electronics company monitors PLM efficiency and adjusts workflows to further optimize \\nproduct development cycles. \\n \\nConclusion: Why a Structured PLM Strategy is Essential'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA consumer electronics company monitors PLM efficiency and adjusts workflows to further optimize \\nproduct development cycles. \\n \\nConclusion: Why a Structured PLM Strategy is Essential \\nA well-planned PLM strategy streamlines product development, collaboration, cost efficiency, and \\ninnovation. Companies that implement PLM effectively benefit from: \\n   Reduced time-to-market (faster product launches). \\n   Better data accuracy & version control. \\n   Improved supply chain coordination. \\n   Seamless compliance & regulatory management. \\nBy following these nine steps, organizations can achieve sustainable product innovation and market \\nleadership.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='12,4. Importance of Process Consistency in \\nProduct Lifecycle Management (PLM) \\nIntroduction \\nProcess consistency is a fundamental pillar of effective Product Lifecycle Management (PLM). It \\nensures standardized workflows, minimizes errors, enhances collaboration, and improves overall \\nefficiency in product development. PLM integrates multiple processes across design, manufacturing, \\nsupply chain, and customer service, making consistency crucial for seamless operations. \\nIn this detailed explanation, we will explore why process consistency is important and how it \\ncontributes to an organization’s efficiency, compliance, quality, and innovation. \\n \\n1. Enhances Efficiency & Reduces Operational Bottlenecks \\nWhy It Matters? \\nInconsistent processes lead to delays, confusion, and inefficiencies in product development. A lack \\nof uniformity in workflows causes teams to waste time resolving discrepancies, redoing tasks, and \\nhandling miscommunications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='of uniformity in workflows causes teams to waste time resolving discrepancies, redoing tasks, and \\nhandling miscommunications. \\nBenefits of Process Consistency in Efficiency: \\n   Streamlined Workflows: Standardized procedures enable teams to follow clear steps for design, \\napprovals, and production. \\n   Faster Product Development: Consistent workflows minimize time spent on unnecessary \\nrevisions or approvals. \\n   Optimized Resource Utilization: Teams can focus on innovation rather than fixing process-\\nrelated issues.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA smartphone manufacturer follows a standardized change request approval process, reducing \\ndesign iteration delays and accelerating time-to-market. \\n \\n2. Ensures Compliance with Industry Standards & Regulations \\nWhy It Matters? \\nMany industries, such as automotive, aerospace, healthcare, and electronics, have strict regulatory \\nrequirements (ISO, FDA, CE, etc.). Without process consistency, tracking compliance documents and \\napprovals becomes chaotic. \\nBenefits of Process Consistency in Compliance: \\n   Traceability & Audit Readiness: Ensures all design changes, approvals, and certifications are \\nwell-documented. \\n   Reduces Risk of Regulatory Penalties: Prevents non-compliance issues that could lead to legal \\nactions or product recalls. \\n   Simplifies Certification Processes: Structured processes enable faster certification and approval \\nfrom regulatory bodies. \\n        Example: \\nA pharmaceutical company maintains a consistent documentation process for FDA approvals,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='from regulatory bodies. \\n        Example: \\nA pharmaceutical company maintains a consistent documentation process for FDA approvals, \\npreventing delays in drug launches. \\n \\n3. Improves Collaboration Across Departments & Supply Chains \\nWhy It Matters? \\nModern product development involves multiple departments and global supply chains. Without \\nconsistency, different teams may use varied formats, tools, and workflows, leading to \\nmiscommunication and data silos. \\nBenefits of Process Consistency in Collaboration: \\n   Clear Communication Between Teams: Design, engineering, manufacturing, and suppliers follow \\na common workflow. \\n   Seamless Data Sharing: Standardized data formats ensure compatibility across PLM, ERP, and \\nCAD systems. \\n   Faster Issue Resolution: A structured change management process ensures everyone adapts to \\nmodifications quickly. \\n        Example: \\nA car manufacturer uses a standardized PLM platform for real-time collaboration between design'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='modifications quickly. \\n        Example: \\nA car manufacturer uses a standardized PLM platform for real-time collaboration between design \\nteams in the USA, Germany, and Japan, reducing errors in component specifications. \\n \\n4. Enhances Product Quality & Reduces Errors'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Why It Matters? \\nInconsistent processes can lead to design defects, manufacturing errors, and quality control \\nfailures. Standardization ensures error-free product development by defining clear quality \\ncheckpoints. \\nBenefits of Process Consistency in Quality Control: \\n   Early Detection of Defects: Ensures rigorous testing and validation at every stage. \\n   Reduces Manufacturing Errors: A structured Bill of Materials (BOM) process eliminates \\nmisconfigurations. \\n   Improves Customer Satisfaction: Ensures products meet or exceed quality expectations. \\n        Example: \\nAn aerospace company follows a standardized engineering change order (ECO) process, reducing \\nerrors in aircraft component assembly. \\n \\n5. Facilitates Scalability & Adaptability for Future Growth \\nWhy It Matters? \\nOrganizations need to scale their operations and adapt to new technologies. Without process \\nconsistency, scaling leads to chaos, inefficiencies, and quality issues.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Why It Matters? \\nOrganizations need to scale their operations and adapt to new technologies. Without process \\nconsistency, scaling leads to chaos, inefficiencies, and quality issues. \\nBenefits of Process Consistency in Scalability: \\n   Easier Onboarding of New Employees & Suppliers: Standardized processes allow quick \\nadaptation. \\n   Smooth Expansion into New Markets: Ensures regulatory compliance and operational \\nalignment across regions. \\n   Enables Digital Transformation: Structured workflows help organizations adopt AI, IoT, and \\ncloud-based PLM solutions. \\n        Example: \\nA tech company expands its manufacturing to Asia and seamlessly integrates new suppliers due to \\nstandardized PLM workflows. \\n \\n6. Strengthens Change Management & Version Control \\nWhy It Matters? \\nFrequent product design changes are common in industries like electronics, automotive, and \\nmedical devices. A lack of process consistency leads to mismanaged updates, version confusion, \\nand costly recalls.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='medical devices. A lack of process consistency leads to mismanaged updates, version confusion, \\nand costly recalls. \\nBenefits of Process Consistency in Change Management: \\n   Clear Change Request Approvals: Prevents uncontrolled modifications to product designs. \\n   Improved Version Control: Ensures teams work on the latest design versions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Minimizes Risk of Production Errors: Prevents manufacturers from using outdated product \\nspecifications. \\n        Example: \\nA smartwatch company maintains a single version of truth in PLM, preventing outdated firmware \\nfrom being installed on new models. \\n \\n7. Enables Cost Reduction & Maximizes ROI \\nWhy It Matters? \\nInconsistent processes lead to waste, rework, and inefficiencies that increase costs. Standardized \\nworkflows optimize costs and maximize return on investment (ROI). \\nBenefits of Process Consistency in Cost Optimization: \\n   Minimizes Rework & Waste: Reduces errors in design and production. \\n   Optimizes Resource Utilization: Prevents overproduction, material waste, and redundant efforts. \\n   Reduces Product Recalls & Failures: Ensures error-free product launches. \\n        Example: \\nA home appliance manufacturer cut production costs by 15% by standardizing supply chain \\nprocurement and assembly workflows. \\n \\nConclusion: Why Process Consistency is a Game-Changer'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content=\"A home appliance manufacturer cut production costs by 15% by standardizing supply chain \\nprocurement and assembly workflows. \\n \\nConclusion: Why Process Consistency is a Game-Changer \\nKey Takeaways: \\n✔ Ensures efficiency & faster time-to-market. \\n✔ Guarantees compliance & regulatory approval. \\n✔ Improves collaboration across teams & suppliers. \\n✔ Reduces errors & enhances product quality. \\n✔ Supports scalability & future growth. \\n✔ Strengthens change management & version control. \\n✔ Optimizes costs & maximizes ROI. \\nA structured, consistent PLM process is essential for companies aiming to innovate, reduce risks, \\nand stay competitive in today's fast-paced market.         \\n \\nWould you like an industry-specific case study on how Tesla, Apple, or Boeing leverage process \\nconsistency in PLM? Let me know!        \\n3.Explain the importance of product reuse and \\nsmart parts . \\nChatGPT said:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Importance of Product Reuse and Smart Parts in PLM \\nIntroduction \\nProduct reuse and smart parts are essential strategies in Product Lifecycle Management (PLM) to \\nenhance efficiency, reduce costs, and improve product quality. Organizations in industries like \\nautomotive, aerospace, electronics, and manufacturing leverage these concepts to accelerate \\ninnovation while maintaining sustainability and profitability. \\nIn this detailed explanation, we will explore: \\n• The importance of product reuse \\n• The role of smart parts \\n• Their benefits in PLM \\n \\n1. Importance of Product Reuse in PLM \\nWhat is Product Reuse? \\nProduct reuse refers to the practice of reusing existing designs, components, or modules in new \\nproduct development instead of creating them from scratch. This strategy enhances efficiency and \\npromotes sustainability. \\nKey Benefits of Product Reuse \\n   1. Reduces Development Time and Costs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='product development instead of creating them from scratch. This strategy enhances efficiency and \\npromotes sustainability. \\nKey Benefits of Product Reuse \\n   1. Reduces Development Time and Costs \\n  Eliminates the need for reinventing the wheel by leveraging existing parts and designs. \\n  Saves time and resources on design, testing, and validation. \\n  Engineers can focus on innovation rather than repetitive tasks. \\n        Example: \\nA smartphone company reuses the battery module and camera sensors from its previous model in a \\nnew version, significantly reducing R&D time and costs. \\n \\n   2. Enhances Product Quality and Reliability \\n  Reused components are already tested and validated, reducing defects. \\n  Improves product consistency by using proven designs. \\n  Minimizes unexpected failures in production. \\n        Example: \\nAn automotive manufacturer uses the same braking system across multiple car models, ensuring \\nreliability and performance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Minimizes unexpected failures in production. \\n        Example: \\nAn automotive manufacturer uses the same braking system across multiple car models, ensuring \\nreliability and performance. \\n \\n   3. Improves Supply Chain Efficiency'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Standardized parts streamline procurement and inventory management. \\n  Reduces dependency on multiple suppliers, lowering supply chain risks. \\n  Enables bulk purchasing, reducing overall costs. \\n        Example: \\nAn aerospace company standardizes fasteners, sensors, and actuators across aircraft models, \\nsimplifying supplier relationships. \\n \\n   4. Supports Sustainability and Environmental Goals \\n  Reduces material waste and lowers carbon footprint. \\n  Encourages eco-friendly manufacturing by minimizing redundant production. \\n  Promotes circular economy principles in product design. \\n        Example: \\nA consumer electronics brand reuses materials from recycled laptops to manufacture new models, \\nreducing electronic waste. \\n \\n   5. Accelerates Innovation and Scalability \\n  Speeds up new product introductions by leveraging existing modules. \\n  Enables mass customization by using a modular design approach. \\n  Ensures faster adaptation to market changes and customer demands.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Enables mass customization by using a modular design approach. \\n  Ensures faster adaptation to market changes and customer demands. \\n        Example: \\nA medical device company reuses core software algorithms from a previous imaging system to \\ndevelop a next-generation scanner with AI capabilities. \\n \\n2. Importance of Smart Parts in PLM \\nWhat are Smart Parts? \\nSmart parts are intelligent, connected components that: \\n   Contain embedded sensors & chips for real-time monitoring. \\n   Enable data-driven decision-making in manufacturing. \\n   Improve predictive maintenance and self-diagnosis. \\nKey Benefits of Smart Parts \\n   1. Enables Real-Time Monitoring & Predictive Maintenance \\n  Smart parts collect data on performance, wear, and failure. \\n  Helps in preventive maintenance, reducing unexpected breakdowns. \\n  Extends product lifespan by optimizing usage conditions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nA jet engine manufacturer installs IoT-enabled sensors in engines to monitor fuel efficiency and \\npredict maintenance needs. \\n \\n   2. Enhances Product Customization & Personalization \\n  Allows dynamic adjustments in performance based on user preferences. \\n  Supports on-the-fly reconfiguration for specific applications. \\n  Improves user experience through adaptive technology. \\n        Example: \\nA smart home appliance brand designs AI-powered washing machines that adjust wash cycles \\nbased on fabric type and weight. \\n \\n   3. Strengthens Supply Chain and Inventory Management \\n  Smart parts enable automated tracking and inventory updates. \\n  Reduce waste by monitoring usage patterns in real time. \\n  Enhances supply chain visibility, preventing shortages. \\n        Example: \\nAn automotive manufacturer uses RFID-equipped smart parts to track components in real time, \\nensuring smooth production flow. \\n \\n   4. Facilitates Digital Twin Implementation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Example: \\nAn automotive manufacturer uses RFID-equipped smart parts to track components in real time, \\nensuring smooth production flow. \\n \\n   4. Facilitates Digital Twin Implementation \\n  Smart parts provide real-time data for digital twin simulations. \\n  Helps in virtual prototyping, troubleshooting, and performance optimization. \\n  Reduces physical testing costs and accelerates innovation. \\n        Example: \\nA manufacturing plant creates a digital twin of its assembly line using data from smart robotic arms, \\noptimizing efficiency. \\n \\n   5. Increases Safety and Compliance \\n  Smart parts monitor temperature, pressure, and stress levels, preventing failures. \\n  Ensures compliance with industry safety regulations. \\n  Alerts operators to hazardous conditions in real time. \\n        Example: \\nAn oil & gas company uses smart pipeline sensors to detect leaks and prevent environmental \\nhazards.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Conclusion: Why Product Reuse & Smart Parts Are Game-Changers in PLM \\nKey Takeaways: \\n✔ Product Reuse reduces costs, accelerates development, and enhances sustainability. \\n✔ Smart Parts improve efficiency, enable real-time monitoring, and enhance product intelligence. \\n✔ Both strategies optimize the PLM process by reducing waste, improving quality, and ensuring \\nregulatory compliance. \\n✔ Organizations adopting reuse and smart part technologies gain a competitive advantage in the \\nglobal market. \\n        Future Outlook: \\n• AI-driven smart part analytics for predictive maintenance. \\n• Blockchain integration for tracking reusable parts in the supply chain. \\n• 3D printing & additive manufacturing for on-demand smart part production. \\nYou said: \\n11.Explain teamcenter workflow designer application \\nprovide detailed answer \\nChatGPT said: \\nTeamcenter Workflow Designer Application – A Detailed Explanation \\nIntroduction to Teamcenter Workflow Designer'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='provide detailed answer \\nChatGPT said: \\nTeamcenter Workflow Designer Application – A Detailed Explanation \\nIntroduction to Teamcenter Workflow Designer \\nTeamcenter Workflow Designer is a critical application within Siemens Teamcenter PLM (Product \\nLifecycle Management) that enables organizations to automate, standardize, and manage business \\nprocesses effectively. It provides a graphical interface for designing, configuring, and optimizing \\nworkflows, ensuring smooth collaboration between teams, departments, and external stakeholders. \\nThis tool is primarily used for: \\n   Automating approval processes (e.g., engineering change orders, document approvals). \\n   Defining roles, responsibilities, and process sequences. \\n   Ensuring process consistency across different product lifecycle stages. \\n \\nKey Features of Teamcenter Workflow Designer \\n1. Graphical Workflow Modeling \\n  Provides a drag-and-drop interface to design workflows visually.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Key Features of Teamcenter Workflow Designer \\n1. Graphical Workflow Modeling \\n  Provides a drag-and-drop interface to design workflows visually. \\n  Users can create flowcharts, task sequences, and decision points. \\n  Reduces the need for manual coding when defining process logic. \\n        Example: \\nAn engineering team sets up an ECN (Engineering Change Notification) workflow, ensuring each \\ndesign change is reviewed by the right stakeholders before implementation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='2. Role-Based Task Assignment \\n  Assigns workflow steps based on user roles and access rights. \\n  Ensures the right engineers, managers, and stakeholders review the task at the right time. \\n  Supports parallel or sequential task execution. \\n        Example: \\nA design change request is automatically routed to the design engineer, quality manager, and \\nproduction lead, ensuring compliance before approval. \\n \\n3. Workflow Triggers & Conditions \\n  Enables workflows to start based on predefined events, conditions, or user actions. \\n  Automates decisions based on data inputs, status updates, or external triggers. \\n  Ensures compliance with standard operating procedures (SOPs). \\n        Example: \\nA manufacturing defect detected in the system automatically triggers a Corrective Action Process in \\nTeamcenter. \\n \\n4. Workflow Process Customization \\n  Users can create custom scripts for advanced automation. \\n  Supports integration with third-party tools and enterprise systems (ERP, MES, etc.).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Teamcenter. \\n \\n4. Workflow Process Customization \\n  Users can create custom scripts for advanced automation. \\n  Supports integration with third-party tools and enterprise systems (ERP, MES, etc.). \\n  Provides flexibility for adapting workflows to unique business needs. \\n        Example: \\nA company integrates Teamcenter with SAP ERP , enabling automatic purchase order creation once a \\npart is approved in Teamcenter. \\n \\n5. Task Status Tracking & Monitoring \\n  Real-time visibility into workflow progress, bottlenecks, and pending tasks. \\n  Provides dashboards & reports for workflow analytics. \\n  Alerts users about pending approvals or overdue tasks. \\n        Example: \\nA project manager receives an alert when an engineering change approval is delayed, allowing them \\nto take corrective action. \\n \\nHow Teamcenter Workflow Designer Works – Step-by-Step Process \\n   Step 1: Define the Workflow Objectives'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Identify the business process to automate (e.g., design approval, change management). \\n✔ Establish workflow rules, required approvals, and role assignments. \\n        Example: \\nA company needs a New Product Introduction (NPI) workflow, ensuring design, compliance, and \\ncost approvals before production starts. \\n \\n   Step 2: Create the Workflow in Teamcenter \\n✔ Open Teamcenter Workflow Designer and select \"Create New Workflow.\" \\n✔ Define tasks, approvals, and decision gates. \\n✔ Configure dependencies (e.g., approvals must happen before production starts). \\n        Example: \\nA supplier part validation workflow includes tasks like supplier audit, material testing, and \\nprocurement approval. \\n \\n   Step 3: Assign Roles & Permissions \\n✔ Assign tasks to specific users, departments, or external stakeholders. \\n✔ Define access control to protect sensitive data. \\n        Example: \\nA regulatory compliance workflow assigns review tasks to engineering, legal, and compliance \\nteams.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Define access control to protect sensitive data. \\n        Example: \\nA regulatory compliance workflow assigns review tasks to engineering, legal, and compliance \\nteams. \\n \\n   Step 4: Configure Workflow Conditions & Triggers \\n✔ Set up automatic triggers for workflow initiation (e.g., upon document submission). \\n✔ Define decision rules (e.g., reject an engineering change if cost exceeds the budget). \\n        Example: \\nIf a cost analysis report exceeds the target budget, the workflow triggers a rework request to the \\nfinance team. \\n \\n   Step 5: Test & Validate the Workflow \\n✔ Run a test case to check for errors, inefficiencies, and bottlenecks. \\n✔ Ensure smooth integration with existing business systems. \\n        Example: \\nA manufacturing company tests an assembly process validation workflow, ensuring that all quality \\nchecks are correctly assigned before implementation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Step 6: Deploy & Monitor Workflow Execution \\n✔ Launch the workflow in a live environment. \\n✔ Track progress using Teamcenter reporting tools. \\n✔ Optimize the process based on real-time feedback and analytics. \\n        Example: \\nA project manager notices approval delays in an engineering change request workflow, prompting a \\nprocess optimization review. \\n \\nBenefits of Using Teamcenter Workflow Designer \\n        1. Increases Process Efficiency \\n✔ Automates repetitive tasks, reducing manual work. \\n✔ Eliminates process delays by ensuring real-time tracking & notifications. \\n        2. Ensures Compliance & Standardization \\n✔ Adheres to industry regulations and company policies. \\n✔ Provides an audit trail for all approvals and workflow actions. \\n        3. Improves Collaboration \\n✔ Connects multiple teams, including engineering, manufacturing, supply chain, and compliance. \\n✔ Provides role-based task assignments, ensuring the right people handle the right tasks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Connects multiple teams, including engineering, manufacturing, supply chain, and compliance. \\n✔ Provides role-based task assignments, ensuring the right people handle the right tasks. \\n        4. Enhances Decision-Making \\n✔ Provides real-time workflow visibility. \\n✔ Helps managers identify bottlenecks and inefficiencies. \\n        5. Reduces Errors & Rework \\n✔ Prevents unauthorized changes in product designs and approvals. \\n✔ Ensures each process follows a predefined sequence, avoiding skipped steps. \\n \\nUse Cases of Teamcenter Workflow Designer \\n     1. Engineering Change Management (ECM) \\nScenario: An automotive company automates design change approvals using workflow designer, \\nensuring regulatory compliance before releasing new vehicle models. \\n     2. New Product Development (NPD) Approval \\nScenario: A medical device company standardizes its New Product Development (NPD) workflow, \\nintegrating testing, validation, and FDA compliance.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='2. New Product Development (NPD) Approval \\nScenario: A medical device company standardizes its New Product Development (NPD) workflow, \\nintegrating testing, validation, and FDA compliance. \\n     3. Supplier Onboarding & Part Qualification'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Scenario: An aerospace manufacturer ensures supplier audits, material testing, and compliance \\napprovals before integrating new suppliers into the supply chain. \\n \\nConclusion: Why Use Teamcenter Workflow Designer? \\n✔ Automates & standardizes processes, improving efficiency. \\n✔ Ensures compliance with industry regulations & quality standards. \\n✔ Enhances collaboration across departments & global teams. \\n✔ Reduces errors & improves decision-making through real-time tracking. \\n✔ Boosts product lifecycle management by optimizing workflows for faster execution. \\n9.Distinguish between Current Information Model and \\nPLM model provide detailed answer \\nChatGPT said: \\nDistinguishing Between the Current Information Model and the PLM Model \\nIntroduction \\nInformation management is crucial for engineering, manufacturing, and business processes. Two \\nprimary approaches to handling information are: \\n1. Current Information Model (CIM) – The traditional way organizations store and manage \\ndata.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='primary approaches to handling information are: \\n1. Current Information Model (CIM) – The traditional way organizations store and manage \\ndata. \\n2. Product Lifecycle Management (PLM) Model – A more advanced, structured, and \\ninterconnected way of managing product data and processes throughout the product’s \\nlifecycle. \\n \\n1. Definition of the Two Models \\n   Current Information Model (CIM): \\nThe Current Information Model (CIM) refers to the existing or legacy way organizations handle \\ninformation. This model is often siloed, lacks structured interconnectivity, and primarily focuses on \\nstoring data for specific departments or purposes without seamless integration across the product \\nlifecycle. \\n  Characteristics of CIM: \\n✔ Data is stored in separate databases. \\n✔ Limited integration between different departments (e.g., design, production, and marketing). \\n✔ Information is often stored in documents, spreadsheets, or legacy systems.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='✔ Limited integration between different departments (e.g., design, production, and marketing). \\n✔ Information is often stored in documents, spreadsheets, or legacy systems. \\n✔ Changes in one area don’t automatically update across the entire organization. \\n        Example: A company stores its engineering designs in CAD software, manufacturing data in ERP, \\nand supplier information in Excel files, leading to data inconsistencies.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='PLM Model: \\nThe Product Lifecycle Management (PLM) Model is a centralized, integrated, and structured \\napproach to managing product data and processes across the entire product lifecycle. It creates a \\nsingle source of truth (SSOT) for product-related information, ensuring collaboration, version \\ncontrol, and traceability. \\n  Characteristics of the PLM Model: \\n✔ Single centralized repository for all product-related data. \\n✔ Seamless integration between design, engineering, manufacturing, sales, and service. \\n✔ Version control & change management to track updates and approvals. \\n✔ Supports collaboration between teams, including suppliers and customers. \\n        Example: A PLM system like Teamcenter or Windchill stores and links CAD models, BOMs (Bill \\nof Materials), simulation data, compliance documents, and change histories in one place. \\n \\n2. Key Differences Between CIM and PLM Model \\nAspect Current Information Model (CIM) PLM Model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='of Materials), simulation data, compliance documents, and change histories in one place. \\n \\n2. Key Differences Between CIM and PLM Model \\nAspect Current Information Model (CIM) PLM Model \\nData Storage Siloed in multiple systems (CAD, ERP , Excel) Centralized database with \\nstructured access \\nData Consistency Prone to duplication and inconsistency Single source of truth (SSOT) \\nensures consistency \\nCollaboration Limited; data exchange between \\ndepartments is manual \\nSeamless collaboration across \\nteams, suppliers, and partners \\nChange \\nManagement \\nChanges must be manually communicated, \\noften leading to errors \\nAutomated workflows track and \\nmanage product changes \\nTraceability Difficult to trace product history across \\ndepartments \\nFull traceability from concept to \\nretirement \\nVersion Control Weak or non-existent; multiple versions of \\nthe same document may exist \\nRobust version control prevents \\nconflicts and data loss \\nRegulatory \\nCompliance \\nRequires manual tracking and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='the same document may exist \\nRobust version control prevents \\nconflicts and data loss \\nRegulatory \\nCompliance \\nRequires manual tracking and \\ndocumentation \\nBuilt-in compliance tracking and \\nreporting \\nProcess \\nAutomation \\nMinimal automation, reliant on human \\nintervention \\nAutomated workflows, reducing \\nerrors and inefficiencies \\nCost of Errors High due to miscommunication and data \\nloss \\nLow, as errors are detected and \\nresolved early'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='Aspect Current Information Model (CIM) PLM Model \\nScalability Hard to scale, as adding new features or \\nusers is complex \\nEasily scalable for large, global \\norganizations \\n \\n3. Importance of Moving from CIM to PLM Model \\nOrganizations that transition from the Current Information Model to a PLM Model experience: \\n✔ Faster time-to-market due to efficient workflows. \\n✔ Improved product quality due to better change management. \\n✔ Cost savings by reducing errors, redundancies, and rework. \\n✔ Better compliance & audit readiness for industry regulations. \\n✔ Enhanced innovation by integrating AI, IoT, and digital twins. \\n        Example: A global automotive manufacturer moves from Excel-based data storage (CIM) to \\nPLM-based digital product management. As a result, they reduce design iterations by 30% and \\nimprove collaboration between R&D, manufacturing, and suppliers. \\n \\n4. Conclusion \\n✔ The Current Information Model (CIM) is outdated, fragmented, and inefficient for modern'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-24T08:15:33+05:30', 'author': 'JASMINE DORATHY V', 'moddate': '2025-03-24T08:15:33+05:30', 'source': '..\\\\data\\\\pdf\\\\plm 15 marks.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'plm 15 marks.pdf', 'file_type': 'pdf'}, page_content='improve collaboration between R&D, manufacturing, and suppliers. \\n \\n4. Conclusion \\n✔ The Current Information Model (CIM) is outdated, fragmented, and inefficient for modern \\nproduct development. \\n✔ The PLM Model provides an integrated, structured, and scalable approach to product data \\nmanagement, ensuring real-time collaboration, version control, and process automation. \\n✔ Transitioning to a PLM system is essential for digital transformation, competitive advantage, and \\nsustainable growth in today’s industries.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"UNIT-01 \\n2 MARKS \\n \\n1.Summarize the reasons for the domain expertise for \\nany type of data analytics. \\n \\nDomain expertise is crucial in data analytics because it provides the necessary context to understand \\ndata, identify relevant patterns, and interpret results, leading to better data quality, more \\nmeaningful insights, and ultimately, more effective decision-making.  \\nHere's a more detailed explanation of why domain expertise is so important: \\n• Data Quality Assessment: \\nDomain experts can identify anomalies, outliers, and potential biases in the data that might go \\nunnoticed by someone lacking domain knowledge, ensuring the data is reliable and accurate.  \\n• Feature Engineering: \\nDomain expertise helps identify which features are most relevant and meaningful for a particular \\nproblem, leading to more effective models and analysis.  \\n• Model Interpretation: \\nUnderstanding the domain context is crucial for interpreting the output of a data model and its\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='problem, leading to more effective models and analysis.  \\n• Model Interpretation: \\nUnderstanding the domain context is crucial for interpreting the output of a data model and its \\nimplications in the real world, ensuring that the insights are actionable and relevant.  \\n• Understanding Data Context: \\nDomain knowledge helps analysts understand how data is collected, its sources, and the context in \\nwhich it exists, which is essential for proper data preprocessing and cleaning.  \\n• Identifying Actionable Insights: \\nDomain expertise allows analysts to translate data insights into practical recommendations and \\nsolutions that address real-world business issues.  \\n• Improved Communication: \\nDomain expertise facilitates better communication between data analysts and stakeholders, \\nensuring that everyone understands the findings and can make informed decisions.  \\n• Innovation and Opportunity Identification:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='ensuring that everyone understands the findings and can make informed decisions.  \\n• Innovation and Opportunity Identification: \\nDomain experts can identify opportunities for innovation and new approaches to problem-solving \\nbased on their understanding of the industry and data.  \\n• Tailored Solutions: \\nDomain knowledge allows for the development of solutions that are tailored to the specific needs \\nand challenges of a particular industry or business.  \\n \\n \\n1. Summarize the reasons for domain expertise in any type of data analytics. \\nDomain expertise is essential in data analytics as it helps in understanding the context of data, \\nidentifying relevant trends, correlations, and anomalies, and ensuring accurate interpretation of \\nresults. Without domain knowledge, analysts may misinterpret insights, leading to incorrect \\nconclusions. Expertise in areas like healthcare, finance, or marketing ensures that data-driven \\ndecisions align with business objectives.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='conclusions. Expertise in areas like healthcare, finance, or marketing ensures that data-driven \\ndecisions align with business objectives. \\nExample: In healthcare, a domain expert can help identify meaningful patterns in patient data, such \\nas risk factors for diseases, leading to better diagnosis and treatment recommendations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='2.How the accuracy in big data is beneficial? \\n \\nData accuracy is vital to the success of all organizations—from sales to accounting and marketing to \\nhuman resources. Data informs decisions, creates impressions about an organization, and drives \\nrevenue. Reasons why data accuracy is important and a priority for the enterprise are that data \\naccuracy: \\n• Delivers better results to the organization’s users \\n• Drives more value from artificial intelligence implementations with accurate and consistent \\ndata to feed algorithms \\n• Enables better decision-making \\n• Enhances efficiency \\n• Makes it easier to achieve consistent results \\n• Mitigates risks associated with flawed data \\n• Provides confidence to users who depend on the data \\n• Reduces the need to spend time and money finding and fixing errors in the data \\n• Supports focused audience targeting and marketing efforts \\n2. How is accuracy in big data beneficial?'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"• Reduces the need to spend time and money finding and fixing errors in the data \\n• Supports focused audience targeting and marketing efforts \\n2. How is accuracy in big data beneficial? \\nAccuracy in big data ensures better decision-making, predictive analytics, and fraud \\ndetection. High accuracy reduces errors, biases, and misinformation, leading to efficient \\nbusiness operations and improved customer experiences. \\nExample: In financial services, accurate big data analysis can detect fraudulent transactions \\nin real-time, preventing losses and enhancing security. \\n3.What are the top challenges in big data? \\nThe top challenges in big data revolve around managing its volume, velocity, \\nvariety, and veracity, along with data quality, security, and integration, as well as \\nfinding and retaining skilled talent.  \\nHere's a more detailed breakdown: \\n1. Volume, Velocity, Variety, and Veracity (The 4 Vs): \\n• Volume: \\nThe sheer amount of data generated and stored is immense, requiring robust\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"Here's a more detailed breakdown: \\n1. Volume, Velocity, Variety, and Veracity (The 4 Vs): \\n• Volume: \\nThe sheer amount of data generated and stored is immense, requiring robust \\nstorage and processing infrastructure.  \\n• Velocity: \\nData is generated and processed at an increasingly rapid pace, demanding real-\\ntime analytics and processing capabilities.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='• Variety: \\nBig data comes in various formats (structured, unstructured, semi-structured) \\nfrom diverse sources, requiring flexible data management and analysis tools.  \\n• Veracity: \\nData quality and accuracy are crucial, as inaccurate or incomplete data can lead \\nto flawed insights and decisions \\n3. What are the top challenges in big data? \\nBig data presents challenges such as data storage, processing speed, data \\nquality, security, and privacy. Managing the 3Vs (Volume, Velocity, Variety) is \\ndifficult, requiring advanced tools like Hadoop and Spark. Ensuring data \\nintegrity and protecting sensitive information from cyber threats is also critical. \\nExample: Social media companies face challenges in analyzing millions of posts \\nper second while filtering out fake news and maintaining user privacy. \\n \\n4,12.Distinguish between Big data and conventional \\ndata. \\n \\nTraditional Data  Big Data  \\nTraditional data is generated in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='per second while filtering out fake news and maintaining user privacy. \\n \\n4,12.Distinguish between Big data and conventional \\ndata. \\n \\nTraditional Data  Big Data  \\nTraditional data is generated in \\nenterprise level. Big data is generated outside the enterprise level. \\nIts volume ranges from Gigabytes to \\nTerabytes. \\nIts volume ranges from Petabytes to Zettabytes or \\nExabytes. \\nTraditional database system deals \\nwith structured data. \\nBig data system deals with structured, semi-\\nstructured,database, and unstructured data. \\nTraditional data is generated per hour or \\nper day or more. \\nBut big data is generated more frequently mainly \\nper seconds. \\nTraditional data source is centralized \\nand it is managed in centralized form. \\nBig data source is distributed and it is managed in \\ndistributed form. \\nData integration is very easy. Data integration is very difficult.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='Traditional Data  Big Data  \\nNormal system configuration is capable \\nto process traditional data. \\nHigh system configuration is required to process \\nbig data. \\nThe size of the data is very small. The size is more than the traditional data size. \\nTraditional data base tools are required \\nto perform any data base operation. \\nSpecial kind of data base tools are required to \\nperform any database schema based operation. \\nNormal functions can manipulate data. Special kind of functions can manipulate data. \\nIts data model is strict schema based \\nand it is static. \\nIts data model is a flat schema based and it is \\ndynamic. \\nTraditional data is stable and inter \\nrelationship. Big data is not stable and unknown relationship. \\nTraditional data is in manageable \\nvolume. \\nBig data is in huge volume which becomes \\nunmanageable. \\nIt is easy to manage and manipulate the \\ndata. It is difficult to manage and manipulate the data. \\n \\n4. Distinguish between big data and conventional data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"unmanageable. \\nIt is easy to manage and manipulate the \\ndata. It is difficult to manage and manipulate the data. \\n \\n4. Distinguish between big data and conventional data. \\n• Big Data: Large, complex datasets that require specialized tools for analysis. It includes \\nstructured, unstructured, and semi-structured data from diverse sources. \\n• Conventional Data: Smaller, structured datasets stored in relational databases (RDBMS) \\nwith fixed schemas. \\nExample: \\n• Big Data: Real-time IoT sensor data from smart cities. \\n• Conventional Data: Employee records stored in an Excel sheet. \\n \\n5.Outline the role of big data analytics. \\n \\nBig data analytics plays a crucial role in modern organizations by enabling them to extract valuable \\ninsights from large datasets, leading to better decision-making, improved operations, and increased \\ncompetitiveness.  \\nHere's a more detailed outline: \\n1. Uncovering Hidden Patterns and Trends: \\n• Identifying Opportunities:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"competitiveness.  \\nHere's a more detailed outline: \\n1. Uncovering Hidden Patterns and Trends: \\n• Identifying Opportunities: \\nBig data analytics helps organizations discover hidden patterns, correlations, and market trends that \\nmight otherwise go unnoticed, allowing them to identify new opportunities for growth and \\ninnovation.  \\n• Predictive Analytics:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"By analyzing historical data, organizations can use big data analytics to predict future outcomes, \\nenabling them to make proactive decisions and mitigate potential risks.  \\n• Understanding Customer Behavior: \\nBig data analytics provides insights into customer behavior, preferences, and needs, allowing \\nbusinesses to personalize their offerings and improve customer experiences.  \\n \\n5. Outline the role of big data analytics. \\nBig data analytics helps in trend identification, predictive modeling, and real-time decision-\\nmaking. It enhances customer experiences, operational efficiency, and fraud detection. \\nExample: E-commerce companies like Amazon use big data analytics to recommend products \\nbased on user behavior. \\n \\n \\n6,15.State the four computing resources of Big Data \\nStorage. \\nThe four main computing resources for Big Data Storage are data storage, data mining, data \\nanalytics, and data visualization.  \\nHere's a more detailed explanation of each: \\n• Data Storage:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content=\"The four main computing resources for Big Data Storage are data storage, data mining, data \\nanalytics, and data visualization.  \\nHere's a more detailed explanation of each: \\n• Data Storage: \\nThis involves the methods and technologies used to store large, complex datasets, which can include \\ndata lakes, data warehouses, cloud storage, and object storage.  \\n• Data Mining: \\nThis process involves extracting valuable insights and patterns from large datasets using various \\ntechniques and algorithms.  \\n• Data Analytics: \\nThis focuses on analyzing and interpreting the extracted data to gain meaningful insights and make \\ninformed decisions.  \\n• Data Visualization: \\nThis involves presenting the analyzed data in a visual format, such as charts, graphs, and dashboards, \\nto facilitate understanding and communication of findings.  \\n \\n6. State the four computing resources of Big Data Storage. \\n1. Processing Power (CPU & GPU): Required for computing large datasets.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='to facilitate understanding and communication of findings.  \\n \\n6. State the four computing resources of Big Data Storage. \\n1. Processing Power (CPU & GPU): Required for computing large datasets. \\n2. Storage (HDFS, Cloud Storage): Stores petabytes of structured and unstructured data. \\n3. Memory (RAM): Supports high-speed processing and real-time analytics. \\n4. Networking (High-Speed Data Transfer): Ensures seamless data movement across systems. \\nExample: Google Cloud provides high-performance computing for real-time big data processing. \\n \\n \\n7.What role does cloud computing play in Big Data \\nManagement? \\n \\nCloud Computing: The Big Data Solution \\nCloud computing offers an effective solution towards dealing with big size information \\nsets. Organizations can store their big-data efficiently manage them as well analyze them by \\nleveraging scalability provided through clouds on demand resources such as storage capacity'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='7. What role does cloud computing play in Big Data Management? \\nCloud computing provides scalability, cost-efficiency, remote accessibility, and real-time processing \\nfor big data applications. Platforms like AWS, Google Cloud, and Azure offer distributed computing \\nand AI-powered analytics. \\nExample: Netflix uses AWS cloud computing to process and recommend personalized content to \\nmillions of users. \\n \\n \\n8.List the various dimensions of growth of big data. \\n \\nThe growth of big data is often characterized by the \"5 V\\'s\": Volume, Velocity, Variety, Veracity, and \\nValue.  \\nHere\\'s a breakdown of each dimension: \\n• Volume: \\nRefers to the sheer amount of data being generated and collected, often measured in terabytes, \\npetabytes, or even exabytes.  \\n• Velocity: \\nDescribes the speed at which data is generated, collected, and processed, including real-time and \\nbatch processing.  \\n• Variety: \\nEncompasses the different types and formats of data, including structured, unstructured, and semi-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='batch processing.  \\n• Variety: \\nEncompasses the different types and formats of data, including structured, unstructured, and semi-\\nstructured data.  \\n• Veracity: \\nRelates to the accuracy, reliability, and trustworthiness of the data.  \\n• Value: \\nFocuses on the insights and actionable knowledge that can be derived from the data, enabling better  \\ndecision-making and business outcomes.  \\n \\n8. List the various dimensions of growth of big data. \\n1. Volume: Increase in the amount of data generated daily. \\n2. Velocity: Speed at which data is created and processed. \\n3. Variety: Diverse data types (text, images, audio, video). \\n4. Veracity: Ensuring data accuracy and reliability. \\n5. Value: Extracting meaningful insights for business impact. \\nExample: Twitter generates terabytes of text data every day, requiring advanced analytics. \\n \\n \\n9.Identify the risks involved in using big data. \\n \\nUsing big data presents risks including  \\ndata privacy breaches \\n security vulnerabilities'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='9.Identify the risks involved in using big data. \\n \\nUsing big data presents risks including  \\ndata privacy breaches \\n security vulnerabilities \\n ethical concerns, \\n potential for misuse \\n managing large datasets.  \\n \\n9. Identify the risks involved in using big data. \\n• Privacy breaches: Exposure of sensitive user data. \\n• Security vulnerabilities: Risk of cyberattacks. \\n• Bias in AI models: Incorrect conclusions due to skewed data.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='• Data inconsistency: Errors caused by poor-quality data. \\nExample: A social media company mishandling user data can lead to data breaches and legal \\nconsequences. \\n \\n \\n \\n10.With example explain structured data. \\nStructured data refers to information organized in a predefined format, making it easily searchable \\nand analyzable by computers, such as data in tables, databases, or spreadsheets. Examples \\ninclude customer names, addresses, dates, times, and product prices.  \\n• Spreadsheets (Excel): \\nData organized in rows and columns, similar to relational databases, but with more flexibility in \\nformatting and data types.  \\n \\n10. With an example, explain structured data. \\nStructured data is highly organized and stored in relational databases with predefined schemas. It is \\neasy to query using SQL. \\nExample: A customer database containing fields like Name, Age, Email, and Purchase History stored \\nin an SQL table. \\n \\n \\n11.State the best practices for Big Data Analytics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='Example: A customer database containing fields like Name, Age, Email, and Purchase History stored \\nin an SQL table. \\n \\n \\n11.State the best practices for Big Data Analytics. \\n \\n• Implement Data Quality Management Programs. \\n• Build More Scalable Infrastructures. \\n• Employ Agile Development Methodologies. \\n• Safeguard Data With Robust Security Measures. \\n• Use Data Ethically. \\n• Monitor and Optimize Continuously. \\n• Provide Workforce Skill Development. \\n11. State the best practices for Big Data Analytics. \\n1. Ensure data quality and consistency. \\n2. Use scalable and efficient storage solutions (HDFS, Cloud). \\n3. Implement security measures to protect sensitive data. \\n4. Leverage AI and ML for deeper insights. \\n5. Optimize processing speed with parallel computing. \\nExample: A bank using AI-driven fraud detection models must ensure data accuracy for reliable \\npredictions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='13.Compare data analytics and big data analytics. \\n \\nThe purpose of big data is to store huge \\nvolume of data and to process it. \\nThe purpose of data analytics is to \\nanalyze the raw data and find out \\ninsights for the information. \\nParallel computing and other complex \\nautomation tools are used to handle big \\ndata. \\nPredictive and statistical modelling with \\nrelatively simple tools used to handle \\ndata analytics. \\nBig data operations are handled by big data \\nprofessionals. \\nData analytics is performed by skilled \\ndata analysts. \\nBig data analysts need the knowledge of \\nprogramming, NoSQL databases, distributed \\nsystems, and frameworks. \\nData Analysts need the knowledge of \\nprogramming, statistics, and \\nmathematics. \\nIt supports in dealing with huge volumes of \\ndata. \\nIt supports in examining raw data and \\nrecognizing useful information. \\n \\n13. Compare data analytics and big data analytics. \\nFeature Data Analytics Big Data Analytics \\nData Size Small to Medium Very Large (Petabytes)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='recognizing useful information. \\n \\n13. Compare data analytics and big data analytics. \\nFeature Data Analytics Big Data Analytics \\nData Size Small to Medium Very Large (Petabytes) \\nProcessing SQL, Excel Hadoop, Spark \\nComplexity Simple Queries AI, ML, Predictive \\nExample HR payroll data analysis Sentiment analysis of social media posts \\nIdentify the four computing resources of Big Data Storage. \\n \\n11,14. List out the best practices of Big Data Analytics. \\n• Define clear business objectives. \\n• Use the right technology stack (Spark, NoSQL). \\n• Ensure compliance with data privacy regulations (GDPR, HIPAA). \\n• Implement real-time monitoring for fraud detection. \\nExample: Amazon optimizes inventory using real-time big data analytics. \\n \\n16. Define big data and under what conditions it is \\ngiven that name. \\nBig Data refers to extremely large datasets that traditional databases cannot handle \\nefficiently. It is characterized by Volume, Velocity, Variety, Veracity, and Value.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='given that name. \\nBig Data refers to extremely large datasets that traditional databases cannot handle \\nefficiently. It is characterized by Volume, Velocity, Variety, Veracity, and Value. \\nExample: Facebook stores and processes billions of images, videos, and messages daily. \\n \\nWhy it\\'s called \"big data\":'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-03-20T08:41:06+05:30', 'author': 'subashb7561@gmail.com', 'moddate': '2025-03-20T08:41:06+05:30', 'source': '..\\\\data\\\\pdf\\\\UNIT-01_2M[1].pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'UNIT-01_2M[1].pdf', 'file_type': 'pdf'}, page_content='The term \"big data\" is used because these datasets are so large and complex that they \\nrequire specialized technologies and tools to manage, store, and analyze them effectively.  \\n \\ncharacterized by the \"5 Vs\": volume, velocity, variety, veracity, and value.  \\n \\n• Examples: \\n• Social media posts, emails, and web traffic data.  \\n• Financial transaction records, customer databases, and inventory data.  \\n• Sensor data from IoT devices, medical records, and scientific experiments.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27975c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 283 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:10<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (283, 384)\n",
      "Adding 283 documents to vector store...\n",
      "Successfully added 283 documents to vector store\n",
      "Total documents in collection: 283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a221a",
   "metadata": {},
   "source": [
    "\n",
    "Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64e2d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "005858b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1a24fb8bb60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0091724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
